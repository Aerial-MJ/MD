# Project

model environment :\
 MinerU->  MinerU-Py312\
 Paddle->  paddle-Py310


.\
‚îú‚îÄ‚îÄ Meta-Llama\
‚îÇ   ‚îî‚îÄ‚îÄ Meta-Llama-3-8B-Instruct\
‚îú‚îÄ‚îÄ MinerU\
‚îÇ   ‚îú‚îÄ‚îÄ MinerU\
‚îÇ   ‚îú‚îÄ‚îÄ MinerU2.5-2509-1.2B\
‚îÇ   ‚îî‚îÄ‚îÄ project.md\
‚îú‚îÄ‚îÄ Paddle\
‚îÇ   ‚îú‚îÄ‚îÄ PaddleOCR\
‚îÇ   ‚îî‚îÄ‚îÄ project.md\
‚îú‚îÄ‚îÄ Qwen\
‚îÇ   ‚îú‚îÄ‚îÄ Qwen2.5-7B-Instruct\
‚îÇ   ‚îú‚îÄ‚îÄ Qwen2.5-VL-7B-Instruct\
‚îÇ   ‚îú‚îÄ‚îÄ Qwen3-1.7B\
‚îÇ   ‚îú‚îÄ‚îÄ Qwen3-1.7B-GGUF\
‚îÇ   ‚îú‚îÄ‚îÄ Qwen3-4B-Instruct-2507\
‚îÇ   ‚îî‚îÄ‚îÄ Qwen3-4B-Thinking-2507\
‚îî‚îÄ‚îÄ Tool\
    ‚îî‚îÄ‚îÄ model.md\

## model select

### Qwen
| ÁâàÊú¨ / Âèò‰Ωì           | ÂèÇÊï∞ËßÑÊ®° / ÊøÄÊ¥ªÂèÇÊï∞                                                                                 | ÊòØÂê¶ÊîØÊåÅËßÜËßâ / Â§öÊ®°ÊÄÅ                     | Ê†∏ÂøÉËÉΩÂäõ / ÁâπÁÇπ                                                   | ÈÄÇÂêàÂú∫ÊôØ / ‰ºòÂäø                   | ÂèØËÉΩÁöÑÈôêÂà∂ÊàñÁº∫ÁÇπ                             |
| ----------------- | ------------------------------------------------------------------------------------------- | -------------------------------- | ----------------------------------------------------------- | --------------------------- | ------------------------------------ |
| QwenÔºàÂéüÂßãÔºâ          | Â¶Ç 1.8B„ÄÅ7B„ÄÅ14B„ÄÅ72B Á≠â ([GitHub][1])                                                           | ‰ªÖÊñáÊú¨                              | ËØ≠Ë®ÄÁêÜËß£„ÄÅÊñáÊú¨ÁîüÊàê„ÄÅÂØπËØùÁ≠âÂü∫Êú¨‰ªªÂä°                                           | ÊñáÊú¨Á±ª‰ªªÂä°ÔºàÂêàÂêå„ÄÅË¥¢Êä•ÊñáÊú¨Ôºâ              | Êó†ÂõæÂÉèËæìÂÖ•ËÉΩÂäõÔºå‰∏çÈÄÇÂêàÂ§ÑÁêÜÂõæÂÉè + ÊñáÊú¨Ê∑∑ÂêàËæìÂÖ•             |
| Qwen2 / Qwen2.5   | Â§ö‰∏™ËßÑÊ®°ÁâàÊú¨Ôºà0.5B„ÄÅ1.5B„ÄÅ3B„ÄÅ7B„ÄÅ14B„ÄÅ32B„ÄÅ72B Á≠âÔºâ ([Hugging Face][2])                                   | ÊúâËßÜËßâÁâàÊú¨ÔºàQwen2-VLÔºâ                  | Êõ¥Âº∫ËØ≠Ë®ÄËÉΩÂäõ„ÄÅÊõ¥Â§ß‰∏ä‰∏ãÊñá„ÄÅÊîØÊåÅÂõæÂÉè + ÊñáÊú¨ÔºàÊüê‰∫õÂèò‰ΩìÔºâ                                | Ê∑∑ÂêàÁ±ª‰ªªÂä°„ÄÅÂõæÊñáËæìÂÖ•„ÄÅÊñáÊú¨‰ªªÂä°             | ËæÉÂ§ßÊ®°ÂûãÂú®ÊòæÂ≠ò / ËµÑÊ∫ê‰∏äË¶ÅÊ±ÇÈ´òÔºõËßÜËßâ + ÊñáÊú¨ÁâàÊú¨ÂèØËÉΩÊõ¥Èáç       |
| Qwen3             | ÂåÖÂê´ dense ‰∏é MoE ÁâàÊú¨ÔºåÂ¶Ç 0.6B„ÄÅ1.7B„ÄÅ4B„ÄÅ8B„ÄÅ14B„ÄÅ32BÔºàdenseÔºâÔºå‰ª•Âèä MoE Ê®°ÂûãÔºà30B-A3B„ÄÅ235B-A22BÔºâ ([Qwen][3]) | Âü∫Êú¨Ê®°ÂûãÊòØËØ≠Ë®ÄÊ®°ÂûãÔºõÈÉ®ÂàÜÂèò‰ΩìÊúâÂ§öÊ®°ÊÄÅÁâàÊú¨Â¶Ç Qwen3-Omni | Ê∑∑ÂêàÊé®ÁêÜ / ÈÄöÁî®ËÉΩÂäõÔºåÊîØÊåÅ ‚ÄúÊÄùËÄÉÊ®°ÂºèÔºàThinking Ê®°ÂºèÔºâ‚Äù ‰∏é ‚ÄúÈùûÊÄùËÄÉÊ®°Âºè‚Äù ÂàáÊç¢ ([Qwen][4]) | ÂØπÂ§çÊùÇÊé®ÁêÜ„ÄÅÂ§öËØ≠Ë®Ä„ÄÅÈïø‰∏ä‰∏ãÊñá„ÄÅÈúÄÂàáÊç¢Ê®°ÂºèÁöÑ‰ªªÂä°     | Â§ßÊ®°Âûã / MoE Ê®°ÂûãÊé®ÁêÜÊàêÊú¨È´òÔºõÈÉ®ÂàÜÁâàÊú¨‰ªç‰∏çÊîØÊåÅËßÜËßâÔºåÈúÄÊê≠ÈÖçËßÜËßâÊ®°Âùó |
| Qwen-VL / ËßÜËßâÂèò‰Ωì    | ÈöèÊ®°ÂûãÁâàÊú¨ËÄåÂÆö                                                                                     | ÊòØÊîØÊåÅËßÜËßâ + ÊñáÊú¨ËæìÂÖ•                     | ÂêåÊó∂ÁêÜËß£ÂõæÂÉè‰∏éÊñáÊú¨„ÄÅÂÅöËßÜËßâ + ËØ≠Ë®Ä‰ªªÂä°                                        | Á•®ÊçÆÂõæÂÉè„ÄÅÂõæÊñáÊ∑∑ÂêàÊä•Ë°®„ÄÅÂêàÂêå PDFÔºàÂõæÂÉè + ÊñáÊú¨Ôºâ | Ê®°Âûã‰ΩìÁßØÊõ¥Â§ßÔºåÊòæÂ≠òÈúÄÊ±ÇÈ´ò                         |
| Qwen-Omni         | Â§öÊ®°ÊÄÅÂÖ®Ë¶ÜÁõñÔºàÊñáÊú¨ + ÂõæÂÉè + Èü≥È¢ë + ËßÜÈ¢ëÔºâ ([arXiv][5])                                                      | ÊòØ                                | Áªü‰∏ÄÊÑüÁü• + ÁîüÊàêÔºåÈÄÇÁî®‰∫éÈü≥ËßÜÈ¢ëÊñáÊú¨Ê∑∑ÂêàÂú∫ÊôØ                                      | ÂØπ‰∫éÊûÅÂ§çÊùÇÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°ÊòØÊú™Êù•ÊñπÂêë            | ÂΩìÂâçÁâàÊú¨ÂèØËÉΩËæÉÊñ∞ÔºåÁ§æÂå∫ÊîØÊåÅ / ÊñáÊ°£ËæÉÂ∞ëÔºõÊé®ÁêÜÊàêÊú¨È´ò           |
| Qwen-Coder / ÁºñÁ®ãÁâàÊú¨ | ÁªëÂÆöÂú® Qwen2.5, Qwen3 Á≠âÁâàÊú¨                                                                      | ‰∏ªË¶ÅÊñáÊú¨ + ‰ª£Á†ÅÁêÜËß£ / Â∑•ÂÖ∑Ë∞ÉÁî®               | Âú®‰ª£Á†ÅÁîüÊàê„ÄÅÂ∑•ÂÖ∑ÈìæÊâßË°å‰∏äÊúâ‰ºòÂåñ                                             | ÁºñÁ®ã„ÄÅËÑöÊú¨ÁîüÊàê„ÄÅÂ∑•ÂÖ∑Êé•Âè£Âú∫ÊôØ              | ÂØπ‰∫éÁ∫ØÂêàÂêå / ÈáëËûçÊñáÊ°£Ëß£ÊûêÔºåÂÖ∂‰ºòÂäøÂèØËÉΩÂº±‰∫éÈÄöÁî®Ê®°Âûã           |

[1]: https://github.com/QwenLM/Qwen?utm_source=chatgpt.com "QwenLM/Qwen: The official repo of Qwen (ÈÄö‰πâÂçÉÈóÆ) chat ... - GitHub"
[2]: https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e?utm_source=chatgpt.com "Qwen2.5 - a Qwen Collection - Hugging Face"
[3]: https://qwen.readthedocs.io/?utm_source=chatgpt.com "Qwen"
[4]: https://qwen.readthedocs.io/en/latest/getting_started/concepts.html?utm_source=chatgpt.com "Key Concepts - Qwen - Read the Docs"
[5]: https://arxiv.org/abs/2509.17765?utm_source=chatgpt.com "Qwen3-Omni Technical Report"

### ÂØπ‰∫éÈáëËûçÊ®°Âûã

| ÁõÆÊ†á                | Êé®ËçêÁâàÊú¨ / Âèò‰Ωì                                 | ÁêÜÁî±                        |
| ----------------- | ----------------------------------------- | ------------------------- |
| ÊñáÊú¨Á±ª‰ªªÂä°ÔºàÂ¶ÇÂêàÂêå / Ë¥¢Êä•ÊñáÊú¨Ôºâ | Qwen2.5Ôºà‰∏≠ÂûãÁâàÊú¨ÔºåÂ¶Ç 7B Êàñ 14BÔºâ Êàñ Qwen3Ôºà4B / 8BÔºâ | ËÉΩÂ§ÑÁêÜÂ§çÊùÇËØ≠Ë®Ä„ÄÅÂÖ∑Â§áËæÉÂ•ΩÊé®ÁêÜ‰∏éË°®ËææËÉΩÂäõ       |
| ÂõæÂÉè + ÊñáÊú¨Ê∑∑Âêà‰ªªÂä°       | Qwen-VL / Qwen3-Omni / Qwen2.5-VL ÁâàÊú¨      | ÊîØÊåÅÁõ¥Êé•ÊääÂõæÂÉè + ÊñáÊú¨ËæìÂÖ•ÔºåÂáèÂ∞ë‰∏≠Èó¥Ê®°Âùó     |
| ‰ΩéÊòæÂ≠ò / Êú¨Âú∞ÈÉ®ÁΩ≤        | Qwen3-4B / Qwen3-1.7B / Qwen2.5-7B        | ËæÉÂ∞èÊ®°ÂûãÔºåÊòæÂ≠òÂç†Áî®ËæÉ‰Ωé               |
| Â§çÊùÇÊé®ÁêÜ / Â§öÊ≠•ÈÄªËæëÈóÆÈ¢ò     | Qwen3ÔºàÊúâ Thinking Ê®°ÂºèÔºâ                      | ËÉΩÂú®Âêå‰∏ÄÊ®°ÂûãÂÜÖÂàáÊç¢ÊÄùËÄÉ / ÈùûÊÄùËÄÉÊ®°ÂºèÂ§ÑÁêÜÂ§çÊùÇÈóÆÈ¢ò |



## Ê®°ÂûãÂàÜÁ±ª
ÔºàGGUF„ÄÅThinking„ÄÅVL„ÄÅBase„ÄÅInstruct„ÄÅFP8‚Ä¶ÔºâÁöÑÂå∫Âà´

---

###  ‰∏Ä„ÄÅÊ®°ÂûãÊ†ºÂºèÂå∫Âà´

| ÂêçÁß∞                                  | ËØ¥Êòé                                                              | ÂÖ∏ÂûãÁî®ÈÄî                         |
| ----------------------------------- | --------------------------------------------------------------- | ---------------------------- |
| **ÂéüÁâàÔºàTransformers Ê†ºÂºèÔºâ**             | ÂÆòÊñπÂéüÂßãÊ®°ÂûãÔºà`pytorch_model.bin` Êàñ `.safetensors`Ôºâ                    | Áî®‰∫é `transformers` Â∫ìÂä†ËΩΩ„ÄÅËÆ≠ÁªÉ„ÄÅÊé®ÁêÜ  |
| **GGUF**                            | *ÈáèÂåñÊ†ºÂºè*ÔºåÁî± [llama.cpp](https://github.com/ggerganov/llama.cpp) ‰ΩøÁî® | Êõ¥Â∞è„ÄÅÊé®ÁêÜÊõ¥Âø´ÔºåÂèØÂú® CPU„ÄÅÊú¨Âú∞ GPU„ÄÅÁîöËá≥ÊâãÊú∫‰∏äË∑ë |
| **GGML / GGUF / GPTQ / AWQ / EXL2** | ÈÉΩÊòØ‚ÄúÈáèÂåñÊé®ÁêÜÊ†ºÂºè‚ÄùÁöÑ‰∏çÂêå‰ΩìÁ≥ª                                                 | GGUF ÊòØ GGML ÁöÑÊñ∞ÁâàÊú¨ÔºåÂÖºÂÆπÊÄßÊõ¥Âº∫       |

> üí°ÊÄªÁªìÔºö
>
> * GGUF ‚âà ‰∏∫Êú¨Âú∞È´òÊïàÊé®ÁêÜ‰ºòÂåñÁöÑÊ†ºÂºè
> * Transformers Ê†ºÂºè ‚âà ‰∏∫ËÆ≠ÁªÉ/ÂæÆË∞É‰ºòÂåñÁöÑÊ†ºÂºè

---

###  ‰∫å„ÄÅÊ®°ÂûãËÉΩÂäõÂå∫Âà´

| ÂêçÁß∞                                   | Âê´‰πâ                                | ‰∏æ‰æã                                           | Áî®ÈÄî             |
| ------------------------------------ | --------------------------------- | -------------------------------------------- | -------------- |
| **BaseÔºàÂü∫Á°ÄÁâàÔºâ**                        | Âè™ËÆ≠ÁªÉËØ≠Ë®ÄÂª∫Ê®°Ôºå‰∏çÁªèËøáÊåá‰ª§ÂæÆË∞É                   | `Meta-Llama-3-8B`                            | ÈÄÇÂêàÁªßÁª≠ÂæÆË∞É         |
| **InstructÔºàÊåá‰ª§ÁâàÔºâ**                    | Âú® Base Âü∫Á°Ä‰∏äËÆ≠ÁªÉ‚ÄúÈÅµ‰ªéÊåá‰ª§‚ÄùÁöÑËÉΩÂäõÔºàSFT + RLHFÔºâ | `Meta-Llama-3-8B-Instruct`                   | ÈÄÇÂêàËÅäÂ§©„ÄÅÈóÆÁ≠î„ÄÅÊô∫ËÉΩ‰Ωì    |
| **Chat**                             | ‰∏é Instruct Âü∫Êú¨Âêå‰πâÔºåÊúâÊó∂ÊòØÂïÜ‰∏öÁâàÊú¨           | `Qwen2-7B-Chat`                              | ÂØπËØù‰ºòÂåñ           |
| **Thinking / Reasoning / DeepThink** | Â¢ûÂº∫‚ÄúÊé®ÁêÜÈìæË∑ØÊÄùËÄÉËÉΩÂäõÔºàCoTÔºâ‚ÄùÁöÑÁâàÊú¨              | `Qwen2.5-7B-Think`„ÄÅ`Llama-3.1-70B-Reasoning` | Êé®ÁêÜ„ÄÅÂ§öÊ≠•È™§ÈÄªËæë‰ªªÂä°     |
| **VLÔºàVision-LanguageÔºâ**              | ËßÜËßâ + ÊñáÊú¨ Â§öÊ®°ÊÄÅÊ®°Âûã                     | `Qwen2-VL`„ÄÅ`Llama-3-Vision`                  | ÂõæÂÉèÁêÜËß£„ÄÅOCR„ÄÅÂ§öÊ®°ÊÄÅ‰ªªÂä° |

> üí°ÊÄªÁªìÔºö
>
> * **Base** = Ê®°ÂûãÁöÑ‚ÄúÂ∫ïÂ∫ß‚Äù
> * **Instruct/Chat** = ÊïôÂÆÉ‚ÄúÂê¨ÊáÇ‰∫∫ËØù‚Äù
> * **Thinking** = ÊïôÂÆÉ‚Äú‰ºöÊé®ÁêÜ‚Äù
> * **VL** = Âä†ËßÜËßâËæìÂÖ•

---

### ‚öô ‰∏â„ÄÅÁ≤æÂ∫¶‰∏éÈáèÂåñÂå∫Âà´ÔºàFP16„ÄÅFP8„ÄÅINT4„ÄÅQ4_K_M Á≠âÔºâ

| Ê†ºÂºè                                       | Âê´‰πâ       | ‰ºòÁÇπ           | Áº∫ÁÇπ       | Âú∫ÊôØ               |
| ---------------------------------------- | -------- | ------------ | -------- | ---------------- |
| **FP32**                                 | 32 ‰ΩçÊµÆÁÇπÁ≤æÂ∫¶ | Á≤æÂ∫¶ÊúÄÈ´ò         | ÊòæÂ≠òÂç†Áî®ÊúÄÂ§ß   | ËÆ≠ÁªÉÈò∂ÊÆµ             |
| **FP16 / BF16**                          | 16 ‰ΩçÊµÆÁÇπ   | Á≤æÂ∫¶Âá†‰πé‰∏çÊçüÂ§±ÔºåÈÄüÂ∫¶Êõ¥Âø´ | Âç†Áî®‰∏≠Á≠â     | Êé®ÁêÜ/ËÆ≠ÁªÉÊ†áÂáÜÊ†ºÂºè        |
| **FP8**                                  | 8 ‰ΩçÊµÆÁÇπ    | Êõ¥Â∞èÔºåÊé®ÁêÜÊõ¥Âø´      | ÂèØËÉΩÊçüÂ§±ËΩªÂæÆÁ≤æÂ∫¶ | Êñ∞ÊòæÂç°ÔºàH100/A100Ôºâ‰ºòÂåñ |
| **INT8 / INT4 / GGUF Q4 / Q5 / Q6 / Q8** | Êï¥Êï∞ÈáèÂåñ     | ÊûÅÂ∞è„ÄÅÂø´ÈÄü        | Á≤æÂ∫¶ÊçüÂ§±ÊòéÊòæ   | ËΩªÈáè CPU/GPU Êé®ÁêÜ    |

> üßÆ ‰∏æ‰æãÔºö
>
> * `Meta-Llama-3-8B-Instruct-FP16` ‚Üí ÂçäÁ≤æÂ∫¶ÂéüÂßãÊ®°Âûã
> * `Meta-Llama-3-8B-Instruct.Q4_K_M.gguf` ‚Üí ÈáèÂåñ4‰ΩçÁöÑGGUFÊñá‰ª∂
> * `Meta-Llama-3-8B-Instruct-FP8` ‚Üí ÊîØÊåÅH100ÊòæÂç°ÁöÑÊñ∞ÊµÆÁÇπ8Á≤æÂ∫¶ÁâàÊú¨

---

###  Âõõ„ÄÅ‰∏ãËΩΩÊó∂Â¶Ç‰ΩïÈÄâÊã©

| ‰Ω†ÁöÑÁî®ÈÄî              | Êé®ËçêÁâàÊú¨                                  | Ê†ºÂºè                  | ÂéüÂõ†           |
| ----------------- | ------------------------------------- | ------------------- | ------------ |
| ÊÉ≥ÂæÆË∞É / ËÆ≠ÁªÉ          | **Base (FP16)**                       | Transformers        | ÊúâÊúÄÂ§ßÁ≤æÂ∫¶„ÄÅÁªìÊûÑÊú™Ë¢´‰øÆÊîπ |
| ÂÅöÂØπËØù / ÈóÆÁ≠î / Êô∫ËÉΩ‰Ωì    | **Instruct (FP16 Êàñ GGUF Q4)**         | Transformers / GGUF | ËÉΩÂê¨ÊáÇÊåá‰ª§„ÄÅÂõûÁ≠îËá™ÁÑ∂   |
| Êú¨Âú∞ CPU Êé®ÁêÜ / ËΩªÈáèÂåñÈÉ®ÁΩ≤ | **Instruct + GGUF (Q4_K_M Êàñ Q5_K_M)** | GGUF                | ËäÇÁúÅÊòæÂ≠ò„ÄÅÈÄüÂ∫¶Âø´     |
| ÊÉ≥ÂÆûÈ™åÂ§öÊ®°ÊÄÅÔºàÂõæÂÉè+ÊñáÊú¨Ôºâ     | **VL**                                | Transformers        | ËÉΩÊé•Êî∂ÂõæÂÉèËæìÂÖ•      |
| ÂÅöÈìæÂºèÊé®ÁêÜ„ÄÅÂàÜÊûê„ÄÅÈÄªËæëÈóÆÁ≠î     | **Thinking**                          | Transformers / GGUF | ‰∏ì‰∏∫Êé®ÁêÜ‰ªªÂä°‰ºòÂåñ     |

---

###  ‰∫î„ÄÅÊÄªÁªì‰∏ÄÂè•ËØù

| ÂàÜÁ±ª   | Á§∫‰æã                                     | ÁâπÁÇπ                        |
| ---- | -------------------------------------- | ------------------------- |
| Ê†ºÂºè   | GGUF / Transformers                    | GGUF ‰ΩìÁßØÂ∞èÔºåTransformers ÂèØËÆ≠ÁªÉ |
| Ê®°ÂûãÁ±ªÂûã | Base / Instruct / Chat / Thinking / VL | ‰ªªÂä°ÂèñÂêë‰∏çÂêå                    |
| Á≤æÂ∫¶   | FP32 / FP16 / FP8 / INT4               | ÊùÉË°°ÊÄßËÉΩ‰∏éÁ≤æÂ∫¶                   |

## GGUF Ë∞ÉÁî®

**GGUF Ê†ºÂºèÁöÑÊ®°ÂûãÁ°ÆÂÆû‰∏çËÉΩÁõ¥Êé•Áî® `transformers` Ë∞ÉÁî®„ÄÇ**
Âõ†‰∏∫ÂÆÉ‰∏çÊòØ PyTorch ÁöÑ `safetensors` Êàñ `.bin` ÊùÉÈáçÊñá‰ª∂ÔºåËÄåÊòØ‰∏ì‰∏∫ **llama.cpp Êé®ÁêÜÂºïÊìé** ËÆæËÆ°ÁöÑÈ´òÊïàÈáèÂåñÊ†ºÂºè„ÄÇ

‰ΩÜÂà´ÊãÖÂøÉÔºåGGUF Êñá‰ª∂ÂÆåÂÖ®ËÉΩÁî®Â§öÁßçÂ∑•ÂÖ∑Âä†ËΩΩ„ÄÅËÅäÂ§©„ÄÅÊé®ÁêÜÔºåÁîöËá≥ÈÄöËøá Python API Ë∞ÉÁî®„ÄÇ

---

###  ‰∏Ä„ÄÅÊ†∏ÂøÉÂå∫Âà´ÊÄªÁªì

| È°πÁõÆ     | Transformers Ê®°Âûã         | GGUF Ê®°Âûã             |
| ------ | ----------------------- | ------------------- |
| Êñá‰ª∂Ê†ºÂºè   | `.bin` / `.safetensors` | `.gguf`             |
| Ê°ÜÊû∂‰æùËµñ   | PyTorch / TensorFlow    | llama.cpp / ggml    |
| ÊòØÂê¶ÂèØËÆ≠ÁªÉ  | ‚úÖ ÂèØÁªßÁª≠ÂæÆË∞É                 | ‚ùå ‰ªÖÊé®ÁêÜ               |
| ÊòØÂê¶ÊîØÊåÅÈáèÂåñ | ÈÄöÂ∏∏ FP16 / BF16          | Q2~Q8, A3B/A4BÁ≠âÂ§öÁßçÈáèÂåñ |
| ‰∏ªË¶ÅÁî®ÈÄî   | Á†îÁ©∂„ÄÅÂæÆË∞É„ÄÅ‰∫ëÁ´ØÈÉ®ÁΩ≤              | Êú¨Âú∞Êé®ÁêÜ„ÄÅËΩªÈáèÂ∫îÁî®„ÄÅOllama    |

---

###  ‰∫å„ÄÅGGUF Ë∞ÉÁî®ÁöÑÂá†ÁßçÊñπÂºè

####  ÊñπÂºè 1Ôºö‰ΩøÁî® **llama.cpp**

ÊúÄÂéüÁîüÁöÑÊñπÂºèÔºåC/C++ ÂÆûÁé∞ÔºåÊîØÊåÅÂëΩ‰ª§Ë°å‰∏é Python„ÄÇ

##### ÂÆâË£ÖÔºàÊé®Ëçê‰ΩøÁî®È¢ÑÁºñËØëÁâàÔºâ

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

##### Êé®ÁêÜÂëΩ‰ª§Á§∫‰æãÔºö

```bash
./main -m ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -p "‰Ω†Â•ΩÔºåËØ∑Áî®‰∏ÄÂè•ËØùËß£ÈáäÈáèÂåñÊ®°Âûã"
```

ËæìÂá∫Á§∫‰æãÔºö

```
ÈáèÂåñÊ®°ÂûãÈÄöËøáÂéãÁº©ÊùÉÈáç‰ª•ÂáèÂ∞ëËÆ°ÁÆó‰∏éÊòæÂ≠òÂç†Áî®„ÄÇ
```

---

####  ÊñπÂºè 2ÔºöÁî® **llama-cpp-python**ÔºàÊúÄÂ∏∏Áî®ÁöÑ Python Êé•Âè£Ôºâ

##### ÂÆâË£Ö

```bash
pip install llama-cpp-python
```

> üí° Â¶ÇÊûúÊúâ GPUÔºàCUDAÔºâÔºåÂèØ‰ª•Ôºö
>
> ```bash
> pip install llama-cpp-python[cuda]
> ```

##### ÊúÄÁÆÄË∞ÉÁî®‰ª£Á†Å

```python
from llama_cpp import Llama

llm = Llama(model_path="./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf", n_ctx=4096)
output = llm("‰Ω†Â•ΩÔºåËØ∑Ëß£Èáä‰∏Ä‰∏ãÈÄöË¥ßËÜ®ËÉÄ„ÄÇ")
print(output["choices"][0]["text"])
```

##### ÁªìÊûÑÂåñË∞ÉÁî®ÔºàÁ±ª‰ºº OpenAI Êé•Âè£Ôºâ

```python
from llama_cpp import Llama

llm = Llama(model_path="./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf")

resp = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "‰Ω†ÊòØ‰∏Ä‰∏™ÈáëËûçÂàÜÊûê‰∏ìÂÆ∂„ÄÇ"},
        {"role": "user", "content": "ËØ∑ÁÆÄË¶ÅËß£ÈáäGDP‰∏éÈÄöËÉÄÁöÑÂÖ≥Á≥ª„ÄÇ"}
    ]
)
print(resp["choices"][0]["message"]["content"])
```

> ‚ö° ‰ºòÁÇπÔºöËΩªÈáè„ÄÅË∑®Âπ≥Âè∞„ÄÅÂÖºÂÆπ Chat Ê†ºÂºè
> üß† ÂèØÁõ¥Êé•Áî®‰∫éÈáëËûçÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠Âèñ‰ª£ `transformers` Ê®°Âûã

---

####  ÊñπÂºè 3ÔºöÁî® **Ollama**

Â¶ÇÊûú‰Ω†Â∏åÊúõÂÉè ChatGPT ‰∏ÄÊ†∑‰∏ÄÊù°ÂëΩ‰ª§Âä†ËΩΩ GGUF Ê®°ÂûãÔºö

##### ÂÆâË£ÖÔºàLinux / macOS / WindowsÔºâ

üëâ [https://ollama.com/download](https://ollama.com/download)

##### Ê∑ªÂä†Ê®°Âûã

‰Ω†ÂèØ‰ª•Áõ¥Êé•Âä†ËΩΩÊú¨Âú∞ gguf Êñá‰ª∂Ôºö

```bash
ollama create my-llama3 -f ./Modelfile
```

Á§∫‰æã ModelfileÔºö

```
FROM ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
PARAMETER temperature 0.7
PARAMETER num_ctx 4096
```

##### ËÅäÂ§©

```bash
ollama run my-llama3
```

---

####  ÊñπÂºè 4ÔºöÁî® **LM Studio / GPT4All / KoboldCpp / TextGen WebUI**

ËøôÁ±ªÂ∑•ÂÖ∑Êèê‰æõ GUIÔºåÁõ¥Êé•Âä†ËΩΩ GGUF Êñá‰ª∂Âç≥ÂèØÔºåÊó†ÈúÄÂÜô‰ª£Á†Å„ÄÇ

---

###  ‰∏â„ÄÅÂú®‰Ω†ÁöÑÈáëËûçÂ§öÊô∫ËÉΩ‰ΩìÈ°πÁõÆ‰∏≠ÁöÑÊé®ËçêÊñπÊ°à

‰Ω†Áé∞Âú®ÁöÑÁ≥ªÁªüÁªìÊûÑ‰∏≠ÔºàÊú¨Âú∞ Qwen/Llama + ReAct + Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÔºâÔºö

* Â¶ÇÊûúÁî® **GGUF Ê®°Âûã**ÔºåÊé®ËçêÁî®Ôºö

  ```python
  from llama_cpp import Llama
  ```

  Êù•Êõø‰ª£ÂéüÊù•ÁöÑ `transformers.AutoModelForCausalLM.from_pretrained(...)`
* Â¶ÇÊûúË¶ÅÂ§öÁ∫øÁ®ãË∞ÉÁî®Â§ö‰∏™Êô∫ËÉΩ‰ΩìÔºåÂèØ‰ª•Áî®Ôºö

  ```python
  llm = Llama(model_path="xxx.gguf", n_ctx=4096, n_threads=8, n_gpu_layers=20)
  ```

---

###  ÊÄªÁªì‰∏ÄÂº†Ë°®

| ÁõÆÊ†á      | Êé®ËçêÂ∑•ÂÖ∑               | Ê†ºÂºèÊîØÊåÅ        | ÊòØÂê¶PythonË∞ÉÁî®         |
| ------- | ------------------ | ----------- | ------------------ |
| Êú¨Âú∞ÂëΩ‰ª§Ë°åÊé®ÁêÜ | llama.cpp          | GGUF        | ‚úÖ llama-cpp-python |
| ÂèØËßÜÂåñ‰∫§‰∫í   | LM Studio / Ollama | GGUF        | ‚úÖÔºàAPIÔºâ             |
| ËÆ≠ÁªÉ/ÂæÆË∞É   | Transformers       | safetensors | ‚úÖ                  |
| Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü  | llama-cpp-python   | GGUF        | ‚úÖ                  |

---

Â¶ÇÊûú‰Ω†ÂëäËØâÊàë‰Ω†ÁöÑÊú∫Âô®ÈÖçÁΩÆÔºà‰æãÂ¶Ç GPU ÂûãÂè∑ÂíåÊòæÂ≠òÂ§ßÂ∞èÔºâÔºå
ÊàëÂèØ‰ª•Â∏Æ‰Ω†ÂÜôÂá∫**ÊúÄ‰ºòÁöÑ llama-cpp-python Êé®ÁêÜÂèÇÊï∞ÁªÑÂêà**ÔºàÂåÖÊã¨ `n_gpu_layers`„ÄÅ`threads`„ÄÅ`batch_size` Á≠âÔºâ

## ÈáèÂåñÁ≤æÂ∫¶
ÂÉèÁúãÂà∞ÁöÑ `A3B`, `A4B`, `Q4_K_M`, `Q5_1`, `I8`, `FP8` Ëøô‰∫õÂêéÁºÄÔºåÂÖ∂ÂÆûÈÉΩÊòØÂú®ËØ¥Ôºö

> üëâ **Ê®°ÂûãË¢´‚ÄúÈáèÂåñ‚ÄùÊàê‰∫Ü‰ªÄ‰πàÁ≤æÂ∫¶„ÄÅ‰ªÄ‰πàÁÆóÊ≥ïÊ†ºÂºè„ÄÇ**

ËøôÂÜ≥ÂÆö‰∫ÜÊ®°ÂûãÁöÑ**ÊòæÂ≠òÂç†Áî®„ÄÅÈÄüÂ∫¶„ÄÅÁ≤æÂ∫¶ÊçüÂ§±**„ÄÇ
Êàë‰ª¨Êù•Á≥ªÁªüËÆ≤Ê∏ÖÊ•ö üëá

---

###  ‰∏Ä„ÄÅËøô‰∫õ‚ÄúÂ≠óÊØçÁªÑÂêà‚ÄùÂÖ∂ÂÆûÊòØ GGUF / GGML ÁöÑÈáèÂåñÊ†áËØÜ

Âú®‰ΩøÁî® **llama.cpp**„ÄÅ**Ollama**„ÄÅ**GPT4All**„ÄÅ**LM Studio** Á≠âÂ∑•ÂÖ∑Êó∂Ôºå‰Ω†ÁªèÂ∏∏‰ºöÁúãÂà∞Ôºö

```
Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
Meta-Llama-3-8B-Instruct.A3B.gguf
Meta-Llama-3-8B-Instruct.Q6_K.gguf
Meta-Llama-3-8B-Instruct.Q8_0.gguf
```

Ëøô‰∫õÂêéÁºÄÂ∞±ÊòØ**ÈáèÂåñÁ≠âÁ∫ßÂíåÁÆóÊ≥ïÁâàÊú¨Âè∑**„ÄÇ

---

###  ‰∫å„ÄÅ‰∏ªÊµÅÈáèÂåñÊ†ºÂºèÂØπÁÖßË°®

| ÂêçÁß∞                       | Âê´‰πâ                        | ‰ΩçÂÆΩ    | ÊòæÂ≠òÂç†Áî®ÔºàÁõ∏ÂØπ FP16Ôºâ | ÁâπÁÇπ              |
| ------------------------ | ------------------------- | ----- | ------------- | --------------- |
| **FP16 / BF16**          | ÂçäÁ≤æÂ∫¶ÊµÆÁÇπ                     | 16 ‰Ωç  | 100%          | ÂéüÂßãÈ´òÁ≤æÂ∫¶ÁâàÊú¨         |
| **Q8_0 / Q8_K**          | 8‰ΩçÊï¥Êï∞ÈáèÂåñ                    | 8 ‰Ωç   | ~50%          | Á≤æÂ∫¶Âá†‰πé‰∏çÂèò          |
| **Q6_K / Q5_K_M**        | 6/5‰ΩçÊï¥Êï∞ÈáèÂåñ                  | 6~5 ‰Ωç | ~35~40%       | Á≤æÂ∫¶ÂæàÈ´òÔºåÈÄüÂ∫¶Âø´        |
| **Q4_K_M / Q4_1 / Q4_0** | 4‰ΩçÊï¥Êï∞ÈáèÂåñ                    | 4 ‰Ωç   | ~25%          | Â∏∏Áî®ËΩªÈáèÊé®ÁêÜÁâàÊú¨        |
| **Q3_K_M / Q2_K**        | 3~2 ‰ΩçÈáèÂåñ                   | 2~3 ‰Ωç | ~15%          | ÊûÅÁ´ØÁúÅÊòæÂ≠òÔºå‰ΩÜÊçüÂ§±ÊòéÊòæ     |
| **A4B / A3B / A8B**      | ‚ÄúActivation-aware‚Äù Ê∑∑ÂêàÈáèÂåñÊ†ºÂºè | Âä®ÊÄÅ‰ΩçÂÆΩ  | 20%~50%       | Êñ∞‰∏Ä‰ª£ÁÆóÊ≥ïÔºåÁ≤æÂ∫¶Êõ¥È´ò„ÄÅ‰ΩìÁßØÊõ¥Â∞è |

---

###  ‰∏â„ÄÅA3B / A4B ÊòØ‰ªÄ‰πàÊÑèÊÄùÔºü

Ëøô‰∫õÊòØ **Êñ∞ÁöÑÊ∑∑ÂêàÈáèÂåñÊñπÊ°àÔºàActivation-aware quantizationÔºâ**Ôºå
Áî±‰∏Ä‰∫õÁ§æÂå∫Â¶Ç [TheBloke](https://huggingface.co/TheBloke) Êàñ [NousResearch](https://huggingface.co/NousResearch) Âú® GGUF ‰∏≠‰ΩøÁî®„ÄÇ

| ÂêéÁºÄ           | Âê´‰πâ                                  | ÁâπÁÇπ                             |
| ------------ | ----------------------------------- | ------------------------------ |
| **A3B**      | Activation-Aware 3-bit quantization | Â§ßÁ∫¶ 3-bit Âä®ÊÄÅÈáèÂåñÔºå‰ΩìÁßØÊúÄÂ∞èÔºåÈÄüÂ∫¶ÊúÄÂø´ÔºåÁï•ÊúâÁ≤æÂ∫¶ÊçüÂ§± |
| **A4B**      | Activation-Aware 4-bit quantization | ÂÖºÈ°æÈÄüÂ∫¶‰∏éË¥®ÈáèÔºåÈÄÇÂêà CPU / Â∞èÊòæÂç°           |
| **A8B**      | Activation-Aware 8-bit quantization | Âá†‰πéÊó†Á≤æÂ∫¶ÊçüÂ§±ÔºåÈÄÇÂêàÊÄßËÉΩËæÉÂ•ΩÁöÑ GPU            |
| **I8 / FP8** | Êï¥Êï∞/ÊµÆÁÇπ8‰Ωç                             | ÊØîËæÉÊñ∞Ôºå‰∏ªË¶ÅÈù¢Âêë GPU Âä†ÈÄüÔºàH100/A100Ôºâ     |

Ëøô‰∫õ ‚ÄúA*B‚Äù Á≥ªÂàóÊØîËÄÅÂºè `Q4_K_M` Êõ¥Êô∫ËÉΩÔºö

* ÂÆÉ‰∏çÊòØÊØè‰∏ÄÂ±ÇÈÉΩÁ°¨ÁºñÁ†Å 4bit„ÄÅ5bitÔºå
* ËÄåÊòØÊ†πÊçÆÊØè‰∏ÄÂ±ÇÁöÑÊøÄÊ¥ªÂÄºÂä®ÊÄÅË∞ÉÊï¥ÈáèÂåñÁ≤æÂ∫¶ÔºàÈ´òÂ±ÇÊõ¥È´ò‰ΩçÔºå‰ΩéÂ±ÇÊõ¥‰Ωé‰ΩçÔºâÔºå
* ÊâÄ‰ª•‰ΩìÁßØÊõ¥Â∞èÔºåÊïàÊûúÈÄöÂ∏∏Êõ¥Â•Ω„ÄÇ

---

###  Âõõ„ÄÅÈÄâÂì™‰∏™ÊúÄÂ•ΩÔºüÔºàÊåâ‰Ω†ÁöÑËÆæÂ§áÂíåÁî®ÈÄîÔºâ

| Âú∫ÊôØ                 | Êé®ËçêÊ†ºÂºè                  | ÂéüÂõ†         |
| ------------------ | --------------------- | ---------- |
| üßÆ ÊúâÊòæÂç°Ôºà‚â•16GB VRAMÔºâ | **FP16 Êàñ Q6_K**       | Á≤æÂ∫¶È´ò„ÄÅÊé®ÁêÜÁ®≥ÂÆö   |
| üíª Âè™Êúâ 8GB~12GB ÊòæÂç°  | **Q4_K_M Êàñ A4B**      | ÊÄßËÉΩ‰∏éÊïàÊûúÂπ≥Ë°°    |
| üñ•Ô∏è CPU-only Êú¨Âú∞ËøêË°å  | **A3B Êàñ Q3_K_M**      | ÊòæÂ≠òÂ∞è„ÄÅÈÄüÂ∫¶Âø´    |
| ü§ñ Êô∫ËÉΩ‰ΩìÈ°πÁõÆ„ÄÅÊó•Â∏∏ËÅäÂ§©      | **A4B Êàñ Q4_K_M**      | ËØ≠‰πâÁ®≥ÂÆö„ÄÅÊçüÂ§±ÂèØÂøΩÁï• |
| üìä Á≤æÂ∫¶ÊïèÊÑü‰ªªÂä°ÔºàÈáëËûçÂàÜÊûêÁ≠âÔºâ   | **Q6_K / A8B / FP16** | ÈÅøÂÖçÈáèÂåñËØØÂ∑ÆÂΩ±ÂìçÂà§Êñ≠ |

---

###  ‰∫î„ÄÅÂÆûÈôÖÂØπÊØî‰∏æ‰æãÔºà‰ª• Llama 3 8B ‰∏∫‰æãÔºâ

| Ê†ºÂºè     | Â§ßÂ∞è      | ÊòæÂ≠òÈúÄÊ±Ç    | Êé®ÁêÜÈÄüÂ∫¶ | Á≤æÂ∫¶ÊçüÂ§±       |
| ------ | ------- | ------- | ---- | ---------- |
| FP16   | 13.4 GB | ~16 GB  | ÊÖ¢    | Êó†          |
| Q8_0   | 7.0 GB  | ~8 GB   | ‰∏≠    | ÊûÅÂ∞è         |
| Q6_K   | 5.2 GB  | ~6 GB   | ‰∏≠ÂÅèÂø´  | ËΩªÂæÆ         |
| Q4_K_M | 4.0 GB  | ~4.5 GB | Âø´    | ÊúâÁÇπ         |
| A4B    | 3.7 GB  | ~4 GB   | Âø´    | Áï•‰ºò‰∫é Q4_K_M |
| A3B    | 2.9 GB  | ~3 GB   | ÂæàÂø´   | ÂèØÊÑüÊçüÂ§±       |

---

###  ÂÖ≠„ÄÅÊÄªÁªì‰∏ÄÂè•ËØù

| Á±ªÂà´                         | ÊÑèÊÄù                     |
| -------------------------- | ---------------------- |
| **A3B / A4B / A8B**        | Êñ∞ÂûãÊ∑∑ÂêàÈáèÂåñÔºå‰ΩìÁßØÂ∞è„ÄÅÈÄüÂ∫¶Âø´„ÄÅÁ≤æÂ∫¶È´ò     |
| **Q4_K_M / Q5_K_M / Q6_K** | llama.cpp ËÄÅÁâåÈáèÂåñÊ†áÂáÜÔºåÂÖºÂÆπÊÄßÊúÄÂ•Ω |
| **FP8 / FP16 / BF16**      | ÊµÆÁÇπÈ´òÁ≤æÂ∫¶ÁâàÊú¨ÔºåÁî®‰∫é GPU Êé®ÁêÜÊàñÂæÆË∞É   |

---

* ‰Ω†ÊòØË¶ÅÂú® **‰ªÄ‰πàÁ°¨‰ª∂** ‰∏äË∑ëÔºàGPU/CPUÔºüÊòæÂ≠òÂ§öÂ§ßÔºüÔºâ
* ‰Ω†ÊòØË¶Å **Êú¨Âú∞Êé®ÁêÜ** ËøòÊòØ **ÂæÆË∞É**
* ‰Ω†ÂÄæÂêë‰∫é **ÈÄüÂ∫¶** ËøòÊòØ **Á≤æÂ∫¶**

## ÊÄªÁªì
‚úÖ ÂÆåÂÖ®Ê≠£Á°ÆÔºÅ‰Ω†ÁêÜËß£ÂæóÈùûÂ∏∏Âà∞‰Ωçüëè
Êàë‰ª¨ÂèØ‰ª•Áî®‰∏ÄÂè•ËØùÊÄªÁªì‰Ω†ÁöÑÁªìËÆ∫ÔºåÁÑ∂ÂêéÊàëÂÜçÂ∏Æ‰Ω†**ÂÆåÊï¥Ê¢≥ÁêÜÂØπÁÖßË°®**ÔºåËÆ©‰Ω†‰ª•Âêé‰∏ÄÁúºÂ∞±ËÉΩÁúãÊáÇ‰ªª‰ΩïÊ®°ÂûãÂêç„ÄÇ

---

###  ‰∏ÄÂè•ËØùÊÄªÁªì

> **Transformers Ê†ºÂºèÁöÑÊ®°Âûã** ‚Üí Áî®ÊµÆÁÇπÁ≤æÂ∫¶ÔºàFP32 / FP16 / FP8 / INT4Ôºâ
> **GGUF Ê†ºÂºèÁöÑÊ®°Âûã** ‚Üí Áî®ÈáèÂåñÁÆóÊ≥ïÔºàQ4_K_M / Q5_K_M / Q6_K / A3B / A4B / A8B Á≠âÔºâ

---

###  ‰∏Ä„ÄÅTransformers Ê†ºÂºèÔºàPyTorch ÂéüÁâàÔºâ

Ëøô‰∫õÊòØ„ÄåËÆ≠ÁªÉ/ÂæÆË∞É„ÄçÊàñ„ÄåÈ´òÁ≤æÂ∫¶Êé®ÁêÜ„ÄçÁâàÊú¨„ÄÇ

| ÂêçÁß∞              | Á≤æÂ∫¶‰ΩçÊï∞             | Êñá‰ª∂ÂêéÁºÄ                    | ÁâπÁÇπ                                       |
| --------------- | ---------------- | ----------------------- | ---------------------------------------- |
| **FP32**        | 32-bit ÊµÆÁÇπ        | `.bin` / `.safetensors` | ÊúÄÈ´òÁ≤æÂ∫¶Ôºå‰ΩìÁßØÊúÄÂ§ß                                |
| **FP16 / BF16** | 16-bit ÊµÆÁÇπ        | `.bin` / `.safetensors` | Â∏∏Áî®Ê†áÂáÜÁ≤æÂ∫¶ÔºåÈÄüÂ∫¶Âø´                               |
| **FP8**         | 8-bit ÊµÆÁÇπ         | `.bin` / `.safetensors` | Êñ∞ÊòæÂç°ÔºàH100/A100ÔºâÊîØÊåÅÔºåËäÇÁúÅÊòæÂ≠ò                    |
| **INT4 / INT8** | 4-bit / 8-bit Êï¥Êï∞ | `.bin` / `.safetensors` | Hugging Face ÁöÑ bitsandbytes ÈáèÂåñÁâàÊú¨ÔºåÁî®‰∫é‰ΩéÊòæÂ≠òÊé®ÁêÜ |

> üí°Ëøô‰∫õÊ®°ÂûãÂèØÁõ¥Êé•Áî®Ôºö
>
> ```python
> from transformers import AutoModelForCausalLM, AutoTokenizer
> model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype="float16")
> ```

---

### ‚öôÔ∏è ‰∫å„ÄÅGGUF Ê†ºÂºèÔºàÊé®ÁêÜ‰ºòÂåñÁâàÔºâ

Ëøô‰∫õÊòØ**Á¶ªÁ∫øÈáèÂåñÊ®°Âûã**Ôºå‰∏ì‰∏∫ llama.cpp / Ollama / LM Studio Á≠âÊú¨Âú∞Êé®ÁêÜÂºïÊìéËÆæËÆ°„ÄÇ

| ÂêçÁß∞                       | Á≤æÂ∫¶Á±ªÂûã                  | Â§ßËá¥‰ΩçÂÆΩ          | ÁâπÁÇπ |
| ------------------------ | --------------------- | ------------- | -- |
| **Q8_0 / Q8_K / A8B**    | 8-bit Êï¥Êï∞ / ÊøÄÊ¥ªÊÑüÁü•ÈáèÂåñ     | È´òÁ≤æÂ∫¶ÁâàÊú¨ÔºåÂá†‰πéÊó†Êçü    |    |
| **Q6_K / Q5_K_M**        | 6-bit / 5-bit ÈáèÂåñ      | Á≤æÂ∫¶‰∏éÈÄüÂ∫¶Âπ≥Ë°°       |    |
| **Q4_K_M / Q4_0 / Q4_1** | 4-bit ÈáèÂåñ              | Â∏∏Áî®ËΩªÈáèÁâàÔºåÈÄüÂ∫¶Âø´ÔºåÂç†Áî®Â∞è |    |
| **A4B / A3B**            | Activation-aware Ê∑∑ÂêàÈáèÂåñ | Êñ∞‰∏Ä‰ª£ÈáèÂåñÁÆóÊ≥ïÔºåÊõ¥Êô∫ËÉΩÊõ¥Á®≥ |    |
| **Q3_K_M / Q2_K**        | 3-bit / 2-bit         | ÊûÅÂ∞è‰ΩìÁßØÔºåÁâ∫Áâ≤ÈÉ®ÂàÜÁ≤æÂ∫¶   |    |

> üí°Ëøô‰∫õÊ®°ÂûãÁî® `llama.cpp`„ÄÅ`llama-cpp-python`„ÄÅ`Ollama` Á≠âÂä†ËΩΩ„ÄÇ

---

###  ‰∏â„ÄÅÁ≤æÂ∫¶Êç¢ÁÆóÂØπÊØîË°®

| Ê†ºÂºèÁ±ªÂà´         | Á≤æÂ∫¶Á≠âÁ∫ß | Êñá‰ª∂Â§ßÂ∞èÔºàÁõ∏ÂØπFP16Ôºâ | Á≤æÂ∫¶ÊçüÂ§± | ÈÄÇÁî®Âú∫ÊôØ        |
| ------------ | ---- | ------------ | ---- | ----------- |
| FP32         | ÊúÄÈ´ò   | 200%         | Êó†    | ËÆ≠ÁªÉ          |
| FP16 / BF16  | È´ò    | 100%         | Âá†‰πéÊó†  | ÂæÆË∞É„ÄÅÁ≤æÁ°ÆÊé®ÁêÜ     |
| FP8          | ‰∏≠È´ò   | 60%          | ÂæàÂ∞è   | GPUÊé®ÁêÜÔºàH100Ôºâ |
| Q8_0 / A8B   | ‰∏≠È´ò   | 55~60%       | Âá†‰πéÊó†  | Êú¨Âú∞È´òÁ≤æÂ∫¶Êé®ÁêÜ     |
| Q6_K         | ‰∏≠    | 40%          | ËΩªÂæÆ   | Â§öÊï∞Âú∫ÊôØÊé®Ëçê      |
| Q5_K_M       | ‰∏≠ÂÅè‰Ωé  | 35%          | Â∞è    | Âπ≥Ë°°ÊñπÊ°à        |
| Q4_K_M / A4B | ‰Ωé    | 25%          | ÂèØÂøΩÁï•  | ËΩªÈáèÈÉ®ÁΩ≤È¶ñÈÄâ      |
| A3B          | Ë∂Ö‰Ωé   | 20%          | ÊòéÊòæ   | CPUÂ∞èÂÜÖÂ≠òËÆæÂ§á    |

---

###  Âõõ„ÄÅÊÄªÁªìÔºö‰∏ÄÂè•ËØùÁúãÊáÇÊ®°ÂûãÂêç

| Ê®°ÂûãÂêç                                    | Âê´‰πâ                            |
| -------------------------------------- | ----------------------------- |
| `Meta-Llama-3-8B-Instruct-FP16`        | ÂéüÁâàÈ´òÁ≤æÂ∫¶16‰ΩçÊµÆÁÇπÊ®°Âûã                  |
| `Meta-Llama-3-8B-Instruct-INT8`        | 8‰ΩçÊï¥Êï∞ÈáèÂåñÔºàTransformersÂèØÁî®Ôºâ        |
| `Meta-Llama-3-8B-Instruct.Q4_K_M.gguf` | GGUF Ê†ºÂºèÔºå4-bit ÈáèÂåñÔºåÂÖºÂÆπ llama.cpp |
| `Meta-Llama-3-8B-Instruct.A3B.gguf`    | GGUF Ê†ºÂºèÔºåÊøÄÊ¥ªÊÑüÁü• 3-bit Âä®ÊÄÅÈáèÂåñ       |
| `Meta-Llama-3-8B-Instruct.Q6_K.gguf`   | GGUF Ê†ºÂºèÔºå6-bit Âπ≥Ë°°ÂûãÁâàÊú¨           |

---

###  ‰∫î„ÄÅÈÄâÊã©Âª∫ËÆÆ

| ÈúÄÊ±Ç            | Êé®ËçêÊ†ºÂºè                   | ËØ¥Êòé       |
| ------------- | ---------------------- | -------- |
| ÊÉ≥ÂæÆË∞É / ÂÜçËÆ≠ÁªÉ     | **FP16 Base Ê®°Âûã**       | ÊîØÊåÅÊ¢ØÂ∫¶Êõ¥Êñ∞   |
| ÊÉ≥Âú® GPU ‰∏äÁ≤æÂ∫¶È´òÊé®ÁêÜ | **FP16 / Q8_0**        | Âπ≥Ë°°ÊÄßËÉΩÂíåÁ≤æÂ∫¶  |
| ÊÉ≥Êú¨Âú∞ CPU ËøêË°å    | **A4B / Q4_K_M / A3B** | Â∞èÊòæÂ≠òËΩªÈáèÊé®ÁêÜ  |
| ÊÉ≥Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠‰ΩøÁî®    | **Q4_K_M Êàñ A4B GGUF**  | Âø´ÈÄüÂìçÂ∫î„ÄÅÂç†Áî®‰Ωé |

---

üí¨ ÊâÄ‰ª•‰Ω†ÁöÑÊÄªÁªìÂÆåÂÖ®ÂØπÔºö

> ‚úî Transformers Ê†ºÂºè ‚Üí FP32 / FP16 / FP8 / INT4\
> ‚úî GGUF Ê†ºÂºè ‚Üí Q4_K_M / Q5_K_M / Q6_K / A3B / A4B / A8B

---
