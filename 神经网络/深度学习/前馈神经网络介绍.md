## 前馈神经网络

**前馈神经网络**

前馈神经⽹络（feedforward neural network）是⼀种简单的神经⽹络，也被称为[多层感知机](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=多层感知机&zhida_source=entity)（multi-layer perceptron，简称MLP），其中不同的神经元属于不同的层，由输⼊层-隐藏层-输出层构成，信号从输⼊层往输出层单向传递，中间无反馈，其目的是为了拟合某个函数，由⼀个[有向无环图](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=有向无环图&zhida_source=entity)表示，如下：

![img](https://picx.zhimg.com/80/v2-e30896ae3b1e27a4cebea7e68d711b40_720w.webp?source=2c26e567)

前馈神经⽹络中包含[激活函数](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=激活函数&zhida_source=entity)（sigmoid函数、[tanh函数](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=tanh函数&zhida_source=entity)等）、损失函数（均⽅差损失函数、[交叉熵损失函数](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=交叉熵损失函数&zhida_source=entity)等）、优化算法（BP算法）等。常⽤的模型结构有：感知机、BP神经⽹络、全连接神经⽹络、[卷积神经⽹络](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=卷积神经⽹络&zhida_source=entity)、RBF神经⽹络等。**模型，学习准则，优化方法**

**感知器**

感知器，也可翻译为感知机，是Frank Rosenblatt在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种[人工神经网络](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=人工神经网络&zhida_source=entity)。它可以被视为一种最简单形式的[前馈式人工神经网络](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=前馈式人工神经网络&zhida_source=entity)，是一种**二元线性分类器**。

感知机的主要特点是结构简单，对所能解决的问题存在着收敛算法，并能从数学上严格证明，从而对神经网络研究起了重要的推动作用。

![image-20250107172009340](../../Image/image-20250107172009340.png)

**BP神经网络**

BP(back propagation)神经网络是1986年由Rumelhart和McClelland为首的科学家提出的概念，是一种按照误差逆向传播算法训练的[多层前馈神经网络](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=多层前馈神经网络&zhida_source=entity)，是应用最广泛的神经网络模型之一 。

BP算法的基本思想是：学习过程由信号的[正向传播](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=正向传播&zhida_source=entity)和误差的反向传播两个过程组成。

正向传播时，把样本的特征从输入层进行输入，信号经过各个隐藏层的处理后，最后从输出层传出。对于网络的实际的输出与期望输出之间的误差，把误差信号从最后一层逐层反传，从而获得各个层的误差学习信号，然后再根据误差学习信号来修正各层神经元的权值。

**全连接神经网络**

全连接神经网络是一种多层的感知机结构。每一层的每一个节点都与上下层节点全部连接，这就是”全连接“的由来。整个全连接神经网络分为输入层，隐藏层和输出层，其中隐藏层可以更好的分离数据的特征，但是过多的隐藏层会导致[过拟合](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=过拟合&zhida_source=entity)问题。

## 反馈神经网络

反馈神经网络（feedback neural network）的输出不仅与当前输入以及网络权重有关，还和网络之前的输入有关。它是一个有向循环图或是[无向图](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=无向图&zhida_source=entity)，具有很强的联想记忆能力和优化计算能力。

常⽤的模型结构有：RNN、Hopfield⽹络、[受限玻尔兹曼机](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=受限玻尔兹曼机&zhida_source=entity)、LSTM等。

**循环神经网络**

**循环神经网络（recurrent neural network，简称RNN）**是指在全连接神经网络的基础上增加了前后时序上的关系，可以更好地处理比如机器翻译等的与时序相关的问题。

RNN之所以称为循环神经网络，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。

RNN用于解决训练样本输入是连续的序列,且序列的长短不一的问题，比如基于时间序列的问题。基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。

**Hopfield⽹络**

**Hopfield神经网络**是一种单层互相全连接的反馈型神经网络。每个神经元既是输入也是输出，网络中的每一个神经元都将自己的输出通过连接权传送给所有其它神经元，同时又都接收所有其它神经元传递过来的信息。即：网络中的神经元在t时刻的输出状态实际上间接地与自己t-1时刻的输出状态有关。神经元之间互连接，所以得到的权重矩阵将是对称矩阵。

**受限玻尔兹曼机**

玻尔兹曼机是一大类的神经网络模型，但是在实际应用中使用最多的则是受限玻尔兹曼机（RBM）。

受限玻尔兹曼机（RBM）是一个随机神经网络（即当网络的神经元节点被激活时会有随机行为，随机取值）。它包含一层可视层和一层隐藏层。在同一层的神经元之间是相互独立的，而在不同的网络层之间的神经元是相互连接的（双向连接）。在网络进行训练以及使用时信息会在两个方向上流动，而且两个方向上的权值是相同的。但是偏置值是不同的（偏置值的个数是和神经元的个数相同的）。

**LSTM**

长短期记忆网络 LSTM（long short-term memory）是 RNN 的一种变体，其核心概念在于细胞状态以及“门”结构。细胞状态相当于信息传输的路径，让信息能在序列连中传递下去。你可以将其看作网络的“记忆”。理论上讲，细胞状态能够将序列处理过程中的相关信息一直传递下去。因此，即使是较早时间步长的信息也能携带到较后时间步长的细胞中来，这克服了短时记忆的影响。信息的添加和移除我们通过“门”结构来实现，“门”结构在训练过程中会去学习该保存或遗忘哪些信息。

## 图神经网络

图（graph）是⼀种在[拓扑空间](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=拓扑空间&zhida_source=entity)内按图结构组织来关系推理的函数集合，包括社交⽹络、[知识图谱](https://zhida.zhihu.com/search?content_id=537642554&content_type=Answer&match_order=1&q=知识图谱&zhida_source=entity)、分⼦图神经⽹络等。



在 PyTorch 的线性层（`nn.Linear`）中，权重矩阵的维度顺序可能与你手动计算时的直觉相反，这是导致你感到困惑的主要原因。我来解释一下这个现象背后的原理，并提供代码示例帮助你理解。

### 1. 为什么需要 "转置"？

在数学中，线性变换通常表示为 y = Wx + b，其中：

- x 是输入向量（维度：`[input_size, 1]`）
- W 是权重矩阵（维度：`[output_size, input_size]`）
- y 是输出向量（维度：`[output_size, 1]`）

但在 PyTorch 中，`nn.Linear` 的权重矩阵 W 的维度是 `[output_size, input_size]`，而输入 x 的维度通常是 `[batch_size, input_size]`。此时，PyTorch 会自动执行 y = xW^T + b（注意这里的 W^T 是权重矩阵的转置）。

这种设计有两个主要原因：

1. 批量处理：PyTorch 需要同时处理多个样本（batch），因此输入维度是 `[batch_size, input_size]`。
2. 内存连续性：按行存储数据（C 风格）更符合计算机内存访问模式，提高计算效率。

### 2. 代码示例：验证权重维度

以下代码可以帮助你理解 `nn.Linear` 的权重矩阵维度：

```python
import torch
import torch.nn as nn

# 创建一个线性层：input_size=3, output_size=5
linear = nn.Linear(3, 5)

# 查看权重矩阵和偏置的维度
print("权重矩阵 W 的维度:", linear.weight.shape)  # 输出: [5, 3]
print("偏置向量 b 的维度:", linear.bias.shape)    # 输出: [5]

# 创建一个输入样本（batch_size=2）
x = torch.randn(2, 3)  # 维度: [2, 3]

# 前向传播
y = linear(x)  # 等价于 y = x @ W^T + b
print("输入 x 的维度:", x.shape)         # 输出: [2, 3]
print("输出 y 的维度:", y.shape)         # 输出: [2, 5]
print("手动计算 y = x @ W^T + b:")
manual_y = x @ linear.weight.t() + linear.bias
print(torch.allclose(y, manual_y))  # 输出: True
```

### 3. 动计算时的正确写法

如果你习惯按数学公式 y = Wx + b 手动计算，需要注意：

1. 将 PyTorch 的权重矩阵 W 转置为 W^T（维度：`[input_size, output_size]`）。
2. 将输入向量 x 保持为列向量（维度：`[input_size, 1]`）。

```python
# 手动计算单个样本（非批量）
x_single = torch.randn(3)  # 维度: [3]（PyTorch中为一维张量，表示向量）

# 手动实现 y = Wx + b
W_transposed = linear.weight.t()  # W^T，维度: [3, 5]
b = linear.bias                   # 维度: [5]

# 注意：x_single 需要转为列向量 [3, 1]
x_column = x_single.unsqueeze(1)  # 维度: [3, 1]
y_manual = W_transposed @ x_column + b.unsqueeze(1)  # 维度: [5, 1]

# 与PyTorch的输出对比
y_pytorch = linear(x_single)  # 维度: [5]
print("手动计算结果与PyTorch结果一致:", torch.allclose(y_manual.squeeze(), y_pytorch))
```

 

### 4. 总结

- PyTorch 实现：

  ```
  y = x @ W^T + b
  ```

  其中：

  - x 维度：`[batch_size, input_size]`
  - W 维度：`[output_size, input_size]`
  - W^T 维度：`[input_size, output_size]`

- 数学公式：

  ```
  y = Wx + b
  ```

  其中：

  - x 维度：`[input_size, 1]`
  - W 维度：`[output_size, input_size]`

这种差异是为了适应批量计算和内存优化，理解后就不需要每次都手动转置了！
