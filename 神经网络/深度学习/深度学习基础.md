# 深度学习基础

## 全连接网络(MLP,ANN)

**有标签监督学习（one hot向量）的数据去训练模型**

**无监督学习，只给原始训练集（训练集），让计算机自行抽取抽象特征得出隐式变量，常用于聚类分析等问题**

**强化学习是不给数据和结果，让计算机自行交互计算**

**数值化->标准化->扁平化**

## 卷积网络(CNN)

**扩充->特征提取（使用卷积核）->最大池化->扁平化**

## 循环网络

### 普通循环网络(RNN)

![image-20240313151015196](../../Image/image-20240313151015196.png)

### 长短期记忆网络LSTM

长短期记忆网络（Long Short-Term Memory，LSTM）是一种特殊类型的循环神经网络（Recurrent Neural Network，RNN），由Hochreiter和Schmidhuber在1997年提出。它被设计用来解决传统RNN中的长期依赖问题，即在处理长序列数据时，传统RNN的梯度消失或梯度爆炸问题。

LSTM通过引入称为“门控”（gate）的机制来实现对信息的选择性保留和遗忘。它包含了三个门：遗忘门（forget gate）、输入门（input gate）和输出门（output gate），以及一个单元状态（cell state）。这些门控机制允许LSTM根据当前输入和之前的状态来决定何时遗忘信息、何时接受新信息以及何时输出信息。

以下是LSTM的基本组件：

1. **单元状态（cell state）**：
   单元状态是LSTM网络中贯穿整个时间步的信息流的主要载体。它可以看作是LSTM内部的记忆单元，可以在长时间范围内保持和传递信息。

2. **遗忘门（forget gate）**：
   遗忘门决定了在当前时间步应该丢弃多少之前的单元状态信息。它根据当前输入和前一个时间步的隐藏状态来计算一个介于0和1之间的值，表示应该保留的信息量。当遗忘门输出接近0时，表示应该完全忘记先前的单元状态；当遗忘门输出接近1时，表示应该保留大部分先前的单元状态。

3. **输入门（input gate）**：
   输入门决定了在当前时间步应该更新多少新的信息到单元状态中。它通过对当前输入和前一个时间步的隐藏状态进行操作，生成一个介于0和1之间的值，表示应该更新的信息量。

4. **输出门（output gate）**：
   输出门决定了在当前时间步应该输出多少信息。它根据当前输入和前一个时间步的隐藏状态来计算一个介于0和1之间的值，表示应该输出的信息量。

LSTM的这些门控机制使其能够更有效地处理长序列数据，并且在很多序列建模任务中取得了很好的表现，如语言建模、机器翻译、语音识别等。

### GRU

门控循环单元（Gated Recurrent Unit，GRU）是另一种常用于处理序列数据的循环神经网络（Recurrent Neural Network，RNN）的变种，由Cho等人于2014年提出。与长短期记忆网络（LSTM）相比，GRU具有更简单的结构，但在某些任务上表现出类似或更好的性能。

GRU包含了更新门（update gate）和重置门（reset gate）两个门，通过这些门的控制，GRU可以更灵活地管理信息的流动。

以下是GRU的基本组件：

1. **更新门（Update Gate）**：
   更新门决定了当前时间步的输入和前一个时间步的隐藏状态之间有多少信息要被更新到当前时间步的隐藏状态中。它的输出值介于0和1之间，用于控制信息的保留和遗忘。

2. **重置门（Reset Gate）**：
   重置门决定了如何将前一个时间步的隐藏状态与当前时间步的输入结合起来。它的输出值也在0到1之间，用于控制前一个时间步的隐藏状态有多少信息被忽略。

3. **当前时间步的候选隐藏状态**：
   候选隐藏状态是由重置门控制的前一个时间步的隐藏状态和当前时间步的输入共同计算得到的。它提供了一个备选的隐藏状态，用于计算更新门。

4. **当前时间步的隐藏状态**：
   当前时间步的隐藏状态是由更新门控制的前一个时间步的隐藏状态和候选隐藏状态之间进行插值得到的。它是GRU的主要输出，用于传递信息到下一个时间步。

GRU的设计相对简单，参数较少，计算效率较高，因此在一些场景下被广泛应用，尤其是当数据量较小或计算资源有限时。它在语言建模、机器翻译、语音识别等任务中取得了很好的表现，并且相对于LSTM来说，更容易训练和调试。

## 注意力机制和Transformer

当谈到注意力机制（Attention Mechanism）和Transformer时，通常是指自然语言处理（NLP）领域中的关键概念。以下是它们的简要介绍：

1. **注意力机制（Attention Mechanism）**：
   注意力机制是一种模拟人类视觉或语言处理中的注意力机制的方式，它允许模型在处理序列数据时，根据序列中不同部分的重要性来分配不同的权重。在自然语言处理中，一个词可能在理解句子的时候比其他词更重要。因此，注意力机制允许模型在处理每个词或序列时，动态地学习并关注输入序列的不同部分。这种动态的关注机制使得模型能够更好地捕捉序列数据中的长距离依赖关系，并且在各种NLP任务中表现出色，如机器翻译、语言建模和文本摘要等。

2. **Transformer**：
   Transformer是一种基于注意力机制的深度学习模型，由Vaswani等人在2017年提出。它被广泛应用于自然语言处理任务，特别是机器翻译。Transformer取消了传统循环神经网络（RNN）和卷积神经网络（CNN）的结构，在序列数据处理中采用了完全基于注意力机制的架构。Transformer包括编码器（Encoder）和解码器（Decoder）两个部分，每个部分都由多个相同的层堆叠而成。在编码器中，输入序列的每个词都通过自注意力机制进行编码，以便模型可以考虑输入序列的全局关系。在解码器中，除了自注意力机制，还使用编码器-解码器注意力机制来捕捉输入序列和输出序列之间的对应关系，从而实现翻译等任务。

总的来说，注意力机制允许模型根据输入数据的不同部分分配不同的重要性，而Transformer则是基于注意力机制构建的深度学习模型，在自然语言处理领域取得了巨大成功。

## 处理文本Bert模型

**input -> tokenizer(分词) -> word to vec（把分开的词映射成一个向量）**

![image-20240312175042245](../../Image/image-20240312175042245.png)

理解BERT的内部原理包括几个关键部分：分词器（Tokenizer）、嵌入（Embedding）、编码器（Encoder）等。这些步骤是BERT模型处理文本的核心。下面详细解释每个步骤。

### 1. Tokenizer（分词器）

BERT使用基于WordPiece的分词器，它能够将文本拆分成子词单位，使得它可以处理未见过的单词。分词器的主要步骤包括：

- **文本清理**：对输入文本进行基本清理，如转换为小写（对于uncased模型）。
- **词汇表匹配**：将文本匹配到预训练的词汇表中，找到最长的子词。
- **特殊标记**：在文本的开头和结尾分别加上`[CLS]`和`[SEP]`标记。

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "Hello, how are you?"
tokens = tokenizer.tokenize(text)
#分词
print(tokens)  # ['hello', ',', 'how', 'are', 'you', '?']

#查看input_id
# 假设我们有以下 token ids
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print(input_ids)  # [7592, 1010, 2129, 2024, 2017, 1029]
```

**input_ids**
  在BERT模型及其衍生体中，输入文本首先经过一个分词处理流程，其中文本被细分为单词或子单词（subwords），每个分词随后映射到一个唯一的整数标识符。这些标识符组成了所谓的input_ids数组，其代表文本的数字化形式。为了适应模型处理的需要，input_ids的长度被规范化为一个固定的值。在这个规范化过程中，长度超出预定值的输入会被截断，而短于此长度的输入则通过添加特定的填充标记（[PAD]，通常对应的整数标识符为0）来补齐。这种处理机制确保了模型输入的一致性，允许模型批量处理不同长度的文本数据。

**token_type_ids**：0表示第一个句子，1表示第二个句子。

**attention_mask**

 与input_ids并行的，attention_mask数组标识了模型应当"关注"的输入部分。具体而言，attention_mask对于实际文本内容的位置赋值为1，而对于填充部分则赋值为0。这使得模型能够区分原始文本与为了长度规范化而添加的填充内容，从而仅对有意义的文本部分进行分析。attention_mask在处理可变长文本输入时尤其关键，因为它直接指导模型聚焦于重要的信息，忽视那些无关紧要的填充部分。

  综上所述，input_ids为文本提供了一种高效的数字化表示，而attention_mask则确保模型能够在处理这些数字化信息时，有效地识别并专注于实质内容，排除无关的填充影响。这两个参数共同构成了模型处理文本信息的基础，对于保证模型的性能和分析精度至关重要。**(相当于有三个数组)**

### 2. Embedding（嵌入层）---Word2Vec

**CBOW**

**Skip-Gram**

BERT的嵌入层由三部分组成：

- **Token Embeddings**：将每个词标记转换为一个向量表示。token embedding 层是要将各个词转换成固定维度的向量。在BERT中，每个词会被转换成768维的向量表示

  在BERT模型中，**词标记（Token）**是模型处理自然语言的最小单位。词标记（Token）可以是一个完整的单词，也可以是一个单词的子词，甚至是字符片段。这取决于BERT使用的分词方法。BERT使用的是**WordPiece分词算法**，这种分词方法将文本分解成子词单元，以便能够处理未见过的单词和词汇。

- **Segment Embeddings**：用于区分不同的句子（用于句对任务）。Segment Embeddings 层只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token, 后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment embedding就是全0。

- **Position Embeddings**：添加位置信息，帮助模型理解词序。 BERT能够处理最长512个token的输入序列。作者通过让BERT在各个位置上**学习一个向量表示**来将序列顺序的信息编码进来。这意味着Position Embeddings layer 实际上就是一个大小为 (512, 768) 的lookup表，表的第一行是代表序列的第一个位置，第二行代表序列的第二个位置，以此类推。因此，如果有这样两个句子“Hello world” 和“Hi there”, “Hello” 和“Hi”会由完全相同的position embeddings，因为他们都是句子的第一个词。同理，“world” 和“there”也会有相同的position embedding。

这些嵌入向量会被相加，然后输入到编码器中。

```python
# 假设我们有以下 token ids
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print(input_ids)  # [7592, 1010, 2129, 2024, 2017, 1029]

# 嵌入示例（实际代码会更复杂，这里只是示意）（开始嵌入）
import torch
#定义嵌入层的三个部分
token_embeddings = torch.nn.Embedding(len(tokenizer.vocab), 768)
segment_embeddings = torch.nn.Embedding(2, 768)
position_embeddings = torch.nn.Embedding(512, 768)

input_embeddings = token_embeddings(torch.tensor(input_ids))
segment_ids = torch.zeros_like(torch.tensor(input_ids))
position_ids = torch.arange(len(input_ids)).unsqueeze(0)

#将三个部分相加，得到嵌入层的完整向量
embedding_output = input_embeddings + segment_embeddings(segment_ids) + position_embeddings(position_ids)
```

### 3. Encoder（编码器）

BERT的核心是Transformer编码器，由多层堆叠的Self-Attention和前馈神经网络组成。**每层编码器都包括**：

- **Self-Attention机制**：计算每个词与其他词的注意力权重，帮助模型关注上下文。
- **前馈神经网络**：对Self-Attention的输出进行进一步处理。

这些层通过残差连接和Layer Normalization结合在一起。

```python
# 自注意力机制（简化示意）
def scaled_dot_product_attention(query, key, value):
    scores = torch.matmul(query, key.transpose(-2, -1)) / (key.size(-1) ** 0.5)
    weights = torch.nn.functional.softmax(scores, dim=-1)
    output = torch.matmul(weights, value)
    return output

# 假设输入 embedding_output 已经计算好
# 通常，query, key, value 是从 embedding_output 线性变换得到
query, key, value = embedding_output, embedding_output, embedding_output
attention_output = scaled_dot_product_attention(query, key, value)

# 前馈神经网络（简化示意）
ffn_output = torch.nn.ReLU()(torch.nn.Linear(768, 3072)(attention_output))
ffn_output = torch.nn.Linear(3072, 768)(ffn_output)
```

### 4. BERT 输出

BERT 的输出包括：

- **[CLS] 向量**：用于句子级任务（如分类）。
- **其他标记向量**：用于标记级任务（如命名实体识别）。

```python
# 获取 [CLS] 向量
cls_vector = embedding_output[:, 0, :]  # 假设 embedding_output 是最终编码层的输出

# 分类任务示例
classification_head = torch.nn.Linear(768, 2)  # 二分类
logits = classification_head(cls_vector)
```

### 总结

BERT模型通过以下几个步骤处理文本：

1. **分词**：将文本转换为子词标记。
2. **嵌入**：将标记转换为嵌入向量，并加上位置和段落信息。
3. **编码**：使用多层Transformer编码器处理嵌入向量，生成上下文敏感的表示。
4. **输出**：使用编码器输出进行下游任务，如分类、问答等。

通过这些步骤，BERT能够理解和处理复杂的自然语言任务。

---

### SEP 标记的作用

==在BERT模型中，[SEP]标记具有多种功能，主要用于区分和分割不同的句子或文本段。下面详细解释[SEP]标记的作用及其在不同任务中的应用。==

1. **句子分隔**：在句子对任务中（如句子对分类或问答任务），`[SEP]`标记用于分隔两个句子或文本段。
2. **句子结束**：标记一个句子的结束，即使只有一个句子，也会在结尾加上`[SEP]`。

#### 示例

**句子对分类任务**

在句子对分类任务中（如自然语言推理任务），模型需要判断两个句子之间的关系（如推断、对比等）。输入格式如下：

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

BERT会用`[SEP]`来分隔句子A和句子B。

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# 两个示例句子
sentence_a = "How are you?"
sentence_b = "I am fine, thank you."

# 分词并添加特殊标记
tokens = tokenizer(sentence_a, sentence_b, return_tensors='pt')
print(tokens)
```

输出的字典包含了`input_ids`、`token_type_ids`和`attention_mask`，这些会被输入到BERT模型中。

**单句任务**

即使只有一个句子，`[SEP]`也会在结尾处加上，用于标记句子的结束。

```python
single_sentence = "How are you?"

tokens = tokenizer(single_sentence, return_tensors='pt')
print(tokens)
```

输出同样会包含`[CLS]`和`[SEP]`标记。

#### SEP标记在BERT中的处理

`[SEP]`标记在嵌入时也会有对应的Token Embedding、Segment Embedding和Position Embedding。特别是对于Segment Embedding，BERT使用`token_type_ids`来区分不同句子：

- **token_type_ids**：0表示第一个句子，1表示第二个句子。

例如：

```python
from transformers import BertModel

model = BertModel.from_pretrained('bert-base-uncased')

# 获取输入张量
input_ids = tokens['input_ids']
token_type_ids = tokens['token_type_ids']
attention_mask = tokens['attention_mask']

# 前向传播
outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

# 获取[CLS]向量
cls_output = outputs.last_hidden_state[:, 0, :]
print(cls_output)
```

通过上述示例，BERT能够区分和处理不同的句子和文本段，`[SEP]`标记在其中起到了关键作用，确保模型能够正确理解输入的结构和内容。

#### 总结

- **`[SEP]`标记用于区分句子或文本段**：在句子对任务中，`[SEP]`分隔句子A和句子B；在单句任务中，`[SEP]`标记句子的结束。
- **处理和编码**：BERT通过`token_type_ids`区分不同的句子部分，并为每个标记（包括`[SEP]`）生成相应的嵌入向量。

通过这些机制，BERT能够高效地处理和理解复杂的自然语言输入。

## 建模形式

确实，**因果语言模型（Causal LM）** 只是 Transformer 的一种建模方式。在实际 NLP 任务中，根据输入输出的结构不同，**Transformer 有几种主要的建模范式（架构类型）**：

### 一、Causal LM（因果语言模型）

**代表模型：** GPT、Qwen、LLaMA、ChatGLM、Baichuan 等
**架构：** 仅用 **Decoder**，并且带有 **Mask Attention**（每个 token 只能看到前面的 token）

#### 特点：

- 输入是部分文本（prompt）
- 模型逐字预测下一个 token
- 用于：**生成类任务**（聊天、续写、编程、总结）

#### 举例：

> 输入：「我今天很」 → 模型预测：「开心」

#### 损失函数：

- CrossEntropy(next_token_logits, target_token)

------

### 📘 二、Masked LM（掩码语言模型）

**代表模型：** BERT、RoBERTa、LayoutLM、DeBERTa
**架构：** 仅用 **Encoder**，全注意力（双向）

#### 特点：

- 在输入中随机 mask 一部分 token
- 模型预测这些被 mask 的 token
- 用于：**理解类任务**（分类、抽取、句子相似度）

#### 举例：

> 输入：「我今天很[MASK]」 → 模型预测：「开心」

#### 损失函数：

- CrossEntropy(预测的 mask token logits, 原始 token)

------

### 📗 三、Seq2Seq（Encoder-Decoder）

**代表模型：** T5、BART、mT5、Whisper
**架构：** Encoder + Decoder

- Encoder 读完整输入（双向注意力）
- Decoder 按顺序生成输出（因果注意力）

#### 特点：

- 输入和输出结构不同
- 用于：**翻译、摘要、问答、代码生成**

#### 举例：

> 输入：「把''我今天很开心''翻译成英文」
>  输出：「I am very happy today」
