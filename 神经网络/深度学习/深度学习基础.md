## å…¨è¿æ¥ç½‘ç»œ(MLP,ANN)

**æœ‰æ ‡ç­¾ç›‘ç£å­¦ä¹ ï¼ˆone hotå‘é‡ï¼‰çš„æ•°æ®å»è®­ç»ƒæ¨¡å‹**

**æ— ç›‘ç£å­¦ä¹ ï¼Œåªç»™åŸå§‹è®­ç»ƒé›†ï¼ˆè®­ç»ƒé›†ï¼‰ï¼Œè®©è®¡ç®—æœºè‡ªè¡ŒæŠ½å–æŠ½è±¡ç‰¹å¾å¾—å‡ºéšå¼å˜é‡ï¼Œå¸¸ç”¨äºèšç±»åˆ†æç­‰é—®é¢˜**

**å¼ºåŒ–å­¦ä¹ æ˜¯ä¸ç»™æ•°æ®å’Œç»“æœï¼Œè®©è®¡ç®—æœºè‡ªè¡Œäº¤äº’è®¡ç®—**

**æ•°å€¼åŒ–->æ ‡å‡†åŒ–->æ‰å¹³åŒ–**

## å·ç§¯ç½‘ç»œ(CNN)

**æ‰©å……->ç‰¹å¾æå–ï¼ˆä½¿ç”¨å·ç§¯æ ¸ï¼‰->æœ€å¤§æ± åŒ–->æ‰å¹³åŒ–**

## å¾ªç¯ç½‘ç»œ

### æ™®é€šå¾ªç¯ç½‘ç»œ(RNN)

![image-20240313151015196](../../Image/image-20240313151015196.png)

### é•¿çŸ­æœŸè®°å¿†ç½‘ç»œLSTM

é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLong Short-Term Memoryï¼ŒLSTMï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networkï¼ŒRNNï¼‰ï¼Œç”±Hochreiterå’ŒSchmidhuberåœ¨1997å¹´æå‡ºã€‚å®ƒè¢«è®¾è®¡ç”¨æ¥è§£å†³ä¼ ç»ŸRNNä¸­çš„é•¿æœŸä¾èµ–é—®é¢˜ï¼Œå³åœ¨å¤„ç†é•¿åºåˆ—æ•°æ®æ—¶ï¼Œä¼ ç»ŸRNNçš„æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚

LSTMé€šè¿‡å¼•å…¥ç§°ä¸ºâ€œé—¨æ§â€ï¼ˆgateï¼‰çš„æœºåˆ¶æ¥å®ç°å¯¹ä¿¡æ¯çš„é€‰æ‹©æ€§ä¿ç•™å’Œé—å¿˜ã€‚å®ƒåŒ…å«äº†ä¸‰ä¸ªé—¨ï¼šé—å¿˜é—¨ï¼ˆforget gateï¼‰ã€è¾“å…¥é—¨ï¼ˆinput gateï¼‰å’Œè¾“å‡ºé—¨ï¼ˆoutput gateï¼‰ï¼Œä»¥åŠä¸€ä¸ªå•å…ƒçŠ¶æ€ï¼ˆcell stateï¼‰ã€‚è¿™äº›é—¨æ§æœºåˆ¶å…è®¸LSTMæ ¹æ®å½“å‰è¾“å…¥å’Œä¹‹å‰çš„çŠ¶æ€æ¥å†³å®šä½•æ—¶é—å¿˜ä¿¡æ¯ã€ä½•æ—¶æ¥å—æ–°ä¿¡æ¯ä»¥åŠä½•æ—¶è¾“å‡ºä¿¡æ¯ã€‚

ä»¥ä¸‹æ˜¯LSTMçš„åŸºæœ¬ç»„ä»¶ï¼š

1. **å•å…ƒçŠ¶æ€ï¼ˆcell stateï¼‰**ï¼š
   å•å…ƒçŠ¶æ€æ˜¯LSTMç½‘ç»œä¸­è´¯ç©¿æ•´ä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯æµçš„ä¸»è¦è½½ä½“ã€‚å®ƒå¯ä»¥çœ‹ä½œæ˜¯LSTMå†…éƒ¨çš„è®°å¿†å•å…ƒï¼Œå¯ä»¥åœ¨é•¿æ—¶é—´èŒƒå›´å†…ä¿æŒå’Œä¼ é€’ä¿¡æ¯ã€‚

2. **é—å¿˜é—¨ï¼ˆforget gateï¼‰**ï¼š
   é—å¿˜é—¨å†³å®šäº†åœ¨å½“å‰æ—¶é—´æ­¥åº”è¯¥ä¸¢å¼ƒå¤šå°‘ä¹‹å‰çš„å•å…ƒçŠ¶æ€ä¿¡æ¯ã€‚å®ƒæ ¹æ®å½“å‰è¾“å…¥å’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ¥è®¡ç®—ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„å€¼ï¼Œè¡¨ç¤ºåº”è¯¥ä¿ç•™çš„ä¿¡æ¯é‡ã€‚å½“é—å¿˜é—¨è¾“å‡ºæ¥è¿‘0æ—¶ï¼Œè¡¨ç¤ºåº”è¯¥å®Œå…¨å¿˜è®°å…ˆå‰çš„å•å…ƒçŠ¶æ€ï¼›å½“é—å¿˜é—¨è¾“å‡ºæ¥è¿‘1æ—¶ï¼Œè¡¨ç¤ºåº”è¯¥ä¿ç•™å¤§éƒ¨åˆ†å…ˆå‰çš„å•å…ƒçŠ¶æ€ã€‚

3. **è¾“å…¥é—¨ï¼ˆinput gateï¼‰**ï¼š
   è¾“å…¥é—¨å†³å®šäº†åœ¨å½“å‰æ—¶é—´æ­¥åº”è¯¥æ›´æ–°å¤šå°‘æ–°çš„ä¿¡æ¯åˆ°å•å…ƒçŠ¶æ€ä¸­ã€‚å®ƒé€šè¿‡å¯¹å½“å‰è¾“å…¥å’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€è¿›è¡Œæ“ä½œï¼Œç”Ÿæˆä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„å€¼ï¼Œè¡¨ç¤ºåº”è¯¥æ›´æ–°çš„ä¿¡æ¯é‡ã€‚

4. **è¾“å‡ºé—¨ï¼ˆoutput gateï¼‰**ï¼š
   è¾“å‡ºé—¨å†³å®šäº†åœ¨å½“å‰æ—¶é—´æ­¥åº”è¯¥è¾“å‡ºå¤šå°‘ä¿¡æ¯ã€‚å®ƒæ ¹æ®å½“å‰è¾“å…¥å’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ¥è®¡ç®—ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„å€¼ï¼Œè¡¨ç¤ºåº”è¯¥è¾“å‡ºçš„ä¿¡æ¯é‡ã€‚

LSTMçš„è¿™äº›é—¨æ§æœºåˆ¶ä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†é•¿åºåˆ—æ•°æ®ï¼Œå¹¶ä¸”åœ¨å¾ˆå¤šåºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­å–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ï¼Œå¦‚è¯­è¨€å»ºæ¨¡ã€æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚

### GRU

é—¨æ§å¾ªç¯å•å…ƒï¼ˆGated Recurrent Unitï¼ŒGRUï¼‰æ˜¯å¦ä¸€ç§å¸¸ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networkï¼ŒRNNï¼‰çš„å˜ç§ï¼Œç”±Choç­‰äººäº2014å¹´æå‡ºã€‚ä¸é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç›¸æ¯”ï¼ŒGRUå…·æœ‰æ›´ç®€å•çš„ç»“æ„ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç±»ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚

GRUåŒ…å«äº†æ›´æ–°é—¨ï¼ˆupdate gateï¼‰å’Œé‡ç½®é—¨ï¼ˆreset gateï¼‰ä¸¤ä¸ªé—¨ï¼Œé€šè¿‡è¿™äº›é—¨çš„æ§åˆ¶ï¼ŒGRUå¯ä»¥æ›´çµæ´»åœ°ç®¡ç†ä¿¡æ¯çš„æµåŠ¨ã€‚

ä»¥ä¸‹æ˜¯GRUçš„åŸºæœ¬ç»„ä»¶ï¼š

1. **æ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰**ï¼š
   æ›´æ–°é—¨å†³å®šäº†å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥å’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä¹‹é—´æœ‰å¤šå°‘ä¿¡æ¯è¦è¢«æ›´æ–°åˆ°å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä¸­ã€‚å®ƒçš„è¾“å‡ºå€¼ä»‹äº0å’Œ1ä¹‹é—´ï¼Œç”¨äºæ§åˆ¶ä¿¡æ¯çš„ä¿ç•™å’Œé—å¿˜ã€‚

2. **é‡ç½®é—¨ï¼ˆReset Gateï¼‰**ï¼š
   é‡ç½®é—¨å†³å®šäº†å¦‚ä½•å°†å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä¸å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ç»“åˆèµ·æ¥ã€‚å®ƒçš„è¾“å‡ºå€¼ä¹Ÿåœ¨0åˆ°1ä¹‹é—´ï¼Œç”¨äºæ§åˆ¶å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æœ‰å¤šå°‘ä¿¡æ¯è¢«å¿½ç•¥ã€‚

3. **å½“å‰æ—¶é—´æ­¥çš„å€™é€‰éšè—çŠ¶æ€**ï¼š
   å€™é€‰éšè—çŠ¶æ€æ˜¯ç”±é‡ç½®é—¨æ§åˆ¶çš„å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€å’Œå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥å…±åŒè®¡ç®—å¾—åˆ°çš„ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¤‡é€‰çš„éšè—çŠ¶æ€ï¼Œç”¨äºè®¡ç®—æ›´æ–°é—¨ã€‚

4. **å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€**ï¼š
   å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ˜¯ç”±æ›´æ–°é—¨æ§åˆ¶çš„å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€å’Œå€™é€‰éšè—çŠ¶æ€ä¹‹é—´è¿›è¡Œæ’å€¼å¾—åˆ°çš„ã€‚å®ƒæ˜¯GRUçš„ä¸»è¦è¾“å‡ºï¼Œç”¨äºä¼ é€’ä¿¡æ¯åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ã€‚

GRUçš„è®¾è®¡ç›¸å¯¹ç®€å•ï¼Œå‚æ•°è¾ƒå°‘ï¼Œè®¡ç®—æ•ˆç‡è¾ƒé«˜ï¼Œå› æ­¤åœ¨ä¸€äº›åœºæ™¯ä¸‹è¢«å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶æ˜¯å½“æ•°æ®é‡è¾ƒå°æˆ–è®¡ç®—èµ„æºæœ‰é™æ—¶ã€‚å®ƒåœ¨è¯­è¨€å»ºæ¨¡ã€æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ä¸­å–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ï¼Œå¹¶ä¸”ç›¸å¯¹äºLSTMæ¥è¯´ï¼Œæ›´å®¹æ˜“è®­ç»ƒå’Œè°ƒè¯•ã€‚

## æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer

å½“è°ˆåˆ°æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰å’ŒTransformeræ—¶ï¼Œé€šå¸¸æ˜¯æŒ‡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä¸­çš„å…³é”®æ¦‚å¿µã€‚ä»¥ä¸‹æ˜¯å®ƒä»¬çš„ç®€è¦ä»‹ç»ï¼š

1. **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰**ï¼š
   æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§æ¨¡æ‹Ÿäººç±»è§†è§‰æˆ–è¯­è¨€å¤„ç†ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶çš„æ–¹å¼ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶ï¼Œæ ¹æ®åºåˆ—ä¸­ä¸åŒéƒ¨åˆ†çš„é‡è¦æ€§æ¥åˆ†é…ä¸åŒçš„æƒé‡ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œä¸€ä¸ªè¯å¯èƒ½åœ¨ç†è§£å¥å­çš„æ—¶å€™æ¯”å…¶ä»–è¯æ›´é‡è¦ã€‚å› æ­¤ï¼Œæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†æ¯ä¸ªè¯æˆ–åºåˆ—æ—¶ï¼ŒåŠ¨æ€åœ°å­¦ä¹ å¹¶å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚è¿™ç§åŠ¨æ€çš„å…³æ³¨æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åºåˆ—æ•°æ®ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”åœ¨å„ç§NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡å’Œæ–‡æœ¬æ‘˜è¦ç­‰ã€‚

2. **Transformer**ï¼š
   Transformeræ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”±Vaswaniç­‰äººåœ¨2017å¹´æå‡ºã€‚å®ƒè¢«å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯æœºå™¨ç¿»è¯‘ã€‚Transformerå–æ¶ˆäº†ä¼ ç»Ÿå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç»“æ„ï¼Œåœ¨åºåˆ—æ•°æ®å¤„ç†ä¸­é‡‡ç”¨äº†å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„ã€‚TransformeråŒ…æ‹¬ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½ç”±å¤šä¸ªç›¸åŒçš„å±‚å †å è€Œæˆã€‚åœ¨ç¼–ç å™¨ä¸­ï¼Œè¾“å…¥åºåˆ—çš„æ¯ä¸ªè¯éƒ½é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç¼–ç ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥è€ƒè™‘è¾“å…¥åºåˆ—çš„å…¨å±€å…³ç³»ã€‚åœ¨è§£ç å™¨ä¸­ï¼Œé™¤äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿˜ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚

æ€»çš„æ¥è¯´ï¼Œæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹æ ¹æ®è¾“å…¥æ•°æ®çš„ä¸åŒéƒ¨åˆ†åˆ†é…ä¸åŒçš„é‡è¦æ€§ï¼Œè€ŒTransformeråˆ™æ˜¯åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ„å»ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚

## å¤„ç†æ–‡æœ¬Bertæ¨¡å‹

**input -> tokenizer(åˆ†è¯) -> word to vecï¼ˆæŠŠåˆ†å¼€çš„è¯æ˜ å°„æˆä¸€ä¸ªå‘é‡ï¼‰**

![image-20240312175042245](../../Image/image-20240312175042245.png)

ç†è§£BERTçš„å†…éƒ¨åŸç†åŒ…æ‹¬å‡ ä¸ªå…³é”®éƒ¨åˆ†ï¼šåˆ†è¯å™¨ï¼ˆTokenizerï¼‰ã€åµŒå…¥ï¼ˆEmbeddingï¼‰ã€ç¼–ç å™¨ï¼ˆEncoderï¼‰ç­‰ã€‚è¿™äº›æ­¥éª¤æ˜¯BERTæ¨¡å‹å¤„ç†æ–‡æœ¬çš„æ ¸å¿ƒã€‚ä¸‹é¢è¯¦ç»†è§£é‡Šæ¯ä¸ªæ­¥éª¤ã€‚

### 1. Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰

BERTä½¿ç”¨åŸºäºWordPieceçš„åˆ†è¯å™¨ï¼Œå®ƒèƒ½å¤Ÿå°†æ–‡æœ¬æ‹†åˆ†æˆå­è¯å•ä½ï¼Œä½¿å¾—å®ƒå¯ä»¥å¤„ç†æœªè§è¿‡çš„å•è¯ã€‚åˆ†è¯å™¨çš„ä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼š

- **æ–‡æœ¬æ¸…ç†**ï¼šå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡ŒåŸºæœ¬æ¸…ç†ï¼Œå¦‚è½¬æ¢ä¸ºå°å†™ï¼ˆå¯¹äºuncasedæ¨¡å‹ï¼‰ã€‚
- **è¯æ±‡è¡¨åŒ¹é…**ï¼šå°†æ–‡æœ¬åŒ¹é…åˆ°é¢„è®­ç»ƒçš„è¯æ±‡è¡¨ä¸­ï¼Œæ‰¾åˆ°æœ€é•¿çš„å­è¯ã€‚
- **ç‰¹æ®Šæ ‡è®°**ï¼šåœ¨æ–‡æœ¬çš„å¼€å¤´å’Œç»“å°¾åˆ†åˆ«åŠ ä¸Š`[CLS]`å’Œ`[SEP]`æ ‡è®°ã€‚

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "Hello, how are you?"
tokens = tokenizer.tokenize(text)
#åˆ†è¯
print(tokens)  # ['hello', ',', 'how', 'are', 'you', '?']

#æŸ¥çœ‹input_id
# å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹ token ids
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print(input_ids)  # [7592, 1010, 2129, 2024, 2017, 1029]
```

**input_ids**
â€ƒâ€ƒåœ¨BERTæ¨¡å‹åŠå…¶è¡ç”Ÿä½“ä¸­ï¼Œè¾“å…¥æ–‡æœ¬é¦–å…ˆç»è¿‡ä¸€ä¸ªåˆ†è¯å¤„ç†æµç¨‹ï¼Œå…¶ä¸­æ–‡æœ¬è¢«ç»†åˆ†ä¸ºå•è¯æˆ–å­å•è¯ï¼ˆsubwordsï¼‰ï¼Œæ¯ä¸ªåˆ†è¯éšåæ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°æ ‡è¯†ç¬¦ã€‚è¿™äº›æ ‡è¯†ç¬¦ç»„æˆäº†æ‰€è°“çš„input_idsæ•°ç»„ï¼Œå…¶ä»£è¡¨æ–‡æœ¬çš„æ•°å­—åŒ–å½¢å¼ã€‚ä¸ºäº†é€‚åº”æ¨¡å‹å¤„ç†çš„éœ€è¦ï¼Œinput_idsçš„é•¿åº¦è¢«è§„èŒƒåŒ–ä¸ºä¸€ä¸ªå›ºå®šçš„å€¼ã€‚åœ¨è¿™ä¸ªè§„èŒƒåŒ–è¿‡ç¨‹ä¸­ï¼Œé•¿åº¦è¶…å‡ºé¢„å®šå€¼çš„è¾“å…¥ä¼šè¢«æˆªæ–­ï¼Œè€ŒçŸ­äºæ­¤é•¿åº¦çš„è¾“å…¥åˆ™é€šè¿‡æ·»åŠ ç‰¹å®šçš„å¡«å……æ ‡è®°ï¼ˆ[PAD]ï¼Œé€šå¸¸å¯¹åº”çš„æ•´æ•°æ ‡è¯†ç¬¦ä¸º0ï¼‰æ¥è¡¥é½ã€‚è¿™ç§å¤„ç†æœºåˆ¶ç¡®ä¿äº†æ¨¡å‹è¾“å…¥çš„ä¸€è‡´æ€§ï¼Œå…è®¸æ¨¡å‹æ‰¹é‡å¤„ç†ä¸åŒé•¿åº¦çš„æ–‡æœ¬æ•°æ®ã€‚

**token_type_ids**ï¼š0è¡¨ç¤ºç¬¬ä¸€ä¸ªå¥å­ï¼Œ1è¡¨ç¤ºç¬¬äºŒä¸ªå¥å­ã€‚

**attention_mask**

â€ƒä¸input_idså¹¶è¡Œçš„ï¼Œattention_maskæ•°ç»„æ ‡è¯†äº†æ¨¡å‹åº”å½“"å…³æ³¨"çš„è¾“å…¥éƒ¨åˆ†ã€‚å…·ä½“è€Œè¨€ï¼Œattention_maskå¯¹äºå®é™…æ–‡æœ¬å†…å®¹çš„ä½ç½®èµ‹å€¼ä¸º1ï¼Œè€Œå¯¹äºå¡«å……éƒ¨åˆ†åˆ™èµ‹å€¼ä¸º0ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒºåˆ†åŸå§‹æ–‡æœ¬ä¸ä¸ºäº†é•¿åº¦è§„èŒƒåŒ–è€Œæ·»åŠ çš„å¡«å……å†…å®¹ï¼Œä»è€Œä»…å¯¹æœ‰æ„ä¹‰çš„æ–‡æœ¬éƒ¨åˆ†è¿›è¡Œåˆ†æã€‚attention_maskåœ¨å¤„ç†å¯å˜é•¿æ–‡æœ¬è¾“å…¥æ—¶å°¤å…¶å…³é”®ï¼Œå› ä¸ºå®ƒç›´æ¥æŒ‡å¯¼æ¨¡å‹èšç„¦äºé‡è¦çš„ä¿¡æ¯ï¼Œå¿½è§†é‚£äº›æ— å…³ç´§è¦çš„å¡«å……éƒ¨åˆ†ã€‚

â€ƒâ€ƒç»¼ä¸Šæ‰€è¿°ï¼Œinput_idsä¸ºæ–‡æœ¬æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ•°å­—åŒ–è¡¨ç¤ºï¼Œè€Œattention_maskåˆ™ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†è¿™äº›æ•°å­—åŒ–ä¿¡æ¯æ—¶ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å¹¶ä¸“æ³¨äºå®è´¨å†…å®¹ï¼Œæ’é™¤æ— å…³çš„å¡«å……å½±å“ã€‚è¿™ä¸¤ä¸ªå‚æ•°å…±åŒæ„æˆäº†æ¨¡å‹å¤„ç†æ–‡æœ¬ä¿¡æ¯çš„åŸºç¡€ï¼Œå¯¹äºä¿è¯æ¨¡å‹çš„æ€§èƒ½å’Œåˆ†æç²¾åº¦è‡³å…³é‡è¦ã€‚**(ç›¸å½“äºæœ‰ä¸‰ä¸ªæ•°ç»„)**

### 2. Embeddingï¼ˆåµŒå…¥å±‚ï¼‰---Word2Vec

**CBOW**

**Skip-Gram**

BERTçš„åµŒå…¥å±‚ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š

- **Token Embeddings**ï¼šå°†æ¯ä¸ªè¯æ ‡è®°è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚token embedding å±‚æ˜¯è¦å°†å„ä¸ªè¯è½¬æ¢æˆå›ºå®šç»´åº¦çš„å‘é‡ã€‚åœ¨BERTä¸­ï¼Œæ¯ä¸ªè¯ä¼šè¢«è½¬æ¢æˆ768ç»´çš„å‘é‡è¡¨ç¤º

  åœ¨BERTæ¨¡å‹ä¸­ï¼Œ**è¯æ ‡è®°ï¼ˆTokenï¼‰**æ˜¯æ¨¡å‹å¤„ç†è‡ªç„¶è¯­è¨€çš„æœ€å°å•ä½ã€‚è¯æ ‡è®°ï¼ˆTokenï¼‰å¯ä»¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„å•è¯ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå•è¯çš„å­è¯ï¼Œç”šè‡³æ˜¯å­—ç¬¦ç‰‡æ®µã€‚è¿™å–å†³äºBERTä½¿ç”¨çš„åˆ†è¯æ–¹æ³•ã€‚BERTä½¿ç”¨çš„æ˜¯**WordPieceåˆ†è¯ç®—æ³•**ï¼Œè¿™ç§åˆ†è¯æ–¹æ³•å°†æ–‡æœ¬åˆ†è§£æˆå­è¯å•å…ƒï¼Œä»¥ä¾¿èƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„å•è¯å’Œè¯æ±‡ã€‚

- **Segment Embeddings**ï¼šç”¨äºåŒºåˆ†ä¸åŒçš„å¥å­ï¼ˆç”¨äºå¥å¯¹ä»»åŠ¡ï¼‰ã€‚Segment Embeddings å±‚åªæœ‰ä¸¤ç§å‘é‡è¡¨ç¤ºã€‚å‰ä¸€ä¸ªå‘é‡æ˜¯æŠŠ0èµ‹ç»™ç¬¬ä¸€ä¸ªå¥å­ä¸­çš„å„ä¸ªtoken, åä¸€ä¸ªå‘é‡æ˜¯æŠŠ1èµ‹ç»™ç¬¬äºŒä¸ªå¥å­ä¸­çš„å„ä¸ªtokenã€‚å¦‚æœè¾“å…¥ä»…ä»…åªæœ‰ä¸€ä¸ªå¥å­ï¼Œé‚£ä¹ˆå®ƒçš„segment embeddingå°±æ˜¯å…¨0ã€‚

- **Position Embeddings**ï¼šæ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£è¯åºã€‚ BERTèƒ½å¤Ÿå¤„ç†æœ€é•¿512ä¸ªtokençš„è¾“å…¥åºåˆ—ã€‚ä½œè€…é€šè¿‡è®©BERTåœ¨å„ä¸ªä½ç½®ä¸Š**å­¦ä¹ ä¸€ä¸ªå‘é‡è¡¨ç¤º**æ¥å°†åºåˆ—é¡ºåºçš„ä¿¡æ¯ç¼–ç è¿›æ¥ã€‚è¿™æ„å‘³ç€Position Embeddings layer å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªå¤§å°ä¸º (512, 768) çš„lookupè¡¨ï¼Œè¡¨çš„ç¬¬ä¸€è¡Œæ˜¯ä»£è¡¨åºåˆ—çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œç¬¬äºŒè¡Œä»£è¡¨åºåˆ—çš„ç¬¬äºŒä¸ªä½ç½®ï¼Œä»¥æ­¤ç±»æ¨ã€‚å› æ­¤ï¼Œå¦‚æœæœ‰è¿™æ ·ä¸¤ä¸ªå¥å­â€œHello worldâ€ å’Œâ€œHi thereâ€, â€œHelloâ€ å’Œâ€œHiâ€ä¼šç”±å®Œå…¨ç›¸åŒçš„position embeddingsï¼Œå› ä¸ºä»–ä»¬éƒ½æ˜¯å¥å­çš„ç¬¬ä¸€ä¸ªè¯ã€‚åŒç†ï¼Œâ€œworldâ€ å’Œâ€œthereâ€ä¹Ÿä¼šæœ‰ç›¸åŒçš„position embeddingã€‚

è¿™äº›åµŒå…¥å‘é‡ä¼šè¢«ç›¸åŠ ï¼Œç„¶åè¾“å…¥åˆ°ç¼–ç å™¨ä¸­ã€‚

```python
# å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹ token ids
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print(input_ids)  # [7592, 1010, 2129, 2024, 2017, 1029]

# åµŒå…¥ç¤ºä¾‹ï¼ˆå®é™…ä»£ç ä¼šæ›´å¤æ‚ï¼Œè¿™é‡Œåªæ˜¯ç¤ºæ„ï¼‰ï¼ˆå¼€å§‹åµŒå…¥ï¼‰
import torch
#å®šä¹‰åµŒå…¥å±‚çš„ä¸‰ä¸ªéƒ¨åˆ†
token_embeddings = torch.nn.Embedding(len(tokenizer.vocab), 768)
segment_embeddings = torch.nn.Embedding(2, 768)
position_embeddings = torch.nn.Embedding(512, 768)

input_embeddings = token_embeddings(torch.tensor(input_ids))
segment_ids = torch.zeros_like(torch.tensor(input_ids))
position_ids = torch.arange(len(input_ids)).unsqueeze(0)

#å°†ä¸‰ä¸ªéƒ¨åˆ†ç›¸åŠ ï¼Œå¾—åˆ°åµŒå…¥å±‚çš„å®Œæ•´å‘é‡
embedding_output = input_embeddings + segment_embeddings(segment_ids) + position_embeddings(position_ids)
```

### 3. Encoderï¼ˆç¼–ç å™¨ï¼‰

BERTçš„æ ¸å¿ƒæ˜¯Transformerç¼–ç å™¨ï¼Œç”±å¤šå±‚å †å çš„Self-Attentionå’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚**æ¯å±‚ç¼–ç å™¨éƒ½åŒ…æ‹¬**ï¼š

- **Self-Attentionæœºåˆ¶**ï¼šè®¡ç®—æ¯ä¸ªè¯ä¸å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨ä¸Šä¸‹æ–‡ã€‚
- **å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼šå¯¹Self-Attentionçš„è¾“å‡ºè¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚

è¿™äº›å±‚é€šè¿‡æ®‹å·®è¿æ¥å’ŒLayer Normalizationç»“åˆåœ¨ä¸€èµ·ã€‚

```python
# è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆç®€åŒ–ç¤ºæ„ï¼‰
def scaled_dot_product_attention(query, key, value):
    scores = torch.matmul(query, key.transpose(-2, -1)) / (key.size(-1) ** 0.5)
    weights = torch.nn.functional.softmax(scores, dim=-1)
    output = torch.matmul(weights, value)
    return output

# å‡è®¾è¾“å…¥ embedding_output å·²ç»è®¡ç®—å¥½
# é€šå¸¸ï¼Œquery, key, value æ˜¯ä» embedding_output çº¿æ€§å˜æ¢å¾—åˆ°
query, key, value = embedding_output, embedding_output, embedding_output
attention_output = scaled_dot_product_attention(query, key, value)

# å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆç®€åŒ–ç¤ºæ„ï¼‰
ffn_output = torch.nn.ReLU()(torch.nn.Linear(768, 3072)(attention_output))
ffn_output = torch.nn.Linear(3072, 768)(ffn_output)
```

### 4. BERT è¾“å‡º

BERT çš„è¾“å‡ºåŒ…æ‹¬ï¼š

- **[CLS] å‘é‡**ï¼šç”¨äºå¥å­çº§ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰ã€‚
- **å…¶ä»–æ ‡è®°å‘é‡**ï¼šç”¨äºæ ‡è®°çº§ä»»åŠ¡ï¼ˆå¦‚å‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚

```python
# è·å– [CLS] å‘é‡
cls_vector = embedding_output[:, 0, :]  # å‡è®¾ embedding_output æ˜¯æœ€ç»ˆç¼–ç å±‚çš„è¾“å‡º

# åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹
classification_head = torch.nn.Linear(768, 2)  # äºŒåˆ†ç±»
logits = classification_head(cls_vector)
```

### æ€»ç»“

BERTæ¨¡å‹é€šè¿‡ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤å¤„ç†æ–‡æœ¬ï¼š

1. **åˆ†è¯**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå­è¯æ ‡è®°ã€‚
2. **åµŒå…¥**ï¼šå°†æ ‡è®°è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼Œå¹¶åŠ ä¸Šä½ç½®å’Œæ®µè½ä¿¡æ¯ã€‚
3. **ç¼–ç **ï¼šä½¿ç”¨å¤šå±‚Transformerç¼–ç å™¨å¤„ç†åµŒå…¥å‘é‡ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡æ•æ„Ÿçš„è¡¨ç¤ºã€‚
4. **è¾“å‡º**ï¼šä½¿ç”¨ç¼–ç å™¨è¾“å‡ºè¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€é—®ç­”ç­‰ã€‚

é€šè¿‡è¿™äº›æ­¥éª¤ï¼ŒBERTèƒ½å¤Ÿç†è§£å’Œå¤„ç†å¤æ‚çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚

---

### SEP æ ‡è®°çš„ä½œç”¨

==åœ¨BERTæ¨¡å‹ä¸­ï¼Œ[SEP]æ ‡è®°å…·æœ‰å¤šç§åŠŸèƒ½ï¼Œä¸»è¦ç”¨äºåŒºåˆ†å’Œåˆ†å‰²ä¸åŒçš„å¥å­æˆ–æ–‡æœ¬æ®µã€‚ä¸‹é¢è¯¦ç»†è§£é‡Š[SEP]æ ‡è®°çš„ä½œç”¨åŠå…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚==

1. **å¥å­åˆ†éš”**ï¼šåœ¨å¥å­å¯¹ä»»åŠ¡ä¸­ï¼ˆå¦‚å¥å­å¯¹åˆ†ç±»æˆ–é—®ç­”ä»»åŠ¡ï¼‰ï¼Œ`[SEP]`æ ‡è®°ç”¨äºåˆ†éš”ä¸¤ä¸ªå¥å­æˆ–æ–‡æœ¬æ®µã€‚
2. **å¥å­ç»“æŸ**ï¼šæ ‡è®°ä¸€ä¸ªå¥å­çš„ç»“æŸï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå¥å­ï¼Œä¹Ÿä¼šåœ¨ç»“å°¾åŠ ä¸Š`[SEP]`ã€‚

#### ç¤ºä¾‹

**å¥å­å¯¹åˆ†ç±»ä»»åŠ¡**

åœ¨å¥å­å¯¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ˆå¦‚è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼‰ï¼Œæ¨¡å‹éœ€è¦åˆ¤æ–­ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼ˆå¦‚æ¨æ–­ã€å¯¹æ¯”ç­‰ï¼‰ã€‚è¾“å…¥æ ¼å¼å¦‚ä¸‹ï¼š

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

BERTä¼šç”¨`[SEP]`æ¥åˆ†éš”å¥å­Aå’Œå¥å­Bã€‚

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# ä¸¤ä¸ªç¤ºä¾‹å¥å­
sentence_a = "How are you?"
sentence_b = "I am fine, thank you."

# åˆ†è¯å¹¶æ·»åŠ ç‰¹æ®Šæ ‡è®°
tokens = tokenizer(sentence_a, sentence_b, return_tensors='pt')
print(tokens)
```

è¾“å‡ºçš„å­—å…¸åŒ…å«äº†`input_ids`ã€`token_type_ids`å’Œ`attention_mask`ï¼Œè¿™äº›ä¼šè¢«è¾“å…¥åˆ°BERTæ¨¡å‹ä¸­ã€‚

**å•å¥ä»»åŠ¡**

å³ä½¿åªæœ‰ä¸€ä¸ªå¥å­ï¼Œ`[SEP]`ä¹Ÿä¼šåœ¨ç»“å°¾å¤„åŠ ä¸Šï¼Œç”¨äºæ ‡è®°å¥å­çš„ç»“æŸã€‚

```python
single_sentence = "How are you?"

tokens = tokenizer(single_sentence, return_tensors='pt')
print(tokens)
```

è¾“å‡ºåŒæ ·ä¼šåŒ…å«`[CLS]`å’Œ`[SEP]`æ ‡è®°ã€‚

#### SEPæ ‡è®°åœ¨BERTä¸­çš„å¤„ç†

`[SEP]`æ ‡è®°åœ¨åµŒå…¥æ—¶ä¹Ÿä¼šæœ‰å¯¹åº”çš„Token Embeddingã€Segment Embeddingå’ŒPosition Embeddingã€‚ç‰¹åˆ«æ˜¯å¯¹äºSegment Embeddingï¼ŒBERTä½¿ç”¨`token_type_ids`æ¥åŒºåˆ†ä¸åŒå¥å­ï¼š

- **token_type_ids**ï¼š0è¡¨ç¤ºç¬¬ä¸€ä¸ªå¥å­ï¼Œ1è¡¨ç¤ºç¬¬äºŒä¸ªå¥å­ã€‚

ä¾‹å¦‚ï¼š

```python
from transformers import BertModel

model = BertModel.from_pretrained('bert-base-uncased')

# è·å–è¾“å…¥å¼ é‡
input_ids = tokens['input_ids']
token_type_ids = tokens['token_type_ids']
attention_mask = tokens['attention_mask']

# å‰å‘ä¼ æ’­
outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

# è·å–[CLS]å‘é‡
cls_output = outputs.last_hidden_state[:, 0, :]
print(cls_output)
```

é€šè¿‡ä¸Šè¿°ç¤ºä¾‹ï¼ŒBERTèƒ½å¤ŸåŒºåˆ†å’Œå¤„ç†ä¸åŒçš„å¥å­å’Œæ–‡æœ¬æ®µï¼Œ`[SEP]`æ ‡è®°åœ¨å…¶ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®ç†è§£è¾“å…¥çš„ç»“æ„å’Œå†…å®¹ã€‚

#### æ€»ç»“

- **`[SEP]`æ ‡è®°ç”¨äºåŒºåˆ†å¥å­æˆ–æ–‡æœ¬æ®µ**ï¼šåœ¨å¥å­å¯¹ä»»åŠ¡ä¸­ï¼Œ`[SEP]`åˆ†éš”å¥å­Aå’Œå¥å­Bï¼›åœ¨å•å¥ä»»åŠ¡ä¸­ï¼Œ`[SEP]`æ ‡è®°å¥å­çš„ç»“æŸã€‚
- **å¤„ç†å’Œç¼–ç **ï¼šBERTé€šè¿‡`token_type_ids`åŒºåˆ†ä¸åŒçš„å¥å­éƒ¨åˆ†ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ‡è®°ï¼ˆåŒ…æ‹¬`[SEP]`ï¼‰ç”Ÿæˆç›¸åº”çš„åµŒå…¥å‘é‡ã€‚

é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒBERTèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†å’Œç†è§£å¤æ‚çš„è‡ªç„¶è¯­è¨€è¾“å…¥ã€‚

## å»ºæ¨¡å½¢å¼

ç¡®å®ï¼Œ**å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal LMï¼‰** åªæ˜¯ Transformer çš„ä¸€ç§å»ºæ¨¡æ–¹å¼ã€‚åœ¨å®é™… NLP ä»»åŠ¡ä¸­ï¼Œæ ¹æ®è¾“å…¥è¾“å‡ºçš„ç»“æ„ä¸åŒï¼Œ**Transformer æœ‰å‡ ç§ä¸»è¦çš„å»ºæ¨¡èŒƒå¼ï¼ˆæ¶æ„ç±»å‹ï¼‰**ï¼š

### ä¸€ã€Causal LMï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰

**ä»£è¡¨æ¨¡å‹ï¼š** GPTã€Qwenã€LLaMAã€ChatGLMã€Baichuan ç­‰
**æ¶æ„ï¼š** ä»…ç”¨ **Decoder**ï¼Œå¹¶ä¸”å¸¦æœ‰ **Mask Attention**ï¼ˆæ¯ä¸ª token åªèƒ½çœ‹åˆ°å‰é¢çš„ tokenï¼‰

#### ç‰¹ç‚¹ï¼š

- è¾“å…¥æ˜¯éƒ¨åˆ†æ–‡æœ¬ï¼ˆpromptï¼‰
- æ¨¡å‹é€å­—é¢„æµ‹ä¸‹ä¸€ä¸ª token
- ç”¨äºï¼š**ç”Ÿæˆç±»ä»»åŠ¡**ï¼ˆèŠå¤©ã€ç»­å†™ã€ç¼–ç¨‹ã€æ€»ç»“ï¼‰

#### ä¸¾ä¾‹ï¼š

> è¾“å…¥ï¼šã€Œæˆ‘ä»Šå¤©å¾ˆã€ â†’ æ¨¡å‹é¢„æµ‹ï¼šã€Œå¼€å¿ƒã€

#### æŸå¤±å‡½æ•°ï¼š

- CrossEntropy(next_token_logits, target_token)

------

### ğŸ“˜ äºŒã€Masked LMï¼ˆæ©ç è¯­è¨€æ¨¡å‹ï¼‰

**ä»£è¡¨æ¨¡å‹ï¼š** BERTã€RoBERTaã€LayoutLMã€DeBERTa
**æ¶æ„ï¼š** ä»…ç”¨ **Encoder**ï¼Œå…¨æ³¨æ„åŠ›ï¼ˆåŒå‘ï¼‰

#### ç‰¹ç‚¹ï¼š

- åœ¨è¾“å…¥ä¸­éšæœº mask ä¸€éƒ¨åˆ† token
- æ¨¡å‹é¢„æµ‹è¿™äº›è¢« mask çš„ token
- ç”¨äºï¼š**ç†è§£ç±»ä»»åŠ¡**ï¼ˆåˆ†ç±»ã€æŠ½å–ã€å¥å­ç›¸ä¼¼åº¦ï¼‰

#### ä¸¾ä¾‹ï¼š

> è¾“å…¥ï¼šã€Œæˆ‘ä»Šå¤©å¾ˆ[MASK]ã€ â†’ æ¨¡å‹é¢„æµ‹ï¼šã€Œå¼€å¿ƒã€

#### æŸå¤±å‡½æ•°ï¼š

- CrossEntropy(é¢„æµ‹çš„ mask token logits, åŸå§‹ token)

------

### ğŸ“— ä¸‰ã€Seq2Seqï¼ˆEncoder-Decoderï¼‰

**ä»£è¡¨æ¨¡å‹ï¼š** T5ã€BARTã€mT5ã€Whisper
**æ¶æ„ï¼š** Encoder + Decoder

- Encoder è¯»å®Œæ•´è¾“å…¥ï¼ˆåŒå‘æ³¨æ„åŠ›ï¼‰
- Decoder æŒ‰é¡ºåºç”Ÿæˆè¾“å‡ºï¼ˆå› æœæ³¨æ„åŠ›ï¼‰

#### ç‰¹ç‚¹ï¼š

- è¾“å…¥å’Œè¾“å‡ºç»“æ„ä¸åŒ
- ç”¨äºï¼š**ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ã€ä»£ç ç”Ÿæˆ**

#### ä¸¾ä¾‹ï¼š

> è¾“å…¥ï¼šã€ŒæŠŠ''æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒ''ç¿»è¯‘æˆè‹±æ–‡ã€
>  è¾“å‡ºï¼šã€ŒI am very happy todayã€
