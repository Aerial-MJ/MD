## tokenizer(turn_str, add_special_tokens=False)

### 使用场景

在Transformer模型输入的文本中常常会额外使用一些特殊[token]来表示一些特殊含义，比如希望对LLM通过设计prompt提升下游任务效果。

最开始在Bert预训练文本中就约定俗成用[CLS]表示句子开头、[SEP]表示隔开两个句子的符号、[UNK]表示未登录词、[PAD]表示该位置补充0。同时将特殊token加入模型词汇表vocab后可以防止tokenzier在切词时将该[token]切开，在文本编码时保留该[token]为一个整体。

常见使用场景在Bert表征文本时，你需要再使用一些特殊的[token]加入输入的文本中，能起到额外补充一些信息的作用。当然效果好的前提是需要足够多数据让模型finetune。如果数据量少，不如采用词表中已有的特殊[token]去做训练。

### 实现方法

常见实现方式有以下三种：

#### 1.修改vocab.txt

直接修改词表vocab.txt，具体通过替换其中[unuesd]的token。

#### 2.tokenizer.add_tokens()

通过tokenizer.add_tokens() 添加新的tokens在tokenizer中，再使用model.resize_token_embeddings() 随机初始化权重。

####  3.tokenizer.add_special_tokens()

通过tokenizer.add_special_tokens() 添加新的 special tokens在tokenizer中，再使用model.resize_token_embeddings() 随机初始化权重。

目前大部分LLM模型已经无法通过直接修改vocab.txt实现添加新的自定义token，方法1已经失效, 方法2和3的效果是等价的。

