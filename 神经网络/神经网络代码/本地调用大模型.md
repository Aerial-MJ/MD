# 本地调用大模型
```python
self.tokenizer = AutoTokenizer.from_pretrained(
	self.model_config.name,
	**self.model_config.tokenizer_kwargs
)
self.model = AutoModelForCausalLM.from_pretrained(
	self.model_config.name,
	**self.model_config.model_kwargs
)

prompt=self.tokenizer.apply_chat_template(
	history or self.messages,
	tokenize=False,
	add_generation_prompt=True,
	enable_thinking=True
)


inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
outputs = self.model.generate(
    **inputs,
    # max_length=max_length,
    max_new_tokens=50,
    temperature=temperature,
    top_p=top_p,
    do_sample=do_sample
)

```



## tokenizer

```json
  "additional_special_tokens": [
    "<|im_start|>",
    "<|im_end|>",
    "<|object_ref_start|>",
    "<|object_ref_end|>",
    "<|box_start|>",
    "<|box_end|>",
    "<|quad_start|>",
    "<|quad_end|>",
    "<|vision_start|>",
    "<|vision_end|>",
    "<|vision_pad|>",
    "<|image_pad|>",
    "<|video_pad|>"
  ],
  "bos_token": null,
  "clean_up_tokenization_spaces": false,
  "eos_token": "<|im_end|>",
  "errors": "replace",
  "model_max_length": 131072,
  "pad_token": "<|endoftext|>",
  "split_special_tokens": false,
  "tokenizer_class": "Qwen2Tokenizer",
  "unk_token": null
}

```

### trust_remote_code

在使用 Hugging Face Transformers 库加载模型时，**trust_remote_code** 是一个重要的参数，用于控制是否信任从远程下载的代码。默认情况下，该参数设置为 **False**，以确保安全性，但在某些情况下需要将其设置为 **True** 来加载自定义模型或非官方模型。

### 使用场景

在Transformer模型输入的文本中常常会额外使用一些特殊[token]来表示一些特殊含义，比如希望对LLM通过设计prompt提升下游任务效果。

最开始在Bert预训练文本中就约定俗成用[CLS]表示句子开头、[SEP]表示隔开两个句子的符号、[UNK]表示未登录词、[PAD]表示该位置补充0。同时将特殊token加入模型词汇表vocab后可以防止tokenzier在切词时将该[token]切开，在文本编码时保留该[token]为一个整体。

常见使用场景在Bert表征文本时，你需要再使用一些特殊的[token]加入输入的文本中，能起到额外补充一些信息的作用。当然效果好的前提是需要足够多数据让模型finetune。如果数据量少，不如采用词表中已有的特殊[token]去做训练。

### 实现方法

常见实现方式有以下三种：

#### 1.修改vocab.txt

直接修改词表vocab.txt，具体通过替换其中[unuesd]的token。

#### 2.tokenizer.add_tokens()

通过tokenizer.add_tokens() 添加新的tokens在tokenizer中，再使用model.resize_token_embeddings() 随机初始化权重。

####  3.tokenizer.add_special_tokens()

通过tokenizer.add_special_tokens() 添加新的 special tokens在tokenizer中，再使用model.resize_token_embeddings() 随机初始化权重。





## model.generate

```python
outputs = self.model.generate(
	**inputs,
	# max_length=max_length,
	max_new_tokens=50,
	temperature=temperature,
    top_p=top_p,
	do_sample=do_sample
)
```

### do_sample

当 **do_sample** 设置为 **True** 时，意味着模型在生成文本时将采用**随机采样**的方式。这种设置允许模型根据下一个单词的概率分布进行采样，从而增加生成文本的多样性。相对而言，设置为 **False** 时，模型将选择概率最高的单词作为输出，这样生成的文本会更加确定性和一致性。

### top_p

**模型会选择概率最高的单词，直到这些单词的累计概率大于或等于** **90%**

top_p的具体作用方法如下：

假设我们设定 top_p = 0.7 ，模型使用以下逻辑选择部分单词加入备选集合
1、对所有单词按照概率从大到小进行排序
2、将备选集合中的概率逐个相加，当超过 0.7 时停止处理后面的单词

考虑 the，将它加入备选集合。他的概率(前面的概率之和是0) 小于 0.7，于是继续考虑下个单词。
考虑 a，他的概率是 0.25，加上前面的所有概率得到 0.75。

这时已经超过了 0.7 的闻值。a会被加入备选集合，但是不再处理后面的单词。现在集合中有 the 和 a 两个单词，模型会根据概率进行选择 ( the 还是更容易选中)

### temperature

“采样温度”是在0到2之间选择的参数。较高的值（如0.8）会使输出更具随机性，而较低的值（如0.2）则会使输出更集中，更确定性。

当 temperature 设置为较高值（接近1或大于1）时，模型在选择下一个 token 时，会倾向于选择概率较低的选项，从而产生更具创新性和多样性的输出。而当 temperature 设置为较低值（接近0但大于0）时，模型在选择下一个 token 时，会更偏向于选择概率较高的选项，从而产生更准确、更确定性的输出。可以将其比喻为一个热力调节器：温度越高，模型的输出就越'热闹'，有更多的可能性和随机性；温度越低，输出就越'冷静'，更加保守和确定。



## 属性注解

