# 强化学习

## 马尔科夫

### 1. 马尔科夫过程 (Markov Process)

**马尔可夫性质**（英语：Markov property）是 **概率论 中的一个概念**，因为俄国数学家 安德雷·马尔可夫 得名。 当一个 **随机过程** 在给定现在状态及所有过去状态情况下，其未来状态的条件 概率分布 仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此 随机过程 即具有马尔可夫性质。 **具有马尔可夫性质的过程通常称之为马尔可夫过程。**

- **定义**: 一种随机过程，其未来状态只依赖于当前状态，而与过去的状态无关（无后效性）。
$$
P(X_{t+1} | X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
$$

强化学习中的应用：
- 描述环境状态的动态转移，最简单的形式不涉及动作。
- 一个马尔可夫过程由 $ \langle S, P \rangle $ 定义，其中：
  - $ S $: 状态空间
  - $ P $: 状态转移概率

1. **状态空间 $ S $**：一个包含所有可能状态的集合。每个时刻系统都处于状态空间中的某个状态。
2. **转移概率矩阵 $ P $**：一个描述系统从一个状态转移到另一个状态的概率的矩阵。具体来说，$ P[i][j] $ 表示从状态 $ i $ 转移到状态 $ j $ 的概率。

**数学定义**

一个马尔可夫过程通常由以下部分定义：
$
\langle S, P \rangle
$

- $ S $：状态空间，包含所有可能的状态。
- $ P $：转移概率矩阵，描述从一个状态转移到另一个状态的概率。

转移概率矩阵 $ P $ 是一个二维矩阵，元素 $ P_{ij} $ 代表从状态 $ i $ 转移到状态 $ j $ 的概率，且每一行的和为 1，因为从任意状态出发，必然会转移到某个状态。

**示例：天气系统**

假设有一个简单的天气系统，它有两个状态：
- $ S = \{\text{晴天}, \text{雨天}\} $

并且，天气的转移遵循以下规则：
- 如果今天是晴天，明天是晴天的概率是 0.8，明天是雨天的概率是 0.2。
- 如果今天是雨天，明天是晴天的概率是 0.4，明天是雨天的概率是 0.6。

我们可以构建转移概率矩阵 $ P $：
$
P = \begin{pmatrix}
0.8 & 0.2 \\
0.4 & 0.6
\end{pmatrix}
$
其中，矩阵的每一行代表从当前状态转移到其他状态的概率。例如：
- 第 1 行表示从晴天转移到晴天的概率为 0.8，转移到雨天的概率为 0.2。
- 第 2 行表示从雨天转移到晴天的概率为 0.4，转移到雨天的概率为 0.6。

**马尔可夫过程的运作：**

- 如果今天是晴天，明天是晴天的概率是 0.8，明天是雨天的概率是 0.2。
- 如果今天是雨天，明天是晴天的概率是 0.4，明天是雨天的概率是 0.6。

随着时间的推移，系统的状态将不断变换，但每个状态转移的概率是固定的。

**总结：**

这个例子展示了一个简单的马尔可夫过程，其中状态空间有两个状态：晴天和雨天，系统的未来状态只依赖于当前状态，不依赖于更早的历史状态。通过构建转移概率矩阵，我们能够描述和模拟这个过程的动态变化。



### 2. 马尔科夫链 (Markov Chain)

马尔可夫链是指状态空间是离散的且系统在离散时间点上演化的马尔可夫过程。

- **转移概率矩阵**:

$$
P = 
\begin{bmatrix}
P_{11} & P_{12} & \dots & P_{1n} \\
P_{21} & P_{22} & \dots & P_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
P_{n1} & P_{n2} & \dots & P_{nn}
\end{bmatrix}
$$

其中 $ P_{ij} = P(X_{t+1}=j | X_t=i) $。

>马尔科夫链是一种 **有向图**。其特点如下：  
>
>1. **有向图的定义**
>   - 马尔科夫链中，节点表示系统的状态。
>   - 边表示状态之间的转移，并且这些转移是有方向性的。例如，从状态 $i$ 到状态 $j$ 的转移可能存在，而从 $j$ 到 $i$ 不一定存在。
>   - 每条边上通常标注一个概率值，称为转移概率。
>
>2. **为何是有向图**
>   - 转移概率 $P(i \to j)$ 是从一个状态 $i$ 到另一个状态 $j$ 的条件概率，这个过程有方向性，不能简单地认为 $P(i \to j) = P(j \to i)$。
>   - 这种方向性使得马尔科夫链的状态转移模型天然构成一个有向图。
>
>3. **补充：马尔科夫链的矩阵表示**
>   - 马尔科夫链常用转移概率矩阵（Transition Probability Matrix）来表示，其中每个元素 $P_{ij}$ 表示从状态 $i$ 到状态 $j$ 的转移概率。
>   - 由于矩阵并不一定对称，进一步说明了马尔科夫链是有向的。
>
>4. **其他相关概念**
>   - 如果将马尔科夫链看作无向图，则会丢失转移概率的方向性，无法准确建模状态之间的动态关系。
>   - **无向图** 通常用于建模对称关系，例如马尔科夫随机场（Markov Random Field，MRF），它是一种无向图模型，与马尔科夫链不同。
>
>总结：马尔科夫链是一个 **有向图模型**，用于表示具有方向性和转移概率的状态转移过程。



### 3. 隐马尔科夫模型 (Hidden Markov Model, HMM)
- **组成部分**:
  - 初始状态概率:
  
  $$
  \pi_i = P(X_0 = i)
  $$

  - 状态转移概率:

  $$
  A_{ij} = P(X_{t+1}=j | X_t=i)
  $$

  - 观测概率:

  $$
  B_{ij} = P(O_t = o_j | X_t = i)
  $$

- **目标函数**:
  评估观测序列的概率：
  $$
  P(O | \lambda) = \sum_{X} P(O, X | \lambda)
  $$

**隐马尔可夫模型（Hidden Markov Model, HMM）** 是一种特殊的 **概率图模型（Probabilistic Graphical Model, PGM）**。它属于 **生成式模型（Generative Model）** 的范畴，并且可以通过有向图模型（Bayesian Network, 贝叶斯网络）的形式来表示。

------
**HMM 满足概率图模型的条件如下：**

1. 随机变量间的依赖关系
   在 HMM 中，有两个类型的随机变量：
   
   - **隐状态序列**：$\{S_t\}$，隐状态之间满足马尔可夫性质。
   - **观测变量序列**：$\{O_t\}$，每个观测变量仅依赖于当前隐状态。
   - 这种依赖关系可以通过有向图表示。
   
2. 生成式建模 
   HMM 建模联合概率分布 $P(S, O)$，通过以下概率定义：
   
   - **状态转移概率**：$P(S_t \mid S_{t-1})$
   - **观测概率**：$P(O_t \mid S_t)$
   
3. 图结构

   HMM 的有向图形式是链式结构： 
   $$   S_1 \to S_2 \to S_3 \to \cdots \to S_T   $$ 
   每个隐状态 $S_t$ 生成一个观测
   $O_t$：   $$   S_t \to O_t   $$

**HMM 的概率图表示**

HMM 的有向图模型可以表示如下：

```
S1 → S2 → S3 → ... → ST
 |     |     |        |
O1    O2    O3       OT
```

1. **隐状态**：$S_t$ 表示系统的潜在状态，通常是无法直接观测的变量。  
2. **观测变量**：$O_t$ 表示在每个时间步观测到的数据。  
3. **依赖关系**：
   - $P(S_t \mid S_{t-1})$：隐状态之间的转移概率。
   - $P(O_t \mid S_t)$：给定隐状态下观测的生成概率。

**联合概率分布**

HMM 的联合概率分布可以写为：
$$
P(S, O) = P(S_1) \prod_{t=2}^{T} P(S_t \mid S_{t-1}) \prod_{t=1}^{T} P(O_t \mid S_t)
$$

>详细讲解一下隐马尔可夫模型（HMM）中的公式：
>
>### 背景
>在隐马尔可夫模型（Hidden Markov Model, HMM）中，通常我们感兴趣的是给定观测序列 $ O = (O_1, O_2, \dots, O_T) $ 的情况下，计算模型参数 $ \lambda = \langle A, B, \pi \rangle $ 下的观测序列的概率 $ P(O | \lambda) $，也就是在已知观测数据的情况下，计算模型生成这些观测数据的概率。
>
>其中：
>- $ A $：状态转移概率矩阵，描述了在一个时刻，状态从 $ i $ 转移到 $ j $ 的概率。
>- $ B $：观测概率矩阵，描述了给定某一状态时，观测到某一观测符号的概率。
>- $ \pi $：初始状态分布，描述了在时刻 1 处于某个状态的概率。
>
>#### 目标公式
>公式 $ P(O | \lambda) = \sum_X P(O, X | \lambda) $ 中的 $ X $ 是指隐状态序列，它是我们在 HMM 中的一个关键概念。具体地：
>
>- $ P(O | \lambda) $ 是给定模型参数 $ \lambda $ 和观测序列 $ O $ 的情况下，模型生成该观测序列的概率。
>- $ P(O, X | \lambda) $ 是给定模型参数 $ \lambda $ 和状态序列 $ X $ 的情况下，观测序列 $ O $ 和状态序列 $ X $ 同时发生的联合概率。
>
>公式的含义是，观测序列的概率可以通过对所有可能的状态序列进行求和，计算每个状态序列 $ X $ 与观测序列 $ O $ 同时发生的联合概率，然后将其求和。也就是说，我们在计算 $ P(O | \lambda) $ 时，考虑了所有可能的状态序列 $ X $。
>
>### 联合概率 $ P(O, X | \lambda) $
>根据 HMM 的定义，联合概率 $ P(O, X | \lambda) $ 可以分解为以下形式：
>$$
>P(O, X | \lambda) = P(X_1) \prod_{t=2}^{T} P(X_t | X_{t-1}) \prod_{t=1}^{T} P(O_t | X_t)
>$$
>- $ P(X_1) $ 是初始状态分布，表示在时刻 1 处于状态 $ X_1 $ 的概率。
>- $ P(X_t | X_{t-1}) $ 是状态转移概率，表示从状态 $ X_{t-1} $ 转移到状态 $ X_t $ 的概率。
>- $ P(O_t | X_t) $ 是观测概率，表示在时刻 $ t $ 给定状态 $ X_t $ 的情况下，观测到 $ O_t $ 的概率。
>
>### 计算 $ P(O | \lambda) $
>为了计算 $ P(O | \lambda) $，我们需要对所有可能的隐状态序列 $ X $ 进行求和：
>$$
>P(O | \lambda) = \sum_X P(O, X | \lambda)
>$$
>这个求和考虑了所有可能的状态序列 $ X $，然后得到每个状态序列下观测序列的联合概率的总和。
>
>由于状态空间可能非常大，直接列举所有的状态序列计算联合概率是非常困难的。为了高效计算，通常使用**前向算法**（Forward Algorithm）或**后向算法**（Backward Algorithm）。
>
>### 举个简单的例子
>
>假设我们有一个非常简单的 HMM，状态空间 $ S = \{\text{晴天}, \text{雨天}\} $，观测空间 $ O = \{\text{干燥}, \text{潮湿}\} $。
>
>假设模型的参数如下：
>- 初始状态概率：$ \pi = [0.6, 0.4] $（即晴天的初始概率是 0.6，雨天的初始概率是 0.4）
>- 转移概率：$ A = \begin{pmatrix} 0.7 & 0.3 \\ 0.4 & 0.6 \end{pmatrix} $
>- 观测概率：$ B = \begin{pmatrix} 0.9 & 0.1 \\ 0.4 & 0.6 \end{pmatrix} $（即晴天时观测到干燥的概率是 0.9，潮湿的概率是 0.1；雨天时观测到干燥的概率是 0.4，潮湿的概率是 0.6）
>
>假设我们的观测序列为 $ O = (\text{干燥}, \text{潮湿}) $，我们希望计算 $ P(O | \lambda) $。
>
>#### 计算联合概率
>我们要考虑所有可能的状态序列 $ X $，在这个例子中，只有两步（两天），所以我们考虑以下可能的状态序列：
>
>1. $ X = (\text{晴天}, \text{晴天}) $
>2. $ X = (\text{晴天}, \text{雨天}) $
>3. $ X = (\text{雨天}, \text{晴天}) $
>4. $ X = (\text{雨天}, \text{雨天}) $
>
>对于每个状态序列，我们计算联合概率 $ P(O, X | \lambda) $：
>
>- 对于 $ X = (\text{晴天}, \text{晴天}) $：
> $$
> P(O, X | \lambda) = \pi_1 \cdot P(X_2 = \text{晴天} | X_1 = \text{晴天}) \cdot P(O_1 = \text{干燥} | X_1 = \text{晴天}) \cdot P(O_2 = \text{潮湿} | X_2 = \text{晴天})
> $$
> $$
>  P(O, X | \lambda) = 0.6 \cdot 0.7 \cdot 0.9 \cdot 0.1 = 0.0378
> $$
>
>- 对于 $ X = (\text{晴天}, \text{雨天}) $：
> $$
>  P(O, X | \lambda) = 0.6 \cdot 0.3 \cdot 0.9 \cdot 0.6 = 0.0972
> $$
>
>- 对于 $ X = (\text{雨天}, \text{晴天}) $：
> $$
>  P(O, X | \lambda) = 0.4 \cdot 0.4 \cdot 0.4 \cdot 0.1 = 0.0064
> $$
>
>- 对于 $ X = (\text{雨天}, \text{雨天}) $：
> $$
>  P(O, X | \lambda) = 0.4 \cdot 0.6 \cdot 0.4 \cdot 0.6 = 0.0576
> $$
>
>#### 求和得到 $ P(O | \lambda) $
>最后，我们将所有联合概率加起来，得到观测序列 $ O $ 的概率：
>$$
>P(O | \lambda) = 0.0378 + 0.0972 + 0.0064 + 0.0576 = 0.199
>$$
>
>### 联合概率与目标函数的关系
>目标函数 $ P(O | \lambda) $ 是我们感兴趣的最终结果，它表示给定观测序列 $ O $ 和模型参数 $ \lambda $ 时，观测序列出现的总概率。而联合概率 $ P(O, X | \lambda) $ 是计算过程中使用的中间步骤。通过对所有可能的状态序列进行求和，联合概率为我们提供了不同状态序列与观测序列发生的概率，从而求得观测序列的总概率。
>
>因此，目标函数 $ P(O | \lambda) $ 是联合概率 $ P(O, X | \lambda) $ 的一个边际化结果，即对所有隐状态序列 $ X $ 的联合概率求和。
>
>### 总结
>- $ P(O | \lambda) $ 计算的是给定观测序列 $ O $ 和模型 $ \lambda $ 下观测到 $ O $ 的总概率。
>- 计算过程中，我们需要对所有可能的隐状态序列 $ X $ 进行求和。
>- 联合概率 $ P(O, X | \lambda) $ 描述了观测序列和状态序列的联合概率，而 $ P(O | \lambda) $ 是对所有状态序列的联合概率的边际化结果。



### 4. **马尔可夫奖励过程（Markov Reward Process, MRP）**

定义：

- 在马尔可夫过程基础上，加入了奖励机制：
  $
  P(s_{t+1}, r_t | s_t) = P(s_{t+1} | s_t), r_t = R(s_t)
  $
- 描述为 $ \langle S, P, R, \gamma \rangle $，其中：
  - $ R(s) $: 状态的奖励函数
  - $ \gamma $: 折扣因子，用于计算累计奖励

强化学习中的应用：

- MRP 用于**状态值函数**的定义：
  $
  V(s) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s\right]
  $
- 在策略评估中，计算某个固定策略的累积奖励值。



### 5. **马尔可夫决策过程（Markov Decision Process, MDP）**

- 强化学习的核心数学框架，包含了**决策**：
  $
  P(s_{t+1}, r_t | s_t, a_t) = P(s_{t+1} | s_t, a_t)
  $
  即未来状态和奖励取决于当前的状态和动作。

- 一个 MDP 由 $ \langle S, A, P, R, \gamma \rangle $ 定义，其中：
  - $ S $: 状态空间
  - $ A $: 动作空间
  - $ P(s'|s,a) $: 状态转移概率
  - $ R(s, a)$: 奖励函数
  - $ \gamma $: 折扣因子

强化学习中的应用：

- 强化学习的目标是学习一个最优策略 $ \pi^* $，使得期望累计奖励最大化：
  $
  \pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t r_t\right]
  $

- **折扣奖励的累积形式**:

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k}
$$

- **贝尔曼方程**:

  对于状态值函数 $ V(s) $:

  $$
  V(s) = \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a) + \gamma V(s') \right]
  $$



### 6. 马尔科夫随机场 (Markov Random Field, MRF)
- **联合概率分布**:

$$
P(X_1, X_2, \dots, X_n) = \frac{1}{Z} \prod_{C \in \text{cliques}} \psi_C(X_C)
$$

其中 $ \psi_C(X_C) $ 是团的势函数，$ Z $ 是归一化常数。



### 7. 马尔科夫链蒙特卡罗方法 (Markov Chain Monte Carlo, MCMC)
- **详细平衡条件**:

$$
\pi(i)P_{ij} = \pi(j)P_{ji}
$$

一种基于马尔可夫链的采样方法，用于从复杂分布中生成样本。

强化学习中的应用：
- 在策略优化中，MCMC 可以用于**策略评估**或**值函数近似**。
- 深度强化学习中，策略梯度方法有时会借助 MCMC 进行经验采样。



### 8. 马尔科夫模型的重要概念

#### **无后效性 (Markov Property)**

$$
P(X_{t+1} | X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
$$

#### **平稳分布 (Stationary Distribution)**

$$
\pi P = \pi
$$

#### **累积奖励的定义**:

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots
$$

#### **混合时间 (Mixing Time)**

指马尔科夫链达到平稳分布所需的时间，没有具体公式。



### 9. 隐马尔科夫模型的解码问题

- **维特比算法**:

$$
\delta_t(i) = \max_{j} \left[ \delta_{t-1}(j) A_{ji} \right] B_{i}(O_t)
$$

其中 $ \delta_t(i) $ 是最优路径的概率。
