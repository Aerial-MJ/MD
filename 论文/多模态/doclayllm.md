# 2025/10/5

**cache**

**mask**z

éå¸¸æ£’çš„é—®é¢˜ï¼Œè¿™ä¸€æ®µ `LlamaForCausalLM` æ˜¯ **Hugging Face çš„ Llama è§£ç å™¨æ ¸å¿ƒç±»**ï¼Œ
 æŒç®¡äº† **token-by-token ç”Ÿæˆ** çš„å…¨éƒ¨æµç¨‹ï¼ˆå°¤å…¶æ˜¯ `forward()` + `prepare_inputs_for_generation()` çš„äº¤äº’ï¼‰ã€‚

æˆ‘ä»¬æ¥**å®Œæ•´å‰–æç”Ÿæˆé˜¶æ®µï¼ˆgenerate è°ƒç”¨æ—¶ï¼‰å„ä¸ªå‡½æ•°çš„è¿ç®—é¡ºåº**ï¼Œå¸®ä½ ç†æ¸…æ•´ä¸ªæ•°æ®æµã€‚
 æˆ‘ä¼šç”¨ä¸€ä¸ªç®€å›¾ + è¯¦ç»†é¡ºåºæ¥è§£é‡Šæ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆã€‚

------

## ğŸ§© ä¸€ã€æ•´ä½“æµç¨‹æ¦‚è§ˆ

ç”Ÿæˆï¼ˆgenerationï¼‰æ—¶çš„è°ƒç”¨é“¾æ˜¯è¿™æ ·çš„ï¼š

```
model.generate()                  # huggingface.generate ä¸»å‡½æ•°
 â”œâ”€â”€ self.prepare_inputs_for_generation()   â† æ¯æ­¥ç”Ÿæˆå‰è°ƒç”¨ä¸€æ¬¡
 â”œâ”€â”€ self.forward()                        â† æ¯æ­¥æ¨¡å‹æ¨ç†è°ƒç”¨ä¸€æ¬¡
 â”‚    â””â”€â”€ self.model.forward()             â† è°ƒç”¨ LlamaModel (Transformer è§£ç å™¨)
 â”‚         â””â”€â”€ self.layers[i].forward()    â† ä¸€å±‚å±‚ self-attn + MLP
 â”‚         â””â”€â”€ å¯èƒ½ä½¿ç”¨ past_key_values åŠ é€Ÿ
 â”‚
 â”œâ”€â”€ logits ç»è¿‡ softmax é‡‡æ ·æˆ–argmax å¾—åˆ° next_token
 â””â”€â”€ æ‹¼æ¥ input_ids â† torch.cat([... , next_token])
      è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯ (ç»§ç»­ prepare_inputs_for_generation)
```

æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹å°±æ˜¯åœ¨ä¸€ä¸ª `while not eos_reached:` å¾ªç¯é‡Œé‡å¤ä¸Šé¢æµç¨‹ã€‚

------

## ğŸ§  äºŒã€é€æ­¥è¯¦ç»†è®²è§£

å‡è®¾ä½ æ‰§è¡Œäº†ï¼š

```python
outputs = model.generate(input_ids, bbox=bbox, pixel_values=img)
```

------

### âœ… Step 1ï¼šè¿›å…¥ `generate()`ï¼ˆtransformers å†…éƒ¨ï¼‰

Hugging Face çš„ `generate()` ä¼šï¼š

1. åˆå§‹åŒ– `past_key_values=None`
2. è°ƒç”¨ `prepare_inputs_for_generation()` æ¥æ„é€ ç¬¬ä¸€æ­¥è¾“å…¥ï¼ˆå« bbox / imageï¼‰
3. è°ƒç”¨ `forward()` å¾—åˆ° logits
4. é‡‡æ ·æˆ–å–æœ€å¤§æ¦‚ç‡çš„ä¸‹ä¸€ä¸ª token
5. æ‹¼å› `input_ids`
6. é‡å¤å¾ªç¯ï¼ˆåç»­æ­¥æœ‰ `past_key_values`ï¼‰

------

### âœ… Step 2ï¼š`prepare_inputs_for_generation()`

ä½ çœ‹åˆ°çš„è¿™ä¸€å¤§æ®µå‡½æ•°ï¼Œå°±æ˜¯ä¸“é—¨ä¸º **ä¸‹ä¸€æ­¥ token** åšè¾“å…¥å‡†å¤‡ã€‚

ä¸»è¦é€»è¾‘åˆ†ä¸‰éƒ¨åˆ†ğŸ‘‡ï¼š

#### ğŸ§© 1ï¸âƒ£ ä½ç½®ç¼–ç ä¸é•¿åº¦å¤„ç†

```python
if position_ids is not None:
    # ä¸ºä¸‹ä¸€æ­¥ç”Ÿæˆçš„ token è¿½åŠ  position_ids
```

> å½“ç”Ÿæˆç¬¬ä¸€ä¸ª token åï¼Œæ¨¡å‹å·²ç»ç”¨äº†éƒ¨åˆ†ä½ç½®ç¼–ç ï¼›
>  åç»­éœ€è¦ä¸ºæ¯ä¸ªæ–°å¢ token åˆ›å»ºè¿ç»­çš„ position_idsã€‚

------

#### ğŸ§© 2ï¸âƒ£ past_key_values å¤„ç†ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰

```python
if past_key_values is not None:
    # ä»ç¼“å­˜ä¸­è·å–å·²è®¡ç®—çš„ K/V
    # ä»…ä¿ç•™æ–° token è¿›å…¥ä¸‹ä¸€æ­¥ forward
```

> âš™ï¸ è¿™ä¸€æ­¥çš„ä½œç”¨æ˜¯ â€œå¢é‡æ¨ç†â€ï¼š
>
> - ä¸å†é‡å¤è®¡ç®—å‰é¢ token çš„æ³¨æ„åŠ›ï¼›
> - åªè®¡ç®—æ–° token ä¸æ—§ç¼“å­˜çš„ cross attentionã€‚

å› æ­¤ï¼š

- **ç¬¬ä¸€æ¬¡ forwardï¼š** `past_key_values=None`ï¼Œå…¨é‡è®¡ç®—ï¼›
- **ç¬¬äºŒæ¬¡åŠä»¥åï¼š** åªè®¡ç®—æ–° tokenã€‚

------

#### ğŸ§© 3ï¸âƒ£ å¤„ç† inputs_embeds ä¸ input_ids çš„äº’æ–¥é€»è¾‘

```python
if inputs_embeds is not None and past_key_values is None:
    model_inputs = {"inputs_embeds": inputs_embeds}
else:
    model_inputs = {"input_ids": input_ids.contiguous()}
```

> åªæœ‰ç¬¬ä¸€æ¬¡ forwardï¼ˆæ²¡æœ‰ cacheï¼‰å¯ä»¥ç›´æ¥è¾“å…¥ embeddingï¼›
>  åç»­æ­¥éª¤éƒ½åªèƒ½å–‚æ–°çš„ token çš„ `input_ids`ã€‚

------

#### ğŸ§© 4ï¸âƒ£ æ‹¼è£…è¿”å›

æœ€åè¿”å›ï¼š

```python
model_inputs = {
    "input_ids": ..., 
    "position_ids": ...,
    "cache_position": ...,
    "past_key_values": ...,
    "attention_mask": ...,
    "bbox": ...,
    "pixel_values": ...
}
```

è¿™äº›ä¼šä¼ å…¥ä¸‹ä¸€æ­¥çš„ `model.forward()`ã€‚

------

### âœ… Step 3ï¼š`forward()`ï¼ˆä½ å±•ç¤ºçš„æ ¸å¿ƒéƒ¨åˆ†ï¼‰

`forward()` æ¥æ”¶åˆ°ä¸Šä¸€æ­¥ç”Ÿæˆçš„ `model_inputs`ï¼Œç„¶åï¼š

#### ğŸ§© 1ï¸âƒ£ è°ƒç”¨ä¸»æ¨¡å‹ï¼ˆLlamaModelï¼‰

```python
outputs = self.model(
    input_ids=input_ids,
    past_key_values=past_key_values,
    bbox=bbox,
    pixel_values=pixel_values,
    ...
)
```

> è¿™ä¸€æ­¥è¿›å…¥äº†å¤šæ¨¡æ€ Llama backboneï¼ˆæ¯”å¦‚ä½ æ”¹çš„ spatial/patch embeddingï¼‰ã€‚

- è‹¥æ˜¯ **ç¬¬ä¸€æ¬¡ forward**ï¼š
  - ä¼šå¤„ç† `bbox`ã€`pixel_values`ï¼›
  - ç”Ÿæˆæ‰€æœ‰ token çš„ hidden statesï¼›
  - ç¼“å­˜æ‰€æœ‰ K/Vã€‚
- è‹¥æ˜¯ **åç»­ forward**ï¼š
  - åªå¤„ç†æ–°å¢ tokenï¼›
  - å¤ç”¨ä¸Šæ¬¡ç¼“å­˜ï¼ˆcache åŠ é€Ÿï¼‰ã€‚

------

#### ğŸ§© 2ï¸âƒ£ è¾“å‡º logits

```python
logits = self.lm_head(hidden_states)
```

hidden_states æ˜¯ `[batch, seq_len, hidden_size]`ï¼Œ
 `lm_head` æŠŠå®ƒæ˜ å°„åˆ° `[batch, seq_len, vocab_size]`ã€‚

------

#### ğŸ§© 3ï¸âƒ£ è®¡ç®— lossï¼ˆè‹¥è®­ç»ƒé˜¶æ®µï¼‰

```python
if labels is not None:
    loss = CrossEntropyLoss(logits, labels)
```

ç”Ÿæˆé˜¶æ®µä¸ä¼šæœ‰ labelsï¼Œå› æ­¤ `loss=None`ã€‚

------

#### ğŸ§© 4ï¸âƒ£ è¿”å›è¾“å‡º

```python
return CausalLMOutputWithPast(
    logits=logits,
    past_key_values=outputs.past_key_values,
)
```

æ³¨æ„ `past_key_values` ä¼šå›ä¼ ç»™ `generate()` çš„å¾ªç¯ä½“ã€‚

------

### âœ… Step 4ï¼š`generate()` å†…éƒ¨æ¥æ”¶ç»“æœ

`generate()` ä¼šä»è¿”å›çš„ logits ä¸­å–å‡ºæœ€åä¸€ä¸ª token çš„é¢„æµ‹åˆ†å¸ƒï¼š

```python
next_token = torch.argmax(logits[:, -1, :], dim=-1)
input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)

if next_token in eos_token_id:
    stop = True
```

ç„¶åå›åˆ° **Step 2**ï¼Œå†æ¬¡è°ƒç”¨ `prepare_inputs_for_generation()`ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥æ¨ç†ã€‚

------

### âœ… Step 5ï¼šç»ˆæ­¢åˆ¤æ–­

`generate()` åœ¨æ¯è½®å¾ªç¯åä¼šåˆ¤æ–­ï¼š

```python
if next_token in eos_token_id:
    stop = True
```

åœ¨ä½ é‚£ç§å¤š `eos_token_id` çš„ç‰ˆæœ¬ä¸­ï¼Œä»»æ„åŒ¹é…éƒ½ä¼šè§¦å‘åœæ­¢ã€‚

------

## ğŸ” ä¸‰ã€å®Œæ•´å¾ªç¯æµç¨‹æ€»ç»“ï¼ˆå¸¦ cacheï¼‰

| Step | å‡½æ•°å                            | å‘ç”Ÿçš„äº‹                      |
| ---- | --------------------------------- | ----------------------------- |
| â‘     | `generate()`                      | å¯åŠ¨ç”Ÿæˆå¾ªç¯                  |
| â‘¡    | `prepare_inputs_for_generation()` | æ„å»ºè¾“å…¥ï¼šæ·»åŠ ä½ç½®ã€è£å‰ª past |
| â‘¢    | `forward()`                       | è¿›å…¥æ¨¡å‹å‰å‘æ¨ç†              |
| â‘£    | `LlamaModel.forward()`            | çœŸæ­£ Transformer å±‚è®¡ç®—       |
| â‘¤    | `lm_head()`                       | ä» hidden state â†’ logits      |
| â‘¥    | `generate()`                      | ä» logits é‡‡æ ·æ–° token        |
| â‘¦    | `prepare_inputs_for_generation()` | å‡†å¤‡ä¸‹ä¸€è½®è¾“å…¥ï¼ˆå¸¦ cacheï¼‰    |
| â‘§    | å¾ªç¯ï¼Œç›´åˆ°é‡åˆ° eos token          | ç»“æŸç”Ÿæˆ                      |

------

## âš™ï¸ å››ã€ç»“åˆä½ é¡¹ç›®çš„å¤šæ¨¡æ€é€»è¾‘ï¼ˆDocLayLLM ç‰ˆï¼‰

ç¬¬ä¸€æ¬¡ forwardï¼š

- æœ‰ `bbox`ã€`pixel_values`ï¼›

- ä¼šæ‰§è¡Œï¼š

  ```python
  bbox = bbox[(bbox != -100).all(dim=-1)]
  spatial_position_embeddings = self.calculate_spatial_position_embeddings(bbox)
  patch_embeddings = self.forward_image(pixel_values)
  ```

- ç”¨ mask æŠŠè¿™äº›ç‰¹æ®Š token çš„ embedding æ³¨å…¥ `inputs_embeds`ã€‚

ç¬¬äºŒæ¬¡åŠä»¥å forwardï¼š

- `past_key_values` å·²å­˜åœ¨ï¼›
- ä¸å†è§¦å‘ä¸Šé¢é€»è¾‘ï¼›
- ç›´æ¥å¤ç”¨ç¼“å­˜ï¼Œä»…è®¡ç®—æ–° tokenã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ å‘ç°ï¼š

> ç¬¬ä¸€æ¬¡æ­£å¸¸ï¼Œç¬¬äºŒæ¬¡æŠ¥é”™ï¼ˆå› ä¸º cache é‡å¤æ‰§è¡Œ embedding æ›¿æ¢ï¼‰

è€Œç°åœ¨ä½ å…³æ‰ cache åã€Œç ´æ¡ˆäº†ã€ âœ…ã€‚

------

## ğŸ“Š äº”ã€æ€»ç»“ä¸€å¥è¯ï¼ˆæ ¸å¿ƒç†è§£ï¼‰

> `generate()` è°ƒç”¨é¡ºåºæ˜¯ï¼š
>  **prepare_inputs_for_generation â†’ forward â†’ generate next token â†’ repeat**
>  ç¬¬ä¸€æ¬¡æœ‰å®Œæ•´è¾“å…¥ï¼ˆå« bboxã€pixel_valuesï¼‰ï¼Œä¹‹åéƒ½æ˜¯å¢é‡è¾“å…¥ï¼ˆä»…æ–° tokenï¼‰ï¼Œé  cache åŠ é€Ÿã€‚

## Trainer çš„è®­ç»ƒæµç¨‹

å¯¹ ğŸ‘ï¼Œå¦‚æœä½ ä½¿ç”¨ **Transformers çš„ `Trainer`**ï¼Œå®ƒä¼šå¸®ä½ è‡ªåŠ¨å¤„ç† **loss â†’ backward â†’ optimizer.step() â†’ scheduler.step()** è¿™äº›æ­¥éª¤ã€‚

å…·ä½“æµç¨‹å¤§è‡´å¦‚ä¸‹ï¼š

------

### **1. Trainer çš„è®­ç»ƒæµç¨‹**

å½“ä½ è°ƒç”¨ï¼š

```python
trainer.train()
```

Trainer ä¼šæ‰§è¡Œä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

1. **å‡†å¤‡æ•°æ®**
    åŠ è½½ `train_dataset` â†’ æŒ‰ batch åˆ†ç»„ â†’ ç”Ÿæˆ `DataLoader`

2. **å‰å‘è®¡ç®—ï¼ˆforwardï¼‰**
    Trainer ä¼šè°ƒç”¨ä½ çš„æ¨¡å‹çš„ `forward()`ï¼Œå¹¶ä¼ å…¥ï¼š

   - `input_ids`
   - `attention_mask`
   - `labels`ï¼ˆå¦‚æœæœ‰çš„è¯ï¼ŒTrainer ä¼šè‡ªåŠ¨ä¼  labelsï¼‰

   ä½ çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ `LlamaForCausalLM`ï¼‰ä¼šæ ¹æ®æœ‰æ—  labels è‡ªåŠ¨è®¡ç®— lossï¼š

   ```python
   if labels is not None:
       loss = CrossEntropyLoss()(logits, labels)
   ```

3. **åå‘ä¼ æ’­ï¼ˆbackwardï¼‰**
    Trainer ä¼šè‡ªåŠ¨è°ƒç”¨ï¼š

   ```python
   loss.backward()
   ```

4. **æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰**
    é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚

5. **ä¼˜åŒ–å™¨æ›´æ–°**
    Trainer ä¼šè°ƒç”¨ï¼š

   ```python
   optimizer.step()
   scheduler.step()
   optimizer.zero_grad()
   ```

6. **æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ä¿å­˜**
    Trainer ä¼šè‡ªåŠ¨è®°å½• lossã€eval_metricsï¼Œå¹¶ä¿å­˜æ¨¡å‹ã€‚

------

### **2. ä½ ä¸éœ€è¦æ˜¾å¼è°ƒç”¨ backward**

å› ä¸º `Trainer` å·²ç»å¸®ä½ å°è£…äº†ï¼š

```python
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

æ‰€ä»¥ä½ åªéœ€å‡†å¤‡ï¼š

- æ¨¡å‹
- æ•°æ®é›†
- è®­ç»ƒå‚æ•°ï¼ˆTrainer é…ç½®ï¼‰

Trainer ä¼šå¸®ä½ å…¨è‡ªåŠ¨è·‘å®Œè®­ç»ƒè¿‡ç¨‹ã€‚

------

### **3. æ³¨æ„**

å¦‚æœä½ æƒ³åš**è‡ªå®šä¹‰å¾®è°ƒ**ï¼ˆæ¯”å¦‚ LoRAã€åŠå¾®è°ƒï¼‰ï¼Œä½ éœ€è¦ï¼š

- è‡ªå®šä¹‰æ¨¡å‹çš„ `forward()`ï¼ˆä¿è¯ labels è¢«æ­£ç¡®ä¼ å…¥ï¼‰
- æˆ–è€…è‡ªå·±å†™ training loopï¼Œç»•è¿‡ Trainer

å› ä¸º Trainer é»˜è®¤çš„è®­ç»ƒå¾ªç¯æ˜¯é€šç”¨çš„ï¼Œå®ƒä¼šä¾èµ–æ¨¡å‹çš„ `forward()` è¿”å› `loss`ã€‚

------

ğŸ“Œ æ€»ç»“ï¼š
 æ˜¯çš„ï¼Œä½¿ç”¨ Transformers çš„ `Trainer`ï¼Œloss çš„è®¡ç®—ã€backward ä»¥åŠå‚æ•°æ›´æ–°éƒ½ä¼šè‡ªåŠ¨è¿›è¡Œã€‚ä½†å‰ææ˜¯æ¨¡å‹çš„ `forward()` è¦èƒ½è¿”å› lossï¼Œè¿™ä¹Ÿæ˜¯ LlamaForCausalLM å·²ç»å®ç°çš„ã€‚

