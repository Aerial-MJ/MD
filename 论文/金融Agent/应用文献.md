# 多Agent

纯**Agent**存在的问题

纯**Reinforcement Learning**存在的问题

累积收益、夏普比率和最大回撤

通过模拟一个动态、协作的交易环境，该框架旨在提升交易表现。详细的架构设计和广泛的实验揭示了其在**累积收益**、**夏普比率**和**最大回撤**方面相较于基线模型的优越性，凸显了多智能体LLM框架在金融交易中的潜力。



multiagent+rl
多模态融合数据


多agent


与其他深度学习方法类似，RL策略通常难以解释，尤其是策略网络较复杂时。
无法回答“为什么做出这个交易决策”这类问题，不符合许多金融机构的合规或审计要求。

直接微调  的缺点
金融大数据模型



大模型（特别是大语言模型，如GPT、BERT）可处理复杂的语义信息，从非结构化文本（如新闻、社交媒体、公告）中提取深层信息。
在交易中，可作为环境理解器（state encoder），将市场文本情绪、政策变化、地缘政治等信息结构化表示给RL模型使用。
举例：通过大模型分析“美联储讲话”，为RL提供情绪因子或事件标签，增强状态表达能力。

大模型擅长表示和理解信息，但缺乏明确的目标函数驱动，容易生成泛泛的建议或“幻觉”。
强化学习提供了奖励信号，可引导大模型学习面向收益、风控、稳定性等目标的策略优化。
举例：在Fine-Tuning或RLHF（Reinforcement Learning with Human Feedback）框架下，强化学习优化大模型的交易建议策略，使其兼顾收益率与回撤控制。





大模型选股->





MCP接XtQuant