## main.py

æ•´ä¸ªè„šæœ¬æ˜¯ä¸ºäº†é©±åŠ¨ä¸€ä¸ª**åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“é—®ç­”ç³»ç»Ÿ**è¿è¡Œåœ¨ä¸€ä¸ªæŒ‡å®šçš„æ•°æ®é›†ï¼ˆå¦‚ `medqa`ï¼‰ä¸Šï¼Œå¹¶æ”¯æŒéš¾åº¦è‡ªé€‚åº”çš„æ¨ç†æµç¨‹ã€‚

### 1. å¼•å…¥ä¾èµ–å’Œå·¥å…·å‡½æ•°

```python
import os
import json
import random
import argparse
from tqdm import tqdm
from termcolor import cprint
from pptree import print_tree
from prettytable import PrettyTable
```

- å¯¼å…¥äº†å¾ˆå¤šå·¥å…·åº“ï¼ŒåŒ…æ‹¬è¿›åº¦æ¡ï¼ˆ`tqdm`ï¼‰ã€å½©è‰²è¾“å‡ºï¼ˆ`termcolor`ï¼‰ã€æ ‘çŠ¶ç»“æ„å¯è§†åŒ–ï¼ˆ`pptree`ï¼‰ã€è¡¨æ ¼ç¾åŒ–ï¼ˆ`PrettyTable`ï¼‰ï¼Œè¿™äº›ç”¨äº**å¢å¼ºäº¤äº’æ€§å’Œè°ƒè¯•ä½“éªŒ**ã€‚

```python
from utils import (
    Agent, Group, parse_hierarchy, parse_group_info, setup_model,
    load_data, create_question, determine_difficulty,
    process_basic_query, process_intermediate_query, process_advanced_query
)
```

- ä» `utils.py` å¼•å…¥äº†**æ ¸å¿ƒç»„ä»¶å‡½æ•°å’Œç±»**ï¼Œæ¯”å¦‚ï¼š
  - `Agent`, `Group`: å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åŸºæœ¬å•ä½ã€‚
  - `setup_model`: åˆå§‹åŒ– LLMï¼ˆå¦‚ OpenAI æ¨¡å‹ï¼‰ã€‚
  - `process_*_query`: ä¸åŒéš¾åº¦ç­‰çº§çš„æ¨ç†è¿‡ç¨‹å‡½æ•°ã€‚
  - `create_question`, `determine_difficulty`: ç”¨äºå¤„ç†è¾“å…¥æ•°æ®å’Œè‡ªé€‚åº”è°ƒåº¦éš¾åº¦ã€‚

### 2. è§£æå‘½ä»¤è¡Œå‚æ•°

```python
parser = argparse.ArgumentParser()
"""
è¿™ä¸€å¥æ˜¯åˆ›å»ºä¸€ä¸ª ArgumentParser å¯¹è±¡ï¼Œå®ƒæ˜¯ Python æ ‡å‡†åº“ argparse æä¾›çš„ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°çš„ã€‚
æ¯”å¦‚ä½ è¿è¡Œè„šæœ¬æ—¶å¯ä»¥è¿™æ ·å†™ï¼š
python main.py --dataset medqa --model gpt-4o-mini --difficulty advanced --num_samples 20
è¿™ä¸ª parser å°±ä¼šå¸®ä½ è§£æè¿™äº›å‚æ•°ï¼Œå¹¶æŠŠå®ƒä»¬å˜æˆç¨‹åºé‡Œçš„å˜é‡ã€‚
"""

parser.add_argument('--dataset', type=str, default='medqa')
parser.add_argument('--model', type=str, default='gpt-4o-mini')
parser.add_argument('--difficulty', type=str, default='adaptive')
parser.add_argument('--num_samples', type=int, default=100)
args = parser.parse_args()


"""
è¿™å¥ä¼šå»å®é™…è¯»å–å‘½ä»¤è¡Œä¸­ä¼ å…¥çš„å‚æ•°ï¼Œå¹¶æŠŠå®ƒä»¬æ‰“åŒ…æˆä¸€ä¸ªå¯¹è±¡ argsï¼Œä½ å°±å¯ä»¥è¿™æ ·ç”¨ï¼š
args.dataset        # ==> 'medqa'
args.model          # ==> 'gpt-4o-mini'
args.difficulty     # ==> 'advanced'
args.num_samples    # ==> 20
ç®€å•æ¥è¯´ï¼šargs æ˜¯ä¸€ä¸ªå¯ä»¥é€šè¿‡ç‚¹ï¼ˆ.ï¼‰è¯­æ³•è®¿é—®çš„é…ç½®å¯¹è±¡ï¼Œè¯»å–çš„æ˜¯ä½ è¿è¡Œç¨‹åºæ—¶é€šè¿‡å‘½ä»¤è¡Œä¼ è¿›æ¥çš„å†…å®¹ã€‚
"""
```

- è¿™é‡Œå…è®¸ç”¨æˆ·æŒ‡å®šï¼š
  - æ•°æ®é›†ï¼ˆå¦‚ `medqa`ï¼‰
  - æ¨¡å‹åç§°ï¼ˆå¦‚ `gpt-4o-mini`ï¼‰
  - éš¾åº¦ç­‰çº§ï¼ˆå¯ä»¥è®¾ä¸º `'basic'`, `'intermediate'`, `'advanced'`, æˆ– `'adaptive'`ï¼‰
  - æ‰§è¡Œæ ·æœ¬æ•°é‡ï¼ˆä¾‹å¦‚æœ€å¤šè¿è¡Œ100ä¸ªé—®é¢˜ï¼‰

### 3. åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®

```python
model, client = setup_model(args.model)
test_qa, examplers = load_data(args.dataset)
```

- `setup_model`ï¼šæ ¹æ®è¾“å…¥çš„æ¨¡å‹åï¼Œåˆå§‹åŒ–è¯­è¨€æ¨¡å‹å’Œ clientï¼ˆå¤§æ¦‚ç‡æ˜¯ OpenAI æˆ–å…¶ä»– API å°è£…ï¼‰ã€‚
- `load_data`ï¼šåŠ è½½æµ‹è¯•æ•°æ®å’Œâ€œç¤ºä¾‹é¢˜â€ï¼ˆ**å³ few-shot prompting æ‰€éœ€çš„ examples**ï¼‰ã€‚

###  **4. åˆå§‹åŒ– agent è¡¨æƒ…ç¬¦å·**

```python
agent_emoji = ['ğŸ§‘â€âš•ï¸', ...]

random.shuffle(agent_emoji)
# ä½œç”¨æ˜¯ï¼šå°† agent_emoji åˆ—è¡¨ä¸­çš„å…ƒç´ æ‰“ä¹±é¡ºåºï¼ˆå°±åœ°ä¹±åºå¤„ç†ï¼‰ã€‚
"""
å› ä¸ºè¿™ä¸ªåˆ—è¡¨æ˜¯ä¸ºå¤šæ™ºèƒ½ä½“å‡†å¤‡çš„â€œemoji å¤´åƒâ€ï¼Œç”¨äºå±•ç¤ºæ¯ä¸ª Agentã€‚æ‰“ä¹±é¡ºåºå¯ä»¥ï¼š
ä¿è¯æ¯æ¬¡è¿è¡Œæ—¶åˆ†é…ç»™æ™ºèƒ½ä½“çš„ emoji æ˜¯éšæœºçš„ï¼›
å¢åŠ å¯è§†åŒ–å¤šæ ·æ€§ï¼Œé¿å…æ€»æ˜¯ç¬¬ä¸€ä¸ª Agent æ˜¯å›ºå®šçš„å¤´åƒã€‚
"""
```

- è¿™äº›æ˜¯ç”¨äºæ¯ä¸ª agent çš„è§†è§‰æ ‡è¯†ï¼Œè™½ç„¶åªæ˜¯ **cosmetic**ï¼Œä½†åœ¨å¤šæ™ºèƒ½ä½“è¾“å‡ºç»“æœä¸­å¸®åŠ©ç›´è§‚åŒºåˆ†è°æ˜¯è°ã€‚

### **5. ä¸»å¾ªç¯å¤„ç†æ¯ä¸ªæ ·æœ¬**

```python
results = []
for no, sample in enumerate(tqdm(test_qa)):
    if no == args.num_samples:
        break
```

- éå†æµ‹è¯•æ•°æ®ä¸­çš„æ¯ä¸ªé—®é¢˜ï¼Œé™åˆ¶æœ€å¤šè¿è¡Œ `args.num_samples` æ¡ã€‚

### **6. ä¸ºæ¯é“é¢˜åˆ›å»º question & åˆ¤æ–­éš¾åº¦**

```python
	question, img_path = create_question(sample, args.dataset)
    difficulty = determine_difficulty(question, args.difficulty)
```

- `create_question`: æå–æ–‡å­—é—®é¢˜ï¼Œæœ‰äº›é—®é¢˜å¯èƒ½é™„å¸¦å›¾ç‰‡ã€‚
- `determine_difficulty`: æ ¹æ®é¢˜ç›®çš„å†…å®¹å’Œè®¾ç½®åˆ¤æ–­éš¾åº¦ï¼ˆå¦‚æœè®¾ç½®ä¸º `adaptive`ï¼Œå¯èƒ½ä¼šè‡ªåŠ¨è¯„ä¼°ï¼‰ã€‚

### **7. è°ƒç”¨ä¸åŒçš„ agent æ¨ç†æµç¨‹**

```python
    if difficulty == 'basic':
        final_decision = process_basic_query(question, examplers, args.model, args)
    elif difficulty == 'intermediate':
        final_decision = process_intermediate_query(question, examplers, args.model, args)
    elif difficulty == 'advanced':
        final_decision = process_advanced_query(question, args.model, args)
```

- **å…³é”®é€»è¾‘åˆ†æ”¯**ï¼šæ ¹æ®éš¾åº¦é€‰æ‹©ä¸åŒçš„æ¨ç†æ–¹æ³•ã€‚
  - `basic`ï¼šå¯èƒ½æ˜¯å•æ™ºèƒ½ä½“ç®€å•å›ç­”ã€‚
  - `intermediate`ï¼šå¯èƒ½æœ‰å°è§„æ¨¡ agent åä½œæˆ– chain-of-thoughtã€‚
  - `advanced`ï¼šå¯èƒ½æ˜¯ full multi-agent cooperationï¼Œç”šè‡³æœ‰ agent hierarchyã€‚

### **8. ä¿å­˜ç»“æœï¼ˆç»“æ„åŒ–ï¼‰**

```python
    if args.dataset == 'medqa':
        results.append({
            'question': question,
            'label': sample['answer_idx'],
            'answer': sample['answer'],
            'options': sample['options'],
            'response': final_decision,
            'difficulty': difficulty
        })
```

- æŠŠæ¯é“é¢˜ç›®çš„ä¿¡æ¯ã€çœŸå®ç­”æ¡ˆã€æ¨¡å‹é¢„æµ‹ç»“æœã€éš¾åº¦ä¸€èµ·ä¿å­˜è¿› `results`ã€‚

## utils.py

### Agent

`Agent` ç±»å°±æ˜¯ä½ ç³»ç»Ÿé‡Œçš„ã€Œæ™ºèƒ½ä½“ã€ã€‚æ¯ä¸ª Agent æœ‰ï¼š

- ä¸€ä¸ªè§’è‰²ï¼ˆ`role`ï¼‰
- ä¸€ä¸ªè¡Œä¸ºæŒ‡ä»¤ï¼ˆ`instruction`ï¼‰
- ä¸€ä¸ª LLM æ¨¡å‹ï¼ˆGPT or Geminiï¼‰
- ä¸€æ®µä¼šè¯å†å²ï¼ˆmessagesï¼‰
- å¯ä»¥è¿›è¡Œé—®ç­”ï¼ˆ`chat()`ï¼‰æˆ–æ§åˆ¶æ¸©åº¦çš„æµ‹è¯•ï¼ˆ`temp_responses()`ï¼‰

```python
def __init__(self, instruction, role, examplers=None, model_info='gpt-4o-mini', img_path=None):
        self.instruction = instruction
        self.role = role
        self.model_info = model_info
        self.img_path = img_path
	"""
		è¿™ä¸ªæ„é€ å™¨ç”¨æ¥åˆå§‹åŒ–ä¸€ä¸ª Agentï¼ŒåŒ…å«ï¼š
		instructionï¼šç³»ç»Ÿ promptï¼Œè®¾å®š Agent çš„è¡Œä¸ºå‡†åˆ™ï¼Œæ¯”å¦‚ã€Œä½ æ˜¯ä¸€åå¿ƒè„ç—…ä¸“å®¶ã€
		roleï¼šAgent çš„è§’è‰²åï¼ˆåªæ˜¯è®°å½•ï¼Œè¾…åŠ©åˆ†æï¼‰
		examplersï¼šå¯é€‰çš„ few-shot æ ·ä¾‹ï¼ˆåŠ å…¥å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
		model_infoï¼šä½¿ç”¨çš„æ¨¡å‹ï¼Œå¦‚ "gpt-4o-mini" æˆ– "gemini-pro"
		img_pathï¼šå¯é€‰ï¼Œå›¾ç‰‡è·¯å¾„ï¼ˆç›®å‰æœªå¯ç”¨ï¼‰
	"""

        if self.model_info == 'gemini-pro':
            self.model = genai.GenerativeModel('gemini-pro')
            self._chat = self.model.start_chat(history=[])
            """
            Gemini ä½¿ç”¨ Google çš„ SDKï¼Œæ„å»ºä¸€ä¸ªä¼šè¯å¯¹è±¡ _chatï¼Œå¯ä»¥å¤šè½®äº¤äº’ã€‚
            """
            
        elif self.model_info in ['gpt-3.5', 'gpt-4', 'gpt-4o', 'gpt-4o-mini']:
            self.client = OpenAI(api_key=os.environ['openai_api_key'])
            self.messages = [
                {"role": "system", "content": instruction},
            ]
            """
            OpenAI æ¨¡å‹ï¼ˆGPTï¼‰ä½¿ç”¨ openai åŒ…ï¼Œæ„å»º messages åˆ—è¡¨ä½œä¸ºå¯¹è¯ä¸Šä¸‹æ–‡ã€‚
            """
            
            if examplers is not None:
                for exampler in examplers:
                    self.messages.append({"role": "user", "content": exampler['question']})
                    self.messages.append({"role": "assistant", "content": exampler['answer'] + "\n\n" + exampler['reason']})
                    #å¹¶åŠ å…¥ few-shot ç¤ºä¾‹ï¼š
                    #è¿™æ˜¯æ ‡å‡† few-shot learning æ„é€ æ–¹å¼ï¼Œå¸® LLM ç†è§£æ ¼å¼å’Œé£æ ¼ã€‚
                    
    def chat(self, message, img_path=None, chat_mode=True):
        if self.model_info == 'gemini-pro':
            for _ in range(10):
                try:
                    response = self._chat.send_message(message, stream=True)
                    responses = ""
                    for chunk in response:
                        responses += chunk.text + "\n"
                    return responses
                except:
                    continue
            return "Error: Failed to get response from Gemini."
        """
        å°è¯•æœ€å¤š 10 æ¬¡è·å–å›å¤ï¼ˆæœ‰äº›æ—¶å€™ Gemini API ä¸ç¨³å®šï¼‰
        ä½¿ç”¨ stream=True å®æ—¶è·å–å“åº”æµ
		"""

        elif self.model_info in ['gpt-3.5', 'gpt-4', 'gpt-4o', 'gpt-4o-mini']:
            self.messages.append({"role": "user", "content": message})
            
            if self.model_info == 'gpt-3.5':
                model_name = "gpt-3.5-turbo"
            else:
                model_name = "gpt-4o-mini"

            response = self.client.chat.completions.create(
                model=model_name,
                messages=self.messages
            )

            self.messages.append({"role": "assistant", "content": response.choices[0].message.content})
            return response.choices[0].message.content
        """
        å°†ç”¨æˆ·è¾“å…¥åŠ å…¥ä¸Šä¸‹æ–‡
		ç”¨æœ€æ–°ä¸Šä¸‹æ–‡è¯·æ±‚ LLM å“åº”
		å°† LLM å“åº”ä¹ŸåŠ å…¥ä¸Šä¸‹æ–‡ï¼Œå½¢æˆå®Œæ•´å†å²
		"""

    def temp_responses(self, message, img_path=None):
        if self.model_info in ['gpt-3.5', 'gpt-4', 'gpt-4o', 'gpt-4o-mini']:      
            self.messages.append({"role": "user", "content": message})
            
            temperatures = [0.0]
            
            responses = {}
            for temperature in temperatures:
                if self.model_info == 'gpt-3.5':
                    model_info = 'gpt-3.5-turbo'
                else:
                    model_info = 'gpt-4o-mini'
                response = self.client.chat.completions.create(
                    model=model_info,
                    messages=self.messages,
                    temperature=temperature,
                )
                
                responses[temperature] = response.choices[0].message.content
                
            return responses
        
        elif self.model_info == 'gemini-pro':
            response = self._chat.send_message(message, stream=True)
            responses = ""
            for chunk in response:
                responses += chunk.text + "\n"
            return responses
        """
        è¿™ä¸ªå‡½æ•°ç”¨æ¥ä»¥ä¸åŒçš„ temperatureï¼ˆéšæœºæ€§ï¼‰ç”Ÿæˆå¤šç§å›ç­”ï¼Œå¯ä»¥ç”¨äºï¼š
		æ£€æŸ¥æ¨¡å‹ç¨³å®šæ€§
		åšå¤šæ•°æŠ•ç¥¨ï¼ˆmajority votingï¼‰
		çœ‹å¤šæ ·æ€§ï¼ˆdiversityï¼‰
		"""
```

### Group

å®ƒå®šä¹‰äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­â€œä¸€ä¸ªå›¢é˜Ÿâ€çš„è¡Œä¸ºæ–¹å¼ï¼Œå°¤å…¶æ˜¯å¦‚ä½•**ä»¥åˆä½œçš„æ–¹å¼å›ç­”åŒ»å­¦é—®é¢˜**ã€‚

```python
class Group:
    def __init__(self, goal, members, question, examplers=None):
        """
        goal	å°ç»„çš„æ•´ä½“ç›®æ ‡ï¼Œæ¯”å¦‚â€œè§£ç­”åŒ»å­¦é—®é¢˜â€
		members	æ¯ä¸ªæˆå‘˜çš„ä¿¡æ¯ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ª dict çš„åˆ—è¡¨ï¼‰
		question	è¦æ±‚è§£çš„é—®é¢˜ï¼ˆåŒ»å­¦é¢˜ï¼‰
		examplers	ç¤ºä¾‹æ•°æ®ï¼Œç”¨äºæ”¯æŒ few-shot prompting
        """
        self.goal = goal
        self.members = []
        
        """
        self.goal = goal
		self.members = []
		å…ˆä¿å­˜ç›®æ ‡ï¼Œå‡†å¤‡æˆå‘˜åˆ—è¡¨ã€‚
		"""
        
        for member_info in members:
            _agent = Agent('You are a {} who {}.'.format(member_info['role'], member_info['expertise_description'].lower()), role=member_info['role'], model_info='gpt-4o-mini')
            """
            ç”¨æ¯ä¸ªæˆå‘˜çš„è§’è‰²å’Œä¸“ä¸šæè¿°ï¼Œåˆå§‹åŒ–ä¸€ä¸ª Agent
            æŒ‡å®šæ¨¡å‹æ˜¯ gpt-4o-mini
            """
            
            _agent.chat('You are a {} who {}.'.format(member_info['role'], member_info['expertise_description'].lower()))
            self.members.append(_agent)
            #ç»™ Agent å‘ä¸€æ¡ç³»ç»Ÿæ¶ˆæ¯ï¼Œç›¸å½“äºâ€œè®©ä»–è¿›å…¥è§’è‰²â€
			#ä¾‹å¦‚ï¼šâ€œä½ æ˜¯ä¸€ä¸ªå¿ƒè„ç—…ä¸“å®¶ï¼Œä½ æ“…é•¿å¿ƒç”µå›¾åˆ†æå’Œä¸´åºŠè¯Šæ–­ã€‚â€
        self.question = question
        self.examplers = examplers


    def interact(self, comm_type, message=None, img_path=None):
        """
        def interact(self, comm_type, message=None, img_path=None):
		ç›®å‰åªå®ç°äº† comm_type == 'internal' çš„é€»è¾‘ï¼Œä¹Ÿå°±æ˜¯ ç»„å†…æ²Ÿé€šæ¨¡å¼ã€‚
		"""
        if comm_type == 'internal':
            lead_member = None
            assist_members = []
            for member in self.members:
                member_role = member.role

                if 'lead' in member_role.lower():
                    lead_member = member
                else:
                    assist_members.append(member)

            if lead_member is None:
                lead_member = assist_members[0]
            
            delivery_prompt = f'''You are the lead of the medical group which aims to {self.goal}. You have the following assistant clinicians who work for you:'''
            for a_mem in assist_members:
                delivery_prompt += "\n{}".format(a_mem.role)
            
            delivery_prompt += "\n\nNow, given the medical query, provide a short answer to what kind investigations are needed from each assistant clinicians.\nQuestion: {}".format(self.question)
            try:
                delivery = lead_member.chat(delivery_prompt)
            except:
                delivery = assist_members[0].chat(delivery_prompt)

            investigations = []
            for a_mem in assist_members:
                investigation = a_mem.chat("You are in a medical group where the goal is to {}. Your group lead is asking for the following investigations:\n{}\n\nPlease remind your expertise and return your investigation summary that contains the core information.".format(self.goal, delivery))
                investigations.append([a_mem.role, investigation])
            
            gathered_investigation = ""
            for investigation in investigations:
                gathered_investigation += "[{}]\n{}\n".format(investigation[0], investigation[1])

            if self.examplers is not None:
                investigation_prompt = f"""The gathered investigation from your asssitant clinicians is as follows:\n{gathered_investigation}.\n\nNow, after reviewing the following example cases, return your answer to the medical query among the option provided:\n\n{self.examplers}\nQuestion: {self.question}"""
            else:
                investigation_prompt = f"""The gathered investigation from your asssitant clinicians is as follows:\n{gathered_investigation}.\n\nNow, return your answer to the medical query among the option provided.\n\nQuestion: {self.question}"""

            response = lead_member.chat(investigation_prompt)

            return response

        elif comm_type == 'external':
            return
```

### parse_hierarchy

```python
def parse_hierarchy(info, emojis):
    """
    info: ä¸€ä¸ªåŒ…å« (expert, hierarchy) å…ƒç»„çš„åˆ—è¡¨ï¼Œæè¿°ä¸“å®¶çš„åç§°å’Œä»–ä»¬çš„ä¸Šä¸‹çº§å…³ç³»ã€‚
	emojis: æ¯ä¸ª Agent å¯¹åº”çš„ emojiï¼Œç”¨æ¥è®©æ ‘çŠ¶å›¾æ›´æœ‰è¶£ã€å¯è§†åŒ–æ›´æ¸…æ™°ã€‚
	"""
    moderator = Node('moderator (\U0001F468\u200D\u2696\uFE0F)')
    agents = [moderator]
    #è¿™è¡¨ç¤ºå›¢é˜Ÿæœ€é¡¶å±‚æœ‰ä¸€ä¸ªåè°ƒäººï¼ˆmoderatorï¼‰ã€‚

    count = 0
    for expert, hierarchy in info:
        #info æ¯ä¸€é¡¹æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œä¾‹å¦‚ï¼š("Dr. Smith - Cardiologist", "Lead > Assistant")
        try:
            expert = expert.split('-')[0].split('.')[1].strip()
        except:
            expert = expert.split('-')[0].strip()
            """
            è¿™æ®µä»£ç éå¸¸â€œé˜²å¾¡å¼â€ï¼Œç¡®ä¿èƒ½æ­£ç¡®æå–ä¸“å®¶åå­—ã€‚ä¾‹å­ï¼š
			è¾“å…¥ï¼š"Dr. Smith - Cardiologist"
			ç»“æœï¼š"Smith"
			å¦‚æœå¤±è´¥äº†ï¼Œå°±ï¼š
			"""
        
        if hierarchy is None:
            hierarchy = 'Independent'
            #åˆ¤æ–­æ˜¯å¦æ˜¯ç‹¬ç«‹ä¸“å®¶
        	#é»˜è®¤æ²¡æœ‰ç»™å±‚çº§çš„å°±å½“ä½œâ€œç‹¬ç«‹ä¸“å®¶â€ã€‚
        
        if 'independent' not in hierarchy.lower():
            #è¡¨ç¤ºè¯¥ä¸“å®¶æœ‰ä¸Šä¸‹çº§å…³ç³»ã€‚
            #å¦‚æœæœ‰å±‚çº§å…³ç³»ï¼šæ·»åŠ ä¸ºæŸä¸ªâ€œparentâ€çš„å­èŠ‚ç‚¹
            parent = hierarchy.split(">")[0].strip()
            child = hierarchy.split(">")[1].strip()

            for agent in agents:
                if agent.name.split("(")[0].strip().lower() == parent.strip().lower():
                    child_agent = Node("{} ({})".format(child, emojis[count]), agent)
                    agents.append(child_agent)
            
                    """
                    æ‰¾åˆ°çˆ¶èŠ‚ç‚¹çš„åå­—ï¼Œæ¯”å¦‚ "Lead"
                    åœ¨å·²æœ‰ agents ä¸­æŸ¥æ‰¾åå­—ä¸º "Lead" çš„èŠ‚ç‚¹
                    æ‰¾åˆ°ä¹‹åï¼ŒæŠŠ child åŠ åˆ°å®ƒçš„ä¸‹é¢
                    """
        else:
            agent = Node("{} ({})".format(expert, emojis[count]), moderator)
            agents.append(agent)
            #å¦‚æœæ˜¯ç‹¬ç«‹ä¸“å®¶ï¼šç›´æ¥æŒ‚åœ¨ moderator ä¸‹é¢
            #è¿™è¯´æ˜è¿™ä¸ªä¸“å®¶ä¸å±äºä»»ä½•ä¸Šä¸‹çº§ï¼Œç›´æ¥æŒ‚åˆ° moderatorï¼ˆåè°ƒäººï¼‰ä¸‹é¢ã€‚

        count += 1

    return agents
	#æ³¨æ„è¿™ä¸æ˜¯è¿”å›ä¸€ä¸ªå®Œæ•´çš„æ ‘ç»“æ„ï¼Œè€Œæ˜¯ä¸€ä¸ªâ€œèŠ‚ç‚¹åˆ—è¡¨â€ï¼Œå¯ä»¥æ‹¿å»ç”¨ pptree.print_tree() æ¥æ‰“å°å±•ç¤ºã€‚
```

**ä¸¾ä¸ªä¾‹å­**

```python
info = [
    ("Dr. Smith - Cardiologist", "Independent"),
    ("Dr. Lee - Neurologist", "Lead > Assistant"),
    ("Dr. Kim - Assistant", "Lead > Assistant"),
]
```

é…åˆ emojisï¼š

```scss
['ğŸ§‘â€', 'ğŸ‘©â€', 'ğŸ‘¨â€']
```

å°±ä¼šæ„é€ å‡ºä¸€æ£µå¦‚ä¸‹ç»“æ„çš„æ ‘ï¼š

```scss
moderator (ğŸ‘¨â€)
â”œâ”€â”€ Smith (ğŸ§‘â€)
â””â”€â”€ Lead (ğŸ‘©â€)
    â””â”€â”€ Assistant (ğŸ‘¨â€)
```

### parse_group_info

```python
def parse_group_info(group_info):
    lines = group_info.split('\n')
    
    parsed_info = {
        'group_goal': '',
        'members': []
    }

    parsed_info['group_goal'] = "".join(lines[0].split('-')[1:])
    
    
    for line in lines[1:]:
        if line.startswith('Member'):
            member_info = line.split(':')
            member_role_description = member_info[1].split('-')
            
            member_role = member_role_description[0].strip()
            member_expertise = member_role_description[1].strip() if len(member_role_description) > 1 else ''
            
            parsed_info['members'].append({
                'role': member_role,
                'expertise_description': member_expertise
            })
    
    return parsed_info
```

>è¯´æ˜ï¼š`group_info` æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºä¸€ç»„ä¿¡æ¯ï¼Œç¬¬ä¸€è¡Œæ˜¯â€œå°ç»„ç›®æ ‡â€ï¼Œåé¢å‡ è¡Œæ˜¯æˆå‘˜ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š
>
>```python
>group_info = """Group Goal - Diagnose lung cancer
>Member 1: Radiologist - Expert in CT scan interpretation
>Member 2: Oncologist - Specialized in cancer treatment"""
>```
>
>`lines[0]`
>
>å–å‡ºç¬¬ä¸€è¡Œï¼Œä¹Ÿå°±æ˜¯ï¼š
>
>```python
>"Group Goal - Diagnose lung cancer"
>```
>
>`.split('-')`
>
>æŒ‰ `-` æ‹†åˆ†æˆåˆ—è¡¨ï¼š
>
>```python
>['Group Goal ', ' Diagnose lung cancer']
>```
>
> `[1:]`
>
>å–å‡ºä»ç¬¬1ä¸ªï¼ˆç´¢å¼•ä¸º1ï¼‰å¼€å§‹çš„æ‰€æœ‰å…ƒç´ ï¼ˆä¸åŒ…æ‹¬ç¬¬0ä¸ªï¼‰ï¼š
>
>```python
>[' Diagnose lung cancer']
>```
>
> `"".join(...)`
>
>æŠŠè¿™ä¸ªåˆ—è¡¨é‡Œçš„å­—ç¬¦ä¸²å…¨éƒ¨æ‹¼æ¥èµ·æ¥ï¼ˆè¿™é‡Œåªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œæ‹¼æ¥åè¿˜æ˜¯ä¸€æ ·ï¼‰ï¼š
>
>```python
>' Diagnose lung cancer'
>```
>
>æœ€ç»ˆç»“æœå°±æ˜¯ï¼š
>
>```python
>parsed_info['group_goal'] = ' Diagnose lung cancer'
>```
>
>ä½ ä¹Ÿå¯ä»¥åŠ  `.strip()` å»é™¤ç©ºæ ¼ï¼Œæ›´å¹²å‡€ï¼š
>
>```python
>parsed_info['group_goal'] = "".join(lines[0].split('-')[1:]).strip()
>```

**æŠŠä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„ group ä¿¡æ¯è§£ææˆç»“æ„åŒ–çš„ Python å­—å…¸ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨ã€‚**

```python
#è¾“å…¥ï¼š
group_info = """Goal - Diagnose cardiovascular disease
Member1: Cardiologist - Expert in heart and blood vessels
Member2: Radiologist - Expert in imaging"""
#è¾“å‡º:

{
  'group_goal': 'Diagnose cardiovascular disease',
  'members': [
    {'role': 'Cardiologist', 'expertise_description': 'Expert in heart and blood vessels'},
    {'role': 'Radiologist', 'expertise_description': 'Expert in imaging'}
  ]
}
```

æŒ‰è¡Œæ‹†åˆ†å­—ç¬¦ä¸² â†’ `lines = group_info.split('\n')`

ç¬¬ä¸€è¡Œå–å‡ºä»»åŠ¡ç›®æ ‡ï¼ˆ`Goal - ...`ï¼‰ â†’ å­˜å…¥ `group_goal`

å…¶ä½™è¡Œåˆ¤æ–­æ˜¯å¦ä¸ºæˆå‘˜ä¿¡æ¯ï¼ŒæŒ‰å†’å·å’Œæ¨ªçº¿åˆ‡åˆ†è§’è‰²ä¸ä¸“ä¸šã€‚

### setup_model

```python
def setup_model(model_name):
    if 'gemini' in model_name:
        genai.configure(api_key=os.environ['genai_api_key'])
        return genai, None
    elif 'gpt' in model_name:
        client = OpenAI(api_key=os.environ['openai_api_key'])
        return None, client
    else:
        raise ValueError(f"Unsupported model: {model_name}")

```

æ ¹æ®æ¨¡å‹åï¼ˆå¦‚ `gpt-4o-mini`ã€`gemini-pro`ï¼‰é…ç½®ç›¸åº”çš„ API å®¢æˆ·ç«¯ï¼Œå¹¶è¿”å›ã€‚

è¾“å…¥ï¼šæ¨¡å‹åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰

è¾“å‡ºï¼š

* å¦‚æœæ˜¯ `gemini`ï¼Œè¿”å› `genai`ï¼ˆGemini çš„æ¨¡å—ï¼‰ã€`None`
* å¦‚æœæ˜¯ `gpt` ç³»åˆ—ï¼Œè¿”å› `None`ã€`OpenAI client`
* å¦åˆ™æŠ›å‡ºå¼‚å¸¸

è¿™æ˜¯ä¸€ä¸ª**æ¨¡å‹å·¥å‚å‡½æ•°**ï¼Œæ ¹æ®æ¨¡å‹åå­—æ¥åš API åˆå§‹åŒ–ã€‚åç»­è°ƒç”¨æ—¶åªéœ€ç”¨è¿”å›çš„å¯¹è±¡è¿›è¡Œè°ƒç”¨å³å¯ã€‚

### load_data

```python
def load_data(dataset):
    test_qa = []
    examplers = []

    test_path = f'../data/{dataset}/test.jsonl'
    with open(test_path, 'r') as file:
        for line in file:
            test_qa.append(json.loads(line))

    train_path = f'../data/{dataset}/train.jsonl'
    with open(train_path, 'r') as file:
        for line in file:
            examplers.append(json.loads(line))

    return test_qa, examplers
```

è¯»å–æŒ‡å®šæ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬ï¼ˆ`.jsonl` æ ¼å¼ï¼‰ï¼Œå¹¶è§£ææˆ Python å¯¹è±¡ã€‚

**è¾“å…¥ï¼š**

æ•°æ®é›†åç§°ï¼ˆä¾‹å¦‚ `"medqa"`ï¼‰

**è¾“å‡ºï¼š**

- `test_qa`: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
- `examplers`: è®­ç»ƒæ ·æœ¬åˆ—è¡¨ï¼ˆä½œä¸ºç¤ºä¾‹é¢˜ï¼‰

æŒ‰è¡Œè¯»å– `.jsonl`ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰ï¼Œç”¨ `json.loads` è½¬æ¢ä¸º Python å­—å…¸ï¼Œä¾¿äºä½¿ç”¨ã€‚

### create_question

```python
def create_question(sample, dataset):
    if dataset == 'medqa':
        #æ ¹æ®ä¸åŒæ•°æ®é›†ï¼ˆä¸»è¦æ˜¯ medqaï¼‰çš„æ ¼å¼ï¼ŒæŠŠé¢˜ç›®å’Œé€‰é¡¹æ‹¼æ¥æˆä¸€æ¡å®Œæ•´çš„é—®é¢˜å­—ç¬¦ä¸²ï¼Œä¾›æ¨¡å‹è¾“å…¥ã€‚
        """
        {
    		"question": "What is the first-line treatment for hypertension?",
    		"options": {
        		"A": "ACE inhibitors",
        		"B": "Beta blockers",
        		"C": "Calcium channel blockers",
        		"D": "Diuretics"
    		}
		}"""
        question = sample['question'] + " Options: "
        options = []
        for k, v in sample['options'].items():
            options.append("({}) {}".format(k, v))
        random.shuffle(options)
        
        question += " ".join(options)
        #å°†å¤„ç†å¥½çš„é€‰é¡¹ç”¨ç©ºæ ¼æ‹¼æ¥ï¼ŒåŠ åˆ°é¢˜å¹²åé¢ã€‚
        #"What is the first-line treatment for hypertension? Options: (C) Calcium channel blockers (A) ACE inhibitors (B) Beta blockers (D) Diuretics"

        
        return question, None
    	"""
    	è¿”å›æ„é€ å¥½çš„å­—ç¬¦ä¸²ï¼›
		ç¬¬äºŒä¸ªè¿”å›å€¼ None æ˜¯ä¸ºæœªæ¥æ‰©å±•å›¾åƒè¾“å…¥é¢„ç•™çš„ï¼ˆæ¯”å¦‚å›¾æ–‡é¢˜ç›®æ—¶åŠ å›¾åƒè·¯å¾„ï¼‰ã€‚
		"""
    
    return sample['question'], None 
	#å¯¹äºä¸æ˜¯ medqa çš„æ•°æ®é›†ï¼Œä¸å¤„ç†é€‰é¡¹ï¼Œåªè¿”å›é¢˜å¹²ï¼ˆåŸå§‹ questionï¼‰ã€‚
```

å°†åŸå§‹æ ·æœ¬è½¬æˆä¸€ä¸ªæ‹¼æ¥äº†é€‰é¡¹çš„æ ‡å‡†é—®é¢˜å­—ç¬¦ä¸²ã€‚

**è¾“å…¥ï¼š**

- `sample`: å•ä¸ªæ ·æœ¬ï¼ˆå­—å…¸ï¼‰
- `dataset`: æ•°æ®é›†åï¼ˆç›®å‰åªå¤„ç† `'medqa'`ï¼‰

**è¾“å‡ºï¼š**

- `question`: æ‹¼æ¥åçš„é—®é¢˜å­—ç¬¦ä¸²
- `img_path`: å§‹ç»ˆä¸º `None`ï¼ˆä¸ºå°†æ¥æ”¯æŒå›¾åƒé¢„ç•™ï¼‰

**é€»è¾‘è§£æï¼š**

æŠŠ `"question"` + `"options"` è½¬æˆä¸€ä¸ªè‡ªç„¶è¯­è¨€å½¢å¼çš„é—®é¢˜ï¼Œæ ¼å¼åƒè¿™æ ·ï¼š

```
What is the likely diagnosis? Options: (A) X (B) Y (C) Z ...
```

### determine_difficulty

```python
def determine_difficulty(question, difficulty):
    if difficulty != 'adaptive':
        return difficulty
    
    difficulty_prompt = f"""Now, given the medical query as below, you need to decide the difficulty/complexity of it:\n{question}.\n\nPlease indicate the difficulty/complexity of the medical query among below options:\n1) basic: a single medical agent can output an answer.\n2) intermediate: number of medical experts with different expertise should dicuss and make final decision.\n3) advanced: multiple teams of clinicians from different departments need to collaborate with each other to make final decision."""
    
    medical_agent = Agent(instruction='You are a medical expert who conducts initial assessment and your job is to decide the difficulty/complexity of the medical query.', role='medical expert', model_info='gpt-3.5')
    medical_agent.chat('You are a medical expert who conducts initial assessment and your job is to decide the difficulty/complexity of the medical query.')
    response = medical_agent.chat(difficulty_prompt)

    if 'basic' in response.lower() or '1)' in response.lower():
        return 'basic'
    elif 'intermediate' in response.lower() or '2)' in response.lower():
        return 'intermediate'
    elif 'advanced' in response.lower() or '3)' in response.lower():
        return 'advanced'
```

æ ¹æ®é¢˜ç›®å†…å®¹å’Œç”¨æˆ·è¾“å…¥çš„ `--difficulty` å‚æ•°ï¼Œ**è‡ªåŠ¨æˆ–æ‰‹åŠ¨åˆ¤æ–­é¢˜ç›®éš¾åº¦ç­‰çº§**ã€‚

**è¾“å…¥ï¼š**

- `question`: é—®é¢˜å­—ç¬¦ä¸²
- `difficulty`: æŒ‡å®šéš¾åº¦æˆ– `"adaptive"`ï¼ˆè‡ªé€‚åº”ï¼‰

**è¾“å‡ºï¼š**

éš¾åº¦å­—ç¬¦ä¸²ï¼š`'basic'`, `'intermediate'`, `'advanced'`

**é€»è¾‘è§£æï¼š**

å¦‚æœä¸æ˜¯ `"adaptive"`ï¼Œç›´æ¥è¿”å›ï¼›

å¦‚æœæ˜¯ `"adaptive"`ï¼š

- æ„é€ æç¤º promptï¼Œé—®ä¸€ä¸ª LLM ä»£ç†ï¼ˆGPT-3.5ï¼‰æ¥åˆ¤æ–­éš¾åº¦ï¼›
- æ ¹æ®è¿”å›æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å« `basic`/`intermediate`/`advanced` ç­‰å…³é”®è¯æ¥åˆ¤æ–­ç­‰çº§ã€‚

























































































































